
@InProceedings{dAlessandro2009,
  Title                    = {Advanced Techniques for Vertical Tablet Playing A Overview of Two Years of Practicing the HandSketch 1.x},
  Author                   = {d'Alessandro, Nicolas and Dutoit, Thierry},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {173--174},
  Abstract                 = {In this paper we present new issues and challenges relatedto the vertical tablet playing. The approach is based on apreviously presented instrument, the HANDSKETCH. Thisinstrument has now been played regularly for more than twoyears by several performers. Therefore this is an opportunityto propose a better understanding of the performing strategy.We present the behavior of the whole body as an underlyingaspect in the manipulation of the instrument.},
  Keywords                 = {graphic tablet, playing position, techniques },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_173.pdf},
  DOI                      = {10.5281/zenodo.1177465}
}

@InProceedings{Bottcher2009,
  Title                    = {An Early Prototype of the Augmented PsychoPhone},
  Author                   = {B\''{o}ttcher, Niels and Dimitrov, Smilen},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {151--152},
  Abstract                 = {In this poster we present the early prototype of the augmented Psychophone --- a saxophone with various applied sensors, allowing the saxophone player to attach effects like pitch shifting, wah-wah and ring modulation to the saxophone, simply by moving the saxophone as one would do when really being enthusiastic and involved in the performance. The possibility of scratching on the previously recorded sound is also possible directly on the saxophone. },
  Keywords                 = {Augmented saxophone, Physical computing, hyper instruments, mapping. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_151.pdf},
  DOI                      = {10.5281/zenodo.1177467}
}

@InProceedings{Baalman2009,
  Title                    = {Code LiveCode Live, or livecode Embodied},
  Author                   = {Baalman, Marije A.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {329--329},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_329.pdf},
  DOI                      = {10.5281/zenodo.1177469}
}

@InProceedings{Baalman2009a,
  Title                    = {Sharing Data in Collaborative, Interactive Performances : the SenseWorld DataNetwork},
  Author                   = {Baalman, Marije A. and Smoak, Harry C. and Salter, Christopher L. and Malloch, Joseph and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {131--134},
  Keywords                 = {Data exchange, collaborative performance, in- teractive performance, interactive art works, sensor data, Open- SoundControl, SuperCollider, Max/MSP Introduction and Background The SenseWorld Data Network addresses one of the major challenges in the research/creation of interactive live per- formance work the sharing and manipulation of raw and/or conditioned sensor data among different media systems (real time audio and video, lighting, mechatronics, show control, etc). While the },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_131.pdf},
  DOI                      = {10.5281/zenodo.1177471}
}

@InProceedings{Barri2009,
  Title                    = {Versum : Audiovisual Composing in 3d},
  Author                   = {Barri, Tarik},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {264--265},
  Abstract                 = {This paper introduces the new audiovisual sequencing system "Versum" that allows users to compose in three dimensions. In the present paper the conceptual soil from which this system has sprung is discussed first. Secondly, the basic concepts with which Versum operates are explained, providing a general idea of what is meant by sequencing in three dimensions and explaining what compositions made in Versum can look and sound like. Thirdly, the practical ways in which a composer can use Versum to make his own audiovisual compositions are presented by means of a more detailed description of the different graphical user interface elements. Fourthly, a short description is given of the modular structure of the software underlying Versum. Finally, several foresights regarding the directions in which Versum will continue to develop in the near future are presented. },
  Keywords                 = {audiovisual, sequencing, collaboration. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_264.pdf},
  DOI                      = {10.5281/zenodo.1177473}
}

@InProceedings{Barri2009a,
  Title                    = {Versum -- Fluor},
  Author                   = {Barri, Tarik},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {325--325},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_325.pdf},
  DOI                      = {10.5281/zenodo.1177475}
}

@InProceedings{Berdahl2009,
  Title                    = {HSP : A Simple and Effective Open-Source Platform for Implementing Haptic Musical Instruments},
  Author                   = {Berdahl, Edgar and Niemeyer, G\''{u}nter and Smith, Julius O.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {262--263},
  Keywords                 = {but,com-,currently chai 3d is,figure 1,fully supported only on,grip,haptic musical instrument,haptics,hsp,musician holding the falcon,nime09,novint,pd,physical modeling,pure data,puter music,windows},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_262.pdf},
  DOI                      = {10.5281/zenodo.1177477}
}

@InProceedings{Berdahl2009a,
  Title                    = {Using Haptic Devices to Interface Directly with Digital Waveguide-Based Musical Instruments},
  Author                   = {Berdahl, Edgar and Niemeyer, G\''{u}nter and Smith, Julius O.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {183--186},
  Abstract                 = {A haptic musical instrument is an electronic musical instrument that provides the musician not only with audio feedback but also with force feedback. By programming feedback controllers to emulate the laws of physics, many haptic musical instruments have been previously designed thatmimic real acoustic musical instruments. The controllerprograms have been implemented using finite difference and(approximate) hybrid digital waveguide models. We presenta novel method for constructing haptic musical instrumentsin which a haptic device is directly interfaced with a conventional digital waveguide model by way of a junction element, improving the quality of the musician's interactionwith the virtual instrument. We introduce both the explicitdigital waveguide control junction and the implicit digitalwaveguide control junction.},
  Keywords                 = {haptic musical instrument, digital waveguide, control junction, explicit, implicit, teleoperation },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_183.pdf},
  DOI                      = {10.5281/zenodo.1177479}
}

@InProceedings{Berdahl2009b,
  Title                    = {Using Haptics to Assist Performers in Making Gestures to a Musical Instrument},
  Author                   = {Berdahl, Edgar and Niemeyer, G\''{u}nter and Smith, Julius O.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {177--182},
  Abstract                 = {Haptic technology, providing force cues and creating a programmable physical instrument interface, can assist musicians in making gestures. The finite reaction time of thehuman motor control system implies that the execution of abrief musical gesture does not rely on immediate feedbackfrom the senses, rather it is preprogrammed to some degree.Consequently, we suggest designing relatively simple anddeterministic interfaces for providing haptic assistance.In this paper, we consider the specific problem of assisting a musician in selecting pitches from a continuous range.We build on a prior study by O'Modhrain of the accuracyof pitches selected by musicians on a Theremin-like hapticinterface. To improve the assistance, we augment the interface with programmed detents so that the musician can feelthe locations of equal tempered pitches. Nevertheless, themusician can still perform arbitrary pitch inflections such asglissandi, falls, and scoops. We investigate various formsof haptic detents, including fixed detent levels and forcesensitive detent levels. Preliminary results from a subjecttest confirm improved accuracy in pitch selection broughtabout by detents.},
  Keywords                 = {Haptic, detent, pitch selection, human motor system, feedback control, response time, gravity well },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_177.pdf},
  DOI                      = {10.5281/zenodo.1177481}
}

@InProceedings{Bianchi2009,
  Title                    = {The Drummer : a Collaborative Musical Interface with Mobility},
  Author                   = {Bianchi, Andrea and Yeo, Woon Seung},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {316--319},
  Abstract                 = {It has been shown that collaborative musical interfaces encourage novice users to explore the sound space and promote their participation as music performers. Nevertheless, such interfaces are generally physically situated and can limit the possibility of movements on the stage, a critical factor in live music performance. In this paper we introduce the Drummer, a networked digital musical interface that allows multiple performers to design and play drum kits simultaneously while, at the same time, keeping their ability to freely move on the stage. The system consists of multiple Nintendo DS clients with an intuitive, user-configurable interface and a server computer which plays drum sounds. The Drummer Machine, a small piece of hardware to augment the performance of the Drummer, is also introduced. },
  Keywords                 = {2,collaborative interface,expression,figure 1,game control,motivations and related work,multiplayer,musical,musical control,nime09,nintendo ds,on a nintendo ds,the drummer client running},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_316.pdf},
  DOI                      = {10.5281/zenodo.1177483}
}

@InProceedings{Bouillot2009,
  Title                    = {Challenges and Performance of High-Fidelity Audio Streaming for Interactive Performances},
  Author                   = {Bouillot, Nicolas and Cooperstock, Jeremy R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {135--140},
  Abstract                 = {Low-latency streaming of high-quality audio has the potential to dramatically transform the world of interactive musical applications. We provide methods for accurately measuring the end-to-end latency and audio quality of a delivered audio stream and apply these methods to an empirical evaluation of several streaming engines. In anticipationof future demands for emerging applications involving audio interaction, we also review key features of streamingengines and discuss potential challenges that remain to beovercome.},
  Keywords                 = {Networked Musical Performance, high-fidelity audio streaming, glitch detection, latency measurement },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_135.pdf},
  DOI                      = {10.5281/zenodo.1177485}
}

@InProceedings{Bukvic2009,
  Title                    = {Elemental \& Cyrene Reefs},
  Author                   = {Bukvic, Ivika and Standley, Eric},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {337--337},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_337.pdf},
  DOI                      = {10.5281/zenodo.1177487}
}

@InProceedings{Bullock2009,
  Title                    = {Towards a Humane Graphical User Interface for Live Electronic Music},
  Author                   = {Bullock, Jamie and Coccioli, Lamberto},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {266--267},
  Abstract                 = {In this paper we describe findings related to user interfacerequirements for live electronic music arising from researchconducted as part of the first three-year phase of the EUfunded Integra project. A number of graphical user interface(GUI) prototypes developed during the Integra project initial phase are described and conclusions drawn about theirdesign and implementation.},
  Keywords                 = {Integra, User Interface, Usability, Design, Live Electronics, Music Technology },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_266.pdf},
  DOI                      = {10.5281/zenodo.1177489}
}

@InProceedings{Collicutt2009,
  Title                    = {From Real to Virtual : A Comparison of Input Devices for Percussion Tasks},
  Author                   = {Collicutt, Mike and Casciato, Carmine and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {1--6},
  Abstract                 = {This paper presents an evaluation and comparison of four input devices for percussion tasks: a standard tom drum, Roland V-Drum, and two established examples of gestural controllers: the Buchla Lightning II, and the Radio Baton. The primary goal of this study was to determine how players' actions changed when moving from an acoustic instrument like the tom drum, to a gestural controller like the Buchla Lightning, which bears little resemblance to an acoustic percussion instrument. Motion capture data was analyzed by comparing a subject's hand height variability and timing accuracy across the four instruments as they performed simple musical tasks. Results suggest that certain gestures such as hand height amplitude can be adapted to these gestural controllers with little change and that in general subjects' timing variability is significantly affected when playing on the Lightning and Radio Baton when compared to the more familiar tom drum and VDrum. Possible explanations and other observations are also presented. },
  Keywords                 = {Evaluation of Input Devices, Motion Capture, Buchla Lightning II, Radio Baton. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_001.pdf},
  DOI                      = {10.5281/zenodo.1177491}
}

@InProceedings{Cook2009,
  Title                    = {Re-Designing Principles for Computer Music Controllers : a Case Study of SqueezeVox Maggie},
  Author                   = {Cook, Perry R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {218--221},
  Keywords                 = {1,batteries,composed instruments,hci,i will restate the,laptop orchestras,nime09,original 13 principles for,sensas,the original principles,to begin,voice synthesis,wireless},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_218.pdf},
  DOI                      = {10.5281/zenodo.1177493}
}

@InProceedings{Crawford2009,
  Title                    = {The Midi-AirGuitar , A serious Musical Controller with a Funny Name Music Technology Program},
  Author                   = {Crawford, Langdon and Fastenow, William D.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {149--150},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_149.pdf},
  DOI                      = {10.5281/zenodo.1177495}
}

@InProceedings{Dolphin2009,
  Title                    = {SpiralSet : A Sound Toy Utilizing Game Engine Technologies},
  Author                   = {Dolphin, Andy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {56--57},
  Abstract                 = {SpiralSet is a sound toy incorporating game enginesoftware used in conjunction with a spectral synthesissound engine constructed in Max/MSP/Jitter. SpiralSetwas presented as an interactive installation piece at theSonic Arts Expo 2008, in Brighton, UK. A custom madesensor-based interface is used for control of the system.The user interactions are designed to be quickly accessiblein an installation context, yet allowing the potential forsonic depth and variation.},
  Keywords                 = {Sound Toys, Game Engines, Animated Interfaces, Spectral Synthesis, Open Work, Max/MSP. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_056.pdf},
  DOI                      = {10.5281/zenodo.1177497}
}

@InProceedings{Dolphin2009a,
  Title                    = {MagNular : Symbolic Control of an External Sound Engine Using an Animated Interface},
  Author                   = {Dolphin, Andy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {159--160},
  Abstract                 = {This paper reports on work in progress on the creativeproject MagNular, part of a wider practical study of thepotential collaborative compositional applications of gameengine technologies. MagNular is a sound toy utilizingcomputer game and physics engine technologies to createan animated interface used in conjunction with an externalsound engine developed within Max/MSP. The playercontrols virtual magnets that attract or repel numerousparticle objects, moving them freely around the virtualspace. Particle object collision data is mapped to controlsound onsets and synthesis/DSP (Digital SignalProcessing) parameters. The user "composes" bycontrolling and influencing the simulated physicalbehaviors of the particle objects within the animatedinterface.},
  Keywords                 = {Sound Toys, Open Work, Game Engines, Animated Interfaces, Max/MSP. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_159.pdf},
  DOI                      = {10.5281/zenodo.1177499}
}

@InProceedings{Dubois2009,
  Title                    = {Bioluminescence},
  Author                   = {Dubois, R. Luke and Flanigan, Lesley},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {336--336},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_336.pdf},
  DOI                      = {10.5281/zenodo.1177501}
}

@InProceedings{Essl2009,
  Title                    = {SpeedDial : Rapid and On-The-Fly Mapping of Mobile Phone Instruments},
  Author                   = {Essl, Georg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {270--273},
  Keywords                 = {mobile phone instruments,nime,nime09,on-the-fly},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_270.pdf},
  DOI                      = {10.5281/zenodo.1177503}
}

@InProceedings{Feehan2009,
  Title                    = {Audio Orienteering -- Navigating an Invisible Terrain},
  Author                   = {Feehan, Noah},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {161--162},
  Abstract                 = {AUDIO ORIENTEERING is a collaborative performance environment in which physical tokens are used to navigate an invisible sonic landscape. In this paper, I describe the hardware and software used to implement a prototype audio terrain with multiple interaction modes and sonic behaviors mapped onto three-dimensional space. },
  Keywords                 = {wii, 3-d positioning, audio terrain, collaborative performance. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_161.pdf},
  DOI                      = {10.5281/zenodo.1177505}
}

@InProceedings{Fels2009,
  Title                    = {ForTouch : A Wearable Digital Ventriloquized Actor},
  Author                   = {Fels, Sidney S. and Pritchard, Bob and Lenters, Allison},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {274--275},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_274.pdf},
  DOI                      = {10.5281/zenodo.1177509}
}

@InProceedings{Ferguson2009,
  Title                    = {An Interface for Live Interactive Sonification},
  Author                   = {Ferguson, Sam and Beilharz, Kirsty},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {35--36},
  Keywords                 = {Sonification, Interactive Sonification, Auditory Display. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_035.pdf},
  DOI                      = {10.5281/zenodo.1177511}
}

@InProceedings{Fiebrink2009,
  Title                    = {A Meta-Instrument for Interactive, On-the-Fly Machine Learning},
  Author                   = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {280--285},
  Abstract                 = {Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance. },
  Keywords                 = {Machine learning, mapping, tools. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_280.pdf},
  DOI                      = {10.5281/zenodo.1177513}
}

@InProceedings{Freed2009,
  Title                    = {Novel and Forgotten Current-steering Techniques for Resistive Multitouch, Duotouch, and Polytouch Position Sensing with Pressure},
  Author                   = {Freed, Adrian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {230--235},
  Abstract                 = {A compendium of foundational circuits for interfacing resistive pressure and position sensors is presented with example applications for music controllers and tangible interfaces. },
  Keywords                 = {Piezoresistive Touch Sensor Pressure Sensing Current Steering Multitouch. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_230.pdf},
  DOI                      = {10.5281/zenodo.1177515}
}

@InProceedings{Freed2009a,
  Title                    = {Features and Future of Open Sound Control version 1.1 for NIME},
  Author                   = {Freed, Adrian and Schmeder, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {116--120},
  Abstract                 = {The history and future of Open Sound Control (OSC) is discussed and the next iteration of the OSC specification is introduced with discussion of new features to support NIME community activities. The roadmap to a major revision of OSC is developed. },
  Keywords                 = {Open Sound Control, Time Tag, OSC, Reservation Protocols. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_116.pdf},
  DOI                      = {10.5281/zenodo.1177517}
}

@InProceedings{Fyans2009,
  Title                    = {Where Did It All Go Wrong ? A Model of Error From the Spectator's Perspective},
  Author                   = {Fyans, A. Cavan and Gurevich, Michael and Stapleton, Paul},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {171--172},
  Abstract                 = {The development of new interfaces for musical expressionhas created a need to study how spectators comprehend newperformance technologies and practices. As part of a largerproject examining how interactions with technology can becommunicated with the spectator, we relate our model ofspectator understanding of error to the NIME discourse surrounding transparency, mapping, skill and success.},
  Keywords                 = {performance, skill, transparency, design, HCI },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_171.pdf},
  DOI                      = {10.5281/zenodo.1177519}
}

@InProceedings{Gallin2009,
  Title                    = {Sensor Technology and the Remaking of Instruments from the Past},
  Author                   = {Gallin, Emmanuelle and Sirguy, Marc},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {199--202},
  Keywords                 = {Controller, Sensor, MIDI, USB, Computer Music, ribbon controllers, ribbon cello. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_199.pdf},
  DOI                      = {10.5281/zenodo.1177521}
}

@InProceedings{Gao2009,
  Title                    = {LUMI : Live Performance Paradigms Utilizing Software Integrated Touch Screen and Pressure Sensitive Button Matrix},
  Author                   = {Gao, Mingfei and Hanson, Craig},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {58--59},
  Abstract                 = {This paper explores a rapidly developed, new musical interface involving a touch-screen, 32 pressure sensitive button pads, infrared sensor, 8 knobs and cross-fader. We provide a versatile platform for computer-based music performance and production using a human computer interface that has strong visual and tactile feedback as well as robust software that exploits the strengths of each individual system component. },
  Keywords                 = {live performance interface,lumi,nime09,pressure},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_058.pdf},
  DOI                      = {10.5281/zenodo.1177547}
}

@InProceedings{Gelineck2009,
  Title                    = {A Quantitative Evaluation of the Differences between Knobs and Sliders},
  Author                   = {Gelineck, Steven and Serafin, Stefania},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {13--18},
  Abstract                 = {This paper presents a HCI inspired evaluation of simple physical interfaces used to control physical models. Specifically knobs and sliders are compared in a creative and exploratory framework, which simulates the natural environment in which an electronic musician would normally explore a new instrument. No significant difference was measured between using knobs and sliders for controlling parameters of a physical modeling electronic instrument. Thereported difference between the tested instruments were mostlydue to the sound synthesis models.},
  Keywords                 = {Evaluation, Interfaces, Sliders, Knobs, Physi- cal Modeling, Electronic Musicians, Exploration, Creativ- ity, Affordances. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_013.pdf},
  DOI                      = {10.5281/zenodo.1177549}
}

@InProceedings{Gillian2009,
  Title                    = {The {SAR}C EyesWeb Catalog : A Pattern Recognition Toolbox for Musician-Computer Interaction},
  Author                   = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {60--61},
  Abstract                 = {This paper presents the SARC EyesWeb Catalog (SEC), agroup of blocks designed for real-time gesture recognitionthat have been developed for the open source program EyesWeb. We describe how the recognition of real-time bodymovements can be used for musician-computer-interaction.},
  Keywords                 = {SARC EyesWeb Catalog, gesture recognition },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_060.pdf},
  DOI                      = {10.5281/zenodo.1177551}
}

@InProceedings{Gillian2009a,
  Title                    = {Scratch-Off : A Gesture Based Mobile Music Game with Tactile Feedback},
  Author                   = {Gillian, Nicholas and O'Modhrain, Sile and Essl, Georg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {308--311},
  Abstract                 = {This paper presents {Scratch-Off}, a new musical multiplayer DJ game that has been designed for a mobile phone. We describe how the game is used as a test platform for experimenting with various types of multimodal feedback. The game uses movement gestures made by the players to scratch a record and control crossfades between tracks, with the objective of the game to make the correct scratch at the correct time in relation to the music. Gestures are detected using the devices built-in tri-axis accelerometer and multi-touch screen display. The players receive visual, audio and various types of vibrotactile feedback to help them make the correct scratch on the beat of the music track. We also discuss the results of a pilot study using this interface. },
  Keywords                 = {Mobile devices, gesture, audio games. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_308.pdf},
  DOI                      = {10.5281/zenodo.1177553}
}

@InProceedings{Gong2009,
  Title                    = {MusicGrip : A Writing Instrument for Music Control},
  Author                   = {Gong, Nan-Wei and Laibowitz, Mat and Paradiso, Joseph A.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {74--77},
  Abstract                 = {In this project, we have developed a real-time writing instrument for music control. The controller, MusicGrip, can capture the subtle dynamics of the user's grip while writing or drawing and map this to musical control signals and sonic outputs. This paper discusses this conversion of the common motor motion of handwriting into an innovative form of music expression. The presented example instrument can be used to integrate the composing aspect of music with painting and writing, creating a new art form from the resultant aural and visual representation of the collaborative performing process. },
  Keywords                 = {Interactive music control, writing instrument, pen controller, MIDI, group performing activity. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_074.pdf},
  DOI                      = {10.5281/zenodo.1177555}
}

@InProceedings{Goto2009,
  Title                    = {BodyJack},
  Author                   = {Goto, Suguru},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {328--328},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_328.pdf},
  DOI                      = {10.5281/zenodo.1177557}
}

@InProceedings{Goto2009a,
  Title                    = {netBody --- "Augmented Body and Virtual Body II" with the System, BodySuit, Powered Suit and Second Life --- Its Introduction of an Application of the System},
  Author                   = {Goto, Suguru and Powell, Rob},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {48--49},
  Abstract                 = {This is intended to introduce the system, which combines BodySuit, especially Powered Suit, and Second Life, as well as its possibilities and its uses in a musical performance application. The system which we propose contains both a gesture controller and robots at the same time. In this system, the Data Suit, BodySuit controls the avatar in Second Life and Second Life controls the exoskeleton, Powered Suit in real time. These are related with each other in conjunction with Second Life in Internet. BodySuit doesn't contain a hand-held controller. A performer, for example a dancer, wears a suit. Gestures are transformed into electronic signals by sensors. Powered Suit is another suit that a dancer wears, but gestures are generated by motors. This is a sort of wearable robot. Second Life is software that is developed by Linden Lab. It allows creating a virtual world and a virtual human (avatar) in Internet. Working together with BodySuit, Powered Suit, and Second Life the idea behind the system is that a human body is augmented by electronic signals and is reflected in a virtual world in order to be able to perform interactively. },
  Keywords                 = {artificial intelligence,gesture controller,humanoid robot,interaction,internet,nime09,robot},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_048.pdf},
  DOI                      = {10.5281/zenodo.1177559}
}

@InProceedings{GreshamLancaster2009,
  Title                    = {Cellphonia: 4'33},
  Author                   = {Gresham-Lancaster, Scot and Bull, Steve},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {338--338},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_338.pdf},
  DOI                      = {10.5281/zenodo.1177561}
}

@InProceedings{Gurevich2009,
  Title                    = {Designing for Style in New Musical Interactions},
  Author                   = {Gurevich, Michael and Stapleton, Paul and Bennett, Peter},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {213--217},
  Abstract                 = {In this paper we discuss the concept of style, focusing in particular on methods of designing new instruments that facilitate the cultivation and recognition of style. We distinguishbetween style and structure of an interaction and discuss thesignificance of this formulation within the context of NIME.Two workshops that were conducted to explore style in interaction design are described, from which we identify elements of style that can inform and influence the design process. From these, we suggest steps toward designing forstyle in new musical interactions.},
  Keywords                 = {expression, style, structure, skill, virtuosity },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_213.pdf},
  DOI                      = {10.5281/zenodo.1177563}
}

@InProceedings{Hoofer2009,
  Title                    = {Gyroscope-Based Conducting Gesture Recognition},
  Author                   = {H\''{o}ofer, Andreas and Hadjakos, Aristotelis and M\''{u}hlh\''{a}user, Max},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {175--176},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_175.pdf},
  DOI                      = {10.5281/zenodo.1177565}
}

@InProceedings{Hadjakos2009,
  Title                    = {Probabilistic Model of Pianists' Arm Touch Movements},
  Author                   = {Hadjakos, Aristotelis and Aitenbichler, Erwin and M\''{u}hlh\''{a}user, Max},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {7--12},
  Abstract                 = {Measurement of pianists' arm movement provides a signal,which is composed of controlled movements and noise. Thenoise is composed of uncontrolled movement generated bythe interaction of the arm with the piano action and measurement error. We propose a probabilistic model for armtouch movements, which allows to estimate the amount ofnoise in a joint. This estimation helps to interpret the movement signal, which is of interest for augmented piano andpiano pedagogy applications.},
  Keywords                 = {Piano, arm movement, gesture, classification, augmented instrument, inertial sensing. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_007.pdf},
  DOI                      = {10.5281/zenodo.1177567}
}

@InProceedings{Havryliv2009,
  Title                    = {Haptic Carillon -- Analysis \& Design of the Carillon Mechanism},
  Author                   = {Havryliv, Mark and Naghdy, Fazel and Schiemer, Greg and Hurd, Timothy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {187--192},
  Abstract                 = {The carillon is one of the few instruments that elicit sophisticated haptic interaction from amateur and professional players alike. Like the piano keyboard, the velocity of a player's impact on each carillon key, or baton, affects the quality of the resultant tone; unlike the piano, each carillon baton returns a different forcefeedback. Force-feedback varies widely from one baton to the next across the entire range of the instrument and with further idiosyncratic variation from one instrument to another. This makes the carillon an ideal candidate for haptic simulation. The application of synthesized forcefeedback based on an analysis of forces operating in a typical carillon mechanism offers a blueprint for the design of an electronic practice clavier and with it the solution to a problem that has vexed carillonists for centuries, namely the inability to rehearse repertoire in private. This paper will focus on design and implementation of a haptic carillon clavier derived from an analysis of the Australian National Carillon in Canberra. },
  Keywords                 = {Haptics, force-feedback, mechanical analysis. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_187.pdf},
  DOI                      = {10.5281/zenodo.1177569}
}

@InProceedings{Henriques2009,
  Title                    = {Double Slide Controller},
  Author                   = {Henriques, Tom\'{a}s},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {260--261},
  Keywords                 = {a,a double slide mechanism,computer music,consisting of a set,hardware and software design,making,musical instrument,nime09,of two,sensor technologies,sensors used in their,these are as follows},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_260.pdf},
  DOI                      = {10.5281/zenodo.1177571}
}

@InProceedings{Hindman2009,
  Title                    = {Performance: Modal Kombat Plays {PON}G},
  Author                   = {Hindman, David and Drummond, Evan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {332--332},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_332.pdf},
  DOI                      = {10.5281/zenodo.1177573}
}

@InProceedings{Hockman2009,
  Title                    = {Real-Time Phase Vocoder Manipulation by Runner's Pace},
  Author                   = {Hockman, Jason A. and Wanderley, Marcelo M. and Fujinaga, Ichiro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {90--93},
  Abstract                 = {This paper presents a method for using a runner's pacefor real-time control of the time-scaling facility of a phasevocoder, resulting in the automated synchronization of anaudio track tempo to the generated control signal. The increase in usage of portable music players during exercisehas given rise to the development of new personal exerciseaids, most notably the Nike+iPod system, which relies onembedded sensor technologies to provide kinematic workout statistics. There are also systems that select songs basedon the measured step frequency of a runner. The proposedsystem also uses the pace of a runner, but this information isused to change the tempo of the music.},
  Keywords                 = {NIME, synchronization, exercise, time-scaling. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_090.pdf},
  DOI                      = {10.5281/zenodo.1177575}
}

@InProceedings{Hong2009,
  Title                    = {Cosmic Strings II},
  Author                   = {Hong, Min Eui},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {322--322},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_322.pdf},
  DOI                      = {10.5281/zenodo.1177577}
}

@InProceedings{Hsu2009,
  Title                    = {Evaluating Interactive Music Systems : An HCI Approach},
  Author                   = {Hsu, William and Sosnick, Marc},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {25--28},
  Abstract                 = {In this paper, we discuss a number of issues related to the design of evaluation tests for comparing interactive music systems for improvisation. Our testing procedure covers rehearsal and performance environments, and captures the experiences of a musician/participant as well as an audience member/observer. We attempt to isolate salient components of system behavior, and test whether the musician or audience are able to discern between systems with significantly different behavioral components. We report on our experiences with our testing methodology, in comparative studies of our London and ARHS improvisation systems [1]. },
  Keywords                 = {Interactive music systems, human computer interaction, evaluation tests. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_025.pdf},
  DOI                      = {10.5281/zenodo.1177579}
}

@InProceedings{Humphrey2009,
  Title                    = {The Navi Activity Monitor : Toward Using Kinematic Data to Humanize Computer Music},
  Author                   = {Humphrey, Eric and Leider, Colby},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {31--32},
  Abstract                 = {Motivated by previous work aimed at developing mathematical models to describe expressive timing in music, and specifically the final ritardandi, using measured kinematic data, we further investigate the linkage of locomotion and timing in music. The natural running behavior of four subjects is measured with a wearable sensor prototype and analyzed to create normalized tempo curves. The resulting curves are then used to modulate the final ritard of MIDI scores, which are also performed by an expert musician. A Turing-inspired listening test is conducted to observe a human listener's ability to determine the nature of the performer. },
  Keywords                 = {Musical kinematics, expressive tempo, machine music. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_031.pdf},
  DOI                      = {10.5281/zenodo.1177581}
}

@InProceedings{Jessop2009,
  Title                    = {The Vocal Augmentation and Manipulation Prosthesis (VAMP): A Conducting-Based Gestural Controller for Vocal Performance},
  Author                   = {Jessop, Elena},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {256--259},
  Abstract                 = {This paper describes The Vocal Augmentation and Manipulation Prosthesis (VAMP) a gesture-based wearable controller for live-time vocal performance. This controller allows a singer to capture and manipulate single notes that he or she sings, using a gestural vocabulary developed from that of choral conducting. By drawing from a familiar gestural vocabulary, this controller and the associated mappings can be more intuitive and expressive for both performer and audience. },
  Keywords                 = {musical expressivity, vocal performance, gestural control, conducting. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_256.pdf},
  DOI                      = {10.5281/zenodo.1177583}
}

@InProceedings{Johnston2009,
  Title                    = {Designing for Conversational Interaction},
  Author                   = {Johnston, Andrew and Candy, Linda and Edmonds, Ernest},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {207--212},
  Abstract                 = {In this paper we describe an interaction framework whichclassifies musicians' interactions with virtual musical instruments into three modes: instrumental, ornamental andconversational. We argue that conversational interactionsare the most difficult to design for, but also the most interesting. To illustrate our approach to designing for conversational interactions we describe the performance workPartial Reflections 3 for two clarinets and interactive software. This software uses simulated physical models to create a virtual sound sculpture which both responds to andproduces sounds and visuals.},
  Keywords                 = {Music, instruments, interaction. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_207.pdf},
  DOI                      = {10.5281/zenodo.1177585}
}

@InProceedings{Jones2009,
  Title                    = {The Fragmented Orchestra},
  Author                   = {Jones, Daniel and Hodgson, Tim and Grant, Jane and Matthias, John and Outram, Nicholas and Ryan, Nick},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {297--302},
  Abstract                 = {The Fragmented Orchestra is a distributed musical instrument which combines live audio streams from geographically disparate sites, and granulates each according to thespike timings of an artificial spiking neural network. Thispaper introduces the work, outlining its historical context,technical architecture, neuronal model and network infrastructure, making specific reference to modes of interactionwith the public.},
  Keywords                 = {distributed,emergent,environmental,installation,neural network,nime09,sound,streaming audio},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_297.pdf},
  DOI                      = {10.5281/zenodo.1177587}
}

@InProceedings{Jones2009a,
  Title                    = {A Force-Sensitive Surface for Intimate Control},
  Author                   = {Jones, Randy and Driessen, Peter and Schloss, Andrew and Tzanetakis, George},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {236--241},
  Abstract                 = {This paper presents a new force-sensitive surface designedfor playing music. A prototype system has been implemented using a passive capacitive sensor, a commodity multichannel audio interface, and decoding software running ona laptop computer. This setup has been a successful, lowcost route to a number of experiments in intimate musicalcontrol.},
  Keywords                 = {Multitouch, sensors, tactile, capacitive, percus- sion controllers. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_236.pdf},
  DOI                      = {10.5281/zenodo.1177589}
}

@InProceedings{DeJong2009,
  Title                    = {Developing the Cyclotactor},
  Author                   = {de Jong, Staas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {163--164},
  Abstract                 = {This paper presents developments in the technology underlying the cyclotactor, a finger-based tactile I/O device for musical interaction. These include significant improvements both in the basic characteristics of tactile interaction and in the related (vibro)tactile sample rates, latencies, and timing precision. After presenting the new prototype's tactile output force landscape, some of the new possibilities for interaction are discussed, especially those for musical interaction with zero audio/tactile latency.},
  Keywords                 = {Musical controller, tactile interface. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_163.pdf},
  DOI                      = {10.5281/zenodo.1177591}
}

@InProceedings{Kahrs2009,
  Title                    = {An Electroacoustically Controlled Vibrating Plate},
  Author                   = {Kahrs, Mark and Skulina, David and Bilbao, Stefan and Campbell, Murray},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {106--109},
  Abstract                 = {Large vibrating plates are used as thunder sheets in orchestras. We have extended the use of flat plates by cementing aflat panel electroacoustic transducer on a large brass sheet.Because of the thickness of the panel, the output is subject tononlinear distortion. When combined with a real-time inputand signal processing algorithm, the active brass plate canbecome an effective musical instrument for performance ofnew music.},
  Keywords                 = {Electroacoustics, flat panel },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_106.pdf},
  DOI                      = {10.5281/zenodo.1177593}
}

@InProceedings{Kanda2009,
  Title                    = {Mims : Interactive Multimedia Live Performance System},
  Author                   = {Kanda, Ryo and Hashida, Mitsuyo and Katayose, Haruhiro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {45--47},
  Keywords                 = {Interaction, audience, performer, visualize, sen- sor, physical, gesture. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_045.pdf},
  DOI                      = {10.5281/zenodo.1177595}
}

@InProceedings{Kapuscinski2009,
  Title                    = {Interfacing Graphic and Musical Elements in Counterlines},
  Author                   = {Kapuscinski, Jaroslaw and Sanchez, Javier},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {222--225},
  Abstract                 = {This paper reports on initial stages of research leading to the development of an intermedia performance Counterlines --- a duet for Disklavier and Wacom Cintiq, in which both performers generate audiovisual gestures that relate to each other contrapuntally. The pianist generates graphic elements while playing music and the graphic performer generates piano notes by drawing lines. The paper focuses on interfacing sounds and images performed by the pianist. It provides rationale for the choice of materials of great simplicity and describes our approach to mapping. },
  Keywords                 = {intermedia, Disklavier, piano, Wacom Cintiq, mapping, visual music },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_222.pdf},
  DOI                      = {10.5281/zenodo.1177597}
}

@InProceedings{Keith2009,
  Title                    = {Controlling Live Generative Electronic Music with Deviate},
  Author                   = {Keith, Sarah},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {54--55},
  Abstract                 = {Deviate generates multiple streams of melodic and rhythmic output in real-time, according to user-specified control parameters. This performance system has been implemented using Max 5 [1] within the genre of popular contemporary electronic music, incorporating techno, IDM, and related forms. The aim of this project is not musical style synthesis, but to construct an environment in which a range of creative and musical goals may be achieved. A key aspect is control over generative processes, as well as consistent yet varied output. An approach is described which frees the user from determining note-level output while allowing control to be maintained over larger structural details, focusing specifically on the melodic aspect of this system. Audio examples are located online at http://www.cetenbaath.com/cb/about-deviate/. },
  Keywords                 = {generative, performance, laptop, popular music },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_054.pdf},
  DOI                      = {10.5281/zenodo.1177599}
}

@InProceedings{Kellum2009,
  Title                    = {A Flexible Mapping Editor for Multi-touch Musical Instruments},
  Author                   = {Kellum, Greg and Crevoisier, Alain},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {242--245},
  Abstract                 = {This paper introduces a flexible mapping editor, which transforms multi-touch devices into musical instruments. The editor enables users to create interfaces by dragging and dropping components onto the interface and attaching actions to them, which will be executed when certain userdefined conditions obtain. The editor receives touch information via the non-proprietary communication protocol, TUIO [9], and can, therefore, be used together with a variety of different multi-touch input devices. },
  Keywords                 = {NIME, multi-touch, multi-modal interface, sonic interaction design. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_242.pdf},
  DOI                      = {10.5281/zenodo.1177601}
}

@InProceedings{Kiefer2009,
  Title                    = {Phalanger : Controlling Music Software With Hand Movement Using A Computer Vision and Machine Learning Approach},
  Author                   = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {246--249},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_246.pdf},
  DOI                      = {10.5281/zenodo.1177603}
}

@InProceedings{Kirk2009,
  Title                    = {The Argus Project : Underwater Soundscape Composition with Laser- Controlled Modulation},
  Author                   = {Kirk, Jonathon and Weisert, Lee},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {290--292},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_290.pdf},
  DOI                      = {10.5281/zenodo.1177605}
}

@InProceedings{Lahdeoja2009,
  Title                    = {Augmenting Chordophones with Hybrid Percussive Sound Possibilities},
  Author                   = {L\''{a}hdeoja, Otso},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {102--105},
  Abstract                 = {In this paper we describe an approach for introducing newelectronic percussive sound possibilities for stringinstruments by "listening" to the sounds of the instrument'sbody and extracting audio and data from the wood'sacoustic vibrations. A method for capturing, localizing andanalyzing the percussive hits on the instrument's body ispresented, in connection with an audio-driven electronicpercussive sound module. The system introduces a newgesture-sound relationship in the electric string instrumentplaying environment, namely the use of percussivetechniques on the instrument's body which are null inregular circumstances due to selective and exclusivemicrophone use for the strings. Instrument bodypercussions are widely used in the acoustic instrumentalpraxis. They yield a strong potential for providing anextended soundscape via instrument augmentation, directlycontrolled by the musician through haptic manipulation ofthe instrument itself. The research work was carried out onthe electric guitar, but the method used can apply to anystring instrument with a resonating body.},
  Keywords                 = {augmented instrument,chordophone,contact microphone systems,electric,electronic percussion,even with,guitar,leaving the instrument body,nime09,there is always a,trade-off,virtually mute},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_102.pdf},
  DOI                      = {10.5281/zenodo.1177607}
}

@InProceedings{Lai2009,
  Title                    = {Hands On Stage : A Sound and Image Performance Interface},
  Author                   = {Lai, Chi-Hsia},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {39--40},
  Abstract                 = {Hands On Stage, designed from a percussionist's perspective, is a new performance interface designed for audiovisual improvisation. It comprises a custom-built table interface and a performance system programmed in two environments, SuperCollider 3 and Isadora. This paper traces the interface's evolution over matters of relevant technology, concept, construction, system design, and its creative outcomes. },
  Keywords                 = {audiovisual, interface design, performance. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_039.pdf},
  DOI                      = {10.5281/zenodo.1177609}
}

@InProceedings{Laurenzo2009,
  Title                    = {YARMI : an Augmented Reality Musical Instrument},
  Author                   = {Laurenzo, Tomas and Rodr\'{\i}guez, Ernesto and Castro, Juan Fabrizio},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {268--269},
  Keywords                 = {Interactive music instruments, visual interfaces, visual feedback, tangible interfaces, augmented reality, collaborative music, networked musical instruments, real-time musical systems, musical sequencer. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_268.pdf},
  DOI                      = {10.5281/zenodo.1177611}
}

@InProceedings{Leeuw2009,
  Title                    = {The Electrumpet , a Hybrid Electro-Acoustic Instrument},
  Author                   = {Leeuw, Hans},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {193--198},
  Abstract                 = {The Electrumpet is an enhancement of a normal trumpet with a variety of electronic sensors and buttons. It is a new hybrid instrument that facilitates simultaneous acoustic and electronic playing. The normal playing skills of a trumpet player apply to the new instrument. The placing of the buttons and sensors is not a hindrance to acoustic use of the instrument and they are conveniently located. The device can be easily attached to and detached from a normal Bb-trumpet. The device has a wireless connection with the computer through Bluetooth-serial (Arduino). Audio and data processing in the computer is effected by three separate instances of MAX/MSP connected through OSC (controller data) and Soundflower (sound data). The current prototype consists of 7 analogue sensors (4 valve-like potentiometers, 2 pressure sensors, 1 "Ribbon" controller) and 9 digital switches. An LCD screen that is controlled by a separate Arduino (mini) is attached to the trumpet and displays the current controller settings that are sent through a serial connection. },
  Keywords                 = {Trumpet, multiple Arduinos, Bluetooth, LCD, low latency, OSC, MAX/MSP. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_193.pdf},
  DOI                      = {10.5281/zenodo.1177613}
}

@InProceedings{Leider2009,
  Title                    = {Afflux/Reflux},
  Author                   = {Leider, Colby},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {333--333},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_333.pdf},
  DOI                      = {10.5281/zenodo.1177615}
}

@InProceedings{Leider2009a,
  Title                    = {The elBo and footPad : Toward Personalized Hardware for Audio Manipulation},
  Author                   = {Leider, Colby and Mann, Doug and Plazas, Daniel and Battaglia, Michael and Draper, Reid},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {147--148},
  Abstract                 = {We describe initial prototypes and a design strategy for new, user-customized audio-manipulation and editing tools. These tools are designed to enable intuitive control of audio-processing tasks while anthropomorphically matching the target user. },
  Keywords                 = {user modeling, user customization },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_147.pdf},
  DOI                      = {10.5281/zenodo.1177617}
}

@InProceedings{Lieberman2009,
  Title                    = {Anigraphical Etude 9},
  Author                   = {Lieberman, David},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {321--321},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_321.pdf},
  DOI                      = {10.5281/zenodo.1177619}
}

@InProceedings{Lyon2009,
  Title                    = {Biomuse Trio},
  Author                   = {Lyon, Eric and Knapp, Benjamin and Ouzounian, Gascia},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {327--327},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_327.pdf},
  DOI                      = {10.5281/zenodo.1177621}
}

@InProceedings{Muller2009,
  Title                    = {Utilizing Tactile Feedback to Guide Movements Between Sounds},
  Author                   = {M\''{u}ller, Alexander and Essl, Georg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {33--34},
  Abstract                 = {Vibetone is a musical input device which was build to explore tactile feedback in gesture based interaction. It is a prototype aimed to allow the performer to play both continuously and discrete pitched sounds in the same space. Our primary focus is on tactile feedback to guide the artist's movements during his performance. Thus, also untrained users are enabled to musical expression through bodily actions and precisely arm movements, guided through tactile feedback signals. },
  Keywords                 = {tactile feedback, intuitive interaction, gestural interaction, MIDI controller },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_033.pdf},
  DOI                      = {10.5281/zenodo.1177623}
}

@InProceedings{Mann2009,
  Title                    = {The Tactus : a Tangible , Rhythmic Grid Interface Using Found-Objects},
  Author                   = {Mann, Yotam and Lubow, Jeff and Freed, Adrian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {86--89},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_086.pdf},
  DOI                      = {10.5281/zenodo.1177625}
}

@InProceedings{McDonald2009,
  Title                    = {The Vibrobyte : A Haptic Interface for Co-Located Performance},
  Author                   = {McDonald, Kyle and Kouttron, Dane and Bahn, Curtis and Braasch, Jonas and Oliveros, Pauline},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {41--42},
  Abstract                 = {The Vibrobyte is a wireless haptic interface specialized forco-located musical performance. The hardware is designedaround the open source Arduino platform, with haptic control data encapsulated in OSC messages, and OSC/hardwarecommunications handled by Processing. The Vibrobyte wasfeatured at the International Computer Music Conference2008 (ICMC) in a telematic performance between ensembles in Belfast, Palo Alto (California, USA), and Troy (NewYork, USA).},
  Keywords                 = {haptics,interface,nime09,performance,telematic},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_041.pdf},
  DOI                      = {10.5281/zenodo.1177627}
}

@InProceedings{Mclean2009,
  Title                    = {Words , Movement and Timbre},
  Author                   = {Mclean, Alex and Wiggins, Geraint},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {276--279},
  Abstract                 = {Phonetic symbols describe movements of the vocal tract,tongue and lips, and are combined into complex movementsforming the words of language. In music, vocables are wordsthat describe musical sounds, by relating vocal movementsto articulations of a musical instrument. We posit that vocable words allow the composers and listeners to engageclosely with dimensions of timbre, and that vocables couldsee greater use in electronic music interfaces. A preliminarysystem for controlling percussive physical modelling synthesis with textual words is introduced, with particular application in expressive specification of timbre during computer music performances.},
  Keywords                 = {nime09,timbre,vocable synthesis},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_276.pdf},
  DOI                      = {10.5281/zenodo.1177629}
}

@InProceedings{Min2009,
  Title                    = {SORISU : Sound with Numbers},
  Author                   = {Min, Hye Ki},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {82--85},
  Abstract                 = {It is surely not difficult for anyone with experience in thesubject known as Music Theory to realize that there is avery definite and precise relationship between music andmathematics. This paper describes the SoriSu, a newelectronic musical instrument based on Sudoku puzzles,which probe the expressive possibilities of mathematicalconcepts in music. The concept proposes a new way ofmapping numbers to sound. This interface was designed toprovide easy and pleasing access to music for users whoare unfamiliar or uncomfortable with current musicaldevices. The motivation behind the project is presented, aswell as hardware and software design.},
  Keywords                 = {Numbers, Game Interfaces, Mathematics and Sound, Mathematics in Music, Puzzles, Tangible User Interfaces. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_082.pdf},
  DOI                      = {10.5281/zenodo.1177631}
}

@InProceedings{Miyama2009,
  Title                    = {Angry Sparrow},
  Author                   = {Miyama, Chikashi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {326--326},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_326.pdf},
  DOI                      = {10.5281/zenodo.1177633}
}

@InProceedings{Nagashima2009,
  Title                    = {Parallel Processing System Design with "Propeller" Processor},
  Author                   = {Nagashima, Yoichi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {169--170},
  Abstract                 = {This is a technical and experimental report of parallel processing, using the "Propeller" chip. Its eight 32 bits processors (cogs) can operate simultaneously, either independently or cooperatively, sharing common resources through a central hub. I introduce this unique processor and discuss about the possibility to develop interactive systems and smart interfaces in media arts, because we need many kinds of tasks at a same time with NIMErelated systems and installations. I will report about (1) Propeller chip and its powerful IDE, (2) external interfaces for analog/digital inputs/outputs, (3) VGA/NTSC/PAL video generation, (4) audio signal processing, and (5) originally-developed MIDI input/output method. I also introduce three experimental prototype systems.},
  Keywords                 = {Propeller, parallel processing, MIDI, sensor, interfaces. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_169.pdf},
  DOI                      = {10.5281/zenodo.1177635}
}

@InProceedings{Nakra2009,
  Title                    = {The UBS Virtual Maestro : an Interactive Conducting System},
  Author                   = {Nakra, Teresa M. and Ivanov, Yuri and Smaragdis, Paris and Ault, Chris},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {250--255},
  Abstract                 = {The UBS Virtual Maestro is an interactive conducting system designed by Immersion Music to simulate the experience of orchestral conducting for the general public attending a classical music concert. The system utilizes the Wii Remote, which users hold and move like a conducting baton to affect the tempo and dynamics of an orchestral video/audio recording. The accelerometer data from the Wii Remote is used to control playback speed and volume in real-time. The system is housed in a UBSbranded kiosk that has toured classical performing arts venues throughout the United States and Europe in 2007 and 2008. In this paper we share our experiences in designing this standalone system for thousands of users, and lessons that we learned from the project. },
  Keywords                 = {conducting, gesture, interactive installations, Wii Remote },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_250.pdf},
  DOI                      = {10.5281/zenodo.1177637}
}

@InProceedings{Neill2009,
  Title                    = {Ben Neill and LEMUR},
  Author                   = {Neill, Ben and Singer, Eric},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {331--331},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_331.pdf},
  DOI                      = {10.5281/zenodo.1177639}
}

@InProceedings{Nicolls2009,
  Title                    = {Twenty-First Century Piano},
  Author                   = {Nicolls, Sarah},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {203--206},
  Keywords                 = {sensor, gestural, technology, performance, piano, motors, interactive },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_203.pdf},
  DOI                      = {10.5281/zenodo.1177641}
}

@InProceedings{Nishino2009,
  Title                    = {A {2D} Fiducial Tracking Method based on Topological Region Adjacency and Angle Information},
  Author                   = {Nishino, Hiroki},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {62--63},
  Abstract                 = {We describe a new method for 2D fiducial tracking. We use region adjacency information together with angles between regions to encode IDs inside fiducials, whereas previous research by Kaltenbrunner and Bencina utilize region adjacency tree. Our method supports a wide ID range and is fast enough to accommodate real-time video. It is also very robust against false positive detection. },
  Keywords                 = {fiducial tracking, computer vision, tangible user interface, interaction techniques. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_062.pdf},
  DOI                      = {10.5281/zenodo.1177643}
}

@InProceedings{Nymoen2009,
  Title                    = {A Discussion of Multidimensional Mapping in Nymophone2},
  Author                   = {Nymoen, Kristian and Jensenius, Alexander R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {94--97},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_094.pdf},
  DOI                      = {10.5281/zenodo.1177645}
}

@InProceedings{Ogawa2009,
  Title                    = {Life Game Orchestra as an Interactive Music Composition System Translating Cellular Patterns of Automata into Musical Scales},
  Author                   = {Ogawa, Keisuke and Kuhara, Yasuo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {50--51},
  Abstract                 = {We developed a system called Life Game Orchestra that generates music by translating cellular patterns of Conway's Game of Life into musical scales. A performer can compose music by controlling varying cell patterns and sounds with visual and auditory fun. A performer assigns the elements of tone to two-dimensional cell patterns in the matrix of the Game of Life. Our system searches defined cell patterns in the varying matrix dynamically. If the patterns are matched, corresponding tones are generated. A performer can make cells in the matrix by moving in front of a camera and interactively influencing the generation of music. The progress of the Game of Life is controlled with a clock defined by the performer to configure the groove of the music. By running multiple matrices with different pattern mapping, clock timing, and instruments, we can perform an ensemble. The Life Game Orchestra is a fusion system of the design of a performer and the emergence of cellular automata as a complex system. },
  Keywords                 = {Conway's Game of Life, Cellular automata, Cell pattern, scale, Interactive composition, performance. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_050.pdf},
  DOI                      = {10.5281/zenodo.1177647}
}

@InProceedings{Overholt2009,
  Title                    = {Pendaphonics},
  Author                   = {Overholt, Dan and Lahey, Byron and Skriver Hansen, Anne-Marie and Burleson, Winslow and Norrgaard Jensen, Camilla},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {339--339},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_339.pdf},
  DOI                      = {10.5281/zenodo.1177649}
}

@InProceedings{Paine2009,
  Title                    = {Fue Sho -- Electrofusion},
  Author                   = {Paine, Garth and Atherton, Michael},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {324--324},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_324.pdf},
  DOI                      = {10.5281/zenodo.1177651}
}

@InProceedings{Parson2009,
  Title                    = {Chess-Based Composition and Improvisation for Non-Musicians},
  Author                   = {Parson, Dale E.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {157--158},
  Abstract                 = {''Music for 32 Chess Pieces'' is a software system that supports composing, performing and improvising music by playing a chess game. A game server stores a representation of the state of a game, validates proposed moves by players, updates game state, and extracts a graph of piece-to-piece relationships. It also loads a plugin code module that acts as a composition. A plugin maps pieces and relationships on the board, such as support or attack relationships, to a timed sequence of notes and accents. The server transmits notes in a sequence to an audio renderer process via network datagrams. Two players can perform a composition by playing chess, and a player can improvise by adjusting a plugin's music mapping parameters via a graphical user interface. A composer can create a new composition by writing a new plugin that uses a distinct algorithm for mapping game rules and states to music. A composer can also write a new note-to-sound mapping program in the audio renderer language. This software is available at http://faculty.kutztown.edu/parson/music/ParsonMusic.html. },
  Keywords                 = {algorithmic composition, chess, ChucK, improvisation, Max/MSP, SuperCollider. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_157.pdf},
  DOI                      = {10.5281/zenodo.1177653}
}

@InProceedings{Partridge2009,
  Title                    = {Let Loose with WallBalls, a Collaborative Tabletop Instrument for Tomorrow},
  Author                   = {Partridge, Grant and Irani, Pourang and Fitzell, Gordon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {78--81},
  Keywords                 = {Tabletop computers, collaborative instruments, collaborative composition, group improvisation, spatial au- dio interfaces, customizable instruments. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_078.pdf},
  DOI                      = {10.5281/zenodo.1177655}
}

@InProceedings{Pedrosa2009,
  Title                    = {Evaluation of {3D} Haptic Target Rendering to Support Timing in Music Tasks},
  Author                   = {Pedrosa, Ricardo and Maclean, Karon E.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {19--24},
  Abstract                 = {Haptic feedback is an important element that needs to be carefully designed in computer music interfaces. This paper presents an evaluation of several force renderings for target acquisition in space when used to support a music related task. The study presented here addresses only one musical aspect: the need to repeat elements accurately in time and in content. Several force scenarios will be rendered over a simple 3D target acquisition task and users' performance will be quantitatively and qualitatively evaluated. The results show how the users' subjective preference for a particular kind of force support does not always correlate to a quantitative measurement of performance enhancement. We describe a way in which a control mapping for a musical interface could be achieved without contradicting the users' preferences as obtained from the study. },
  Keywords                 = {music interfaces, force feedback, tempo, comfort, target acquisition. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_019.pdf},
  DOI                      = {10.5281/zenodo.1177657}
}

@InProceedings{Peng2009,
  Title                    = {A Wii-Based Gestural Interface for Computer Conducting Systems},
  Author                   = {Peng, Lijuan and Gerhard, David},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {155--156},
  Abstract                 = {With the increase of sales of Wii game consoles, it is becoming commonplace for the Wii remote to be used as analternative input device for other computer systems. In thispaper, we present a system which makes use of the infraredcamera within the Wii remote to capture the gestures of aconductor using a baton with an infrared LED and battery.Our system then performs data analysis with gesture classification and following, and finally displays the gestures using visual baton trajectories and audio feedback. Gesturetrajectories are displayed in real time and can be comparedto the corresponding diagram shown in a textbook. In addition, since a conductor normally does not look at a screenwhile conducting, tones are played to represent a certainbeat in a conducting gesture. Further, the system can be controlled entirely with the baton, removing the need to switchfrom baton to mouse. The interface is intended to be usedfor pedagogy purposes.},
  Keywords                 = {Conducting, Gesture, Infrared, Learning, Wii. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_155.pdf},
  DOI                      = {10.5281/zenodo.1177659}
}

@InProceedings{Polfreman2009,
  Title                    = {FrameWorks {3D} : Composition in the Third Dimension},
  Author                   = {Polfreman, Richard},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {226--229},
  Abstract                 = {Music composition on computer is a challenging task, involving a range of data types to be managed within a single software tool. A composition typically comprises a complex arrangement of material, with many internal relationships between data in different locations repetition, inversion, retrograde, reversal and more sophisticated transformations. The creation of such complex artefacts is labour intensive, and current systems typically place a significant cognitive burden on the composer in terms of maintaining a work as a coherent whole. FrameWorks 3D is an attempt to improve support for composition tasks within a Digital Audio Workstation (DAW) style environment via a novel three-dimensional (3D) user-interface. In addition to the standard paradigm of tracks, regions and tape recording analogy, FrameWorks displays hierarchical and transformational information in a single, fully navigable workspace. The implementation combines Java with Max/MSP to create a cross-platform, user-extensible package and will be used to assess the viability of such a tool and to develop the ideas further. },
  Keywords                 = {Digital Audio Workstation, graphical user- interfaces, 3D graphics, Max/MSP, Java. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_226.pdf},
  DOI                      = {10.5281/zenodo.1177661}
}

@InProceedings{Reben2009,
  Title                    = {Responsive Music Interfaces for Performance},
  Author                   = {Reben, Alexander and Laibowitz, Mat and Paradiso, Joseph A.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {37--38},
  Abstract                 = {In this project we have developed reactive instruments for performance. Reactive instruments provide feedback for the performer thereby providing a more dynamic experience. This is achieved through the use of haptics and robotics. Haptics provide a feedback system to the control surface. Robotics provides a way to actuate the instruments and their control surfaces. This allows a highly coordinated "dance" between performer and the instrument. An application for this idea is presented as a linear slide interface. Reactive interfaces represent a dynamic way for music to be portrayed in performance. },
  Keywords                 = {haptics, robotics, dynamic interfaces },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_037.pdf},
  DOI                      = {10.5281/zenodo.1177663}
}

@InProceedings{Rogers2009,
  Title                    = {Study no. 1 for {PAM} and MADI},
  Author                   = {Rogers, Troy and Kemper, Steven and Barton, Scott},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {323--323},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_323.pdf},
  DOI                      = {10.5281/zenodo.1177665}
}

@InProceedings{Schacher2009,
  Title                    = {Action and Perception in Interactive Sound Installations : An Ecological Approach},
  Author                   = {Schacher, Jan C.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {286--289},
  Abstract                 = {In this paper mappings and adaptation in the context of interactive sound installations are discussed. Starting from an ecological perspective on non-expert audience interaction a brief overview and discussion of mapping strategies with a special focus on adaptive systems using machine learning algorithms is given. An audio-visual interactive installation is analyzed and its implementation used to illustrate the issues of audience engagement and to discuss the efficiency of adaptive mappings. },
  Keywords                 = {Interaction, adaptive mapping, machine learning, audience engagement },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_286.pdf},
  DOI                      = {10.5281/zenodo.1177667}
}

@InProceedings{Schiesser2009,
  Title                    = {midOSC : a Gumstix-Based MIDI-to-OSC Converter},
  Author                   = {Schiesser, S\'{e}bastien},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {165--168},
  Abstract                 = {A MIDI-to-OSC converter is implemented on a commercially available embedded linux system, tighly integratedwith a microcontroller. A layered method is developed whichpermits the conversion of serial data such as MIDI to OSCformatted network packets with an overall system latencybelow 5 milliseconds for common MIDI messages.The Gumstix embedded computer provide an interesting and modular platform for the development of such anembedded applications. The project shows great potentialto evolve into a generic sensors-to-OSC ethernet converterwhich should be very useful for artistic purposes and couldbe used as a fast prototyping interface for gesture acquisitiondevices.},
  Keywords                 = {MIDI, Open Sound Control, converter, gumstix },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_165.pdf},
  DOI                      = {10.5281/zenodo.1177669}
}

@InProceedings{Schlessinger2009,
  Title                    = {The Kalichord : A Physically Modeled Electro-Acoustic Plucked String Instrument},
  Author                   = {Schlessinger, Daniel and Smith, Julius O.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {98--101},
  Abstract                 = { We present the Kalichord: a small, handheld electro/acoustic instrument in which the player's right hand plucks virtual strings while his left hand uses buttons to play independent bass lines. The Kalichord uses the analog signal from plucked acoustic tines to excite a physical string model, allowing a nuanced and intuitive plucking experience. First, we catalog instruments related to the Kalichord. Then we examine the use of analog signals to excite a physical string model and discuss the expressiveness and form factors that this technique affords. We then describe the overall construction of the Kalichord and possible playing styles, and finally we consider ways we hope to improve upon the current prototype. },
  Keywords                 = {Kalichord, physical model, tine, piezo, plucked string, electro-acoustic instruments, kalimba, accordion },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_098.pdf},
  DOI                      = {10.5281/zenodo.1177671}
}

@InProceedings{Schmeder2009,
  Title                    = {A Low-level Embedded Service Architecture for Rapid DIY Design of Real-time Musical Instruments},
  Author                   = {Schmeder, Andrew and Freed, Adrian},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {121--124},
  Abstract                 = {An on-the-fly reconfigurable low-level embedded servicearchitecture is presented as a means to improve scalability, improve conceptual comprehensibility, reduce humanerror and reduce development time when designing newsensor-based electronic musical instruments with real-timeresponsiveness. The implementation of the concept ina project called micro-OSC is described. Other sensorinterfacing products are evaluated in the context of DIYprototyping of musical instruments. The capabilities ofthe micro-OSC platform are demonstrated through a set ofexamples including resistive sensing, mixed digital-analogsystems, many-channel sensor interfaces and time-basedmeasurement methods.},
  Keywords                 = {real-time musical interface, DIY design, em- bedded web services, rapid prototyping, reconfigurable firmware },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_121.pdf},
  DOI                      = {10.5281/zenodo.1177673}
}

@InProceedings{Siwiak2009,
  Title                    = {Catch Your Breath --- Musical Biofeedback for Breathing Regulation},
  Author                   = {Siwiak, Diana and Berger, Jonathan and Yang, Yao},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {153--154},
  Abstract                 = {Catch Your Breath is an interactive audiovisual bio-feedbacksystem adapted from a project designed to reduce respiratory irregularity in patients undergoing 4D CT scans for oncological diagnosis. The system is currently implementedand assessed as a potential means to reduce motion-induceddistortion in CT images.A museum installation based on the same principle wascreated in which an inexpensive wall-mounted web camera tracks an IR sensor embedded into a pendant worn bythe user. The motion of the subjects breathing is trackedand interpreted as a real-time variable tempo adjustment toa stored musical file. The subject can then adjust his/herbreathing to synchronize with a separate accompanimentline. When the breathing is regular and is at the desiredtempo, the audible result sounds synchronous and harmonious. The accompaniment's tempo progresses and gradually decrease which causes the breathing to synchronize andslow down, thus increasing relaxation.},
  Keywords                 = {sensor, music, auditory display. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_153.pdf},
  DOI                      = {10.5281/zenodo.1177675}
}

@InProceedings{Smallwood2009,
  Title                    = {Sound Lanterns},
  Author                   = {Smallwood, Scott},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {340--340},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_340.pdf},
  DOI                      = {10.5281/zenodo.1177677}
}

@InProceedings{Smallwood2009a,
  Title                    = {Don't Forget the Loudspeaker --- A History of Hemispherical Speakers at Princeton , Plus a DIY Guide},
  Author                   = {Smallwood, Scott and Cook, Perry R. and Trueman, Dan and McIntyre, Lawrence},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {110--115},
  Abstract                 = {This paper gives a historical overview of the development of alternative sonic display systems at Princeton University; in particular, the design, construction, and use in live performance of a series of spherical and hemispherical speaker systems. We also provide a DIY guide to constructing the latest series of loudspeakers that we are currently using in our research and music making. },
  Keywords                 = {loudspeakers, hemispherical speakers, sonic display systems, laptop orchestras. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_110.pdf},
  DOI                      = {10.5281/zenodo.1177679}
}

@InProceedings{Solis2009,
  Title                    = {Anthropomorphic Musical Performance Robots at Waseda University : Increasing Understanding of the Nature of Human Musical Interaction Abstract},
  Author                   = {Solis, Jorge and Ninomiya, Takeshi and Petersen, Klaus and Takeuchi, Masaki and Takanishi, Atsuo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {64--69},
  Abstract                 = {During several decades, the research at Waseda University has been focused on developing anthropomorphic robots capable performing musical instruments. As a result of our research efforts, the Waseda Flutist Robot WF-4RIV and the Waseda Saxophonist Robot WAS-1 have been designed to reproduce the human player performance. As a long-term goal, we are proposing to enable the interaction between musical performance robots as well as with human players. In general the communication of humans within a band is a special case of conventional human social behavior. Rhythm, harmony and timbre of the music played represent the emotional states of the musicians. So the development of an artificial entity that participates in such an interaction may contribute to the better understanding of some of the mechanisms that enable the communication of humans in musical terms. Therefore, we are not considering a musical performance robot (MPR) just as a mere sophisticated MIDI instrument. Instead, its human-like design and the integration of perceptual capabilities may enable to act on its own autonomous initiative based on models which consider its own physical constrains. In this paper, we present an overview of our research approaches towards enabling the interaction between musical performance robots as well as with musicians. },
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_064.pdf},
  DOI                      = {10.5281/zenodo.1177681}
}

@InProceedings{Spowage2009,
  Title                    = {The Ghetto Bastard : A Portable Noise Instrument},
  Author                   = {Spowage, Neal},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {29--30},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_029.pdf},
  DOI                      = {10.5281/zenodo.1177683}
}

@InProceedings{StClair2009,
  Title                    = {PlaySoundGround : An Interactive Musical Playground},
  Author                   = {St. Clair, Michael and Leitman, Sasha},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {293--296},
  Keywords                 = {expression,in doing so,installation,interactive,it also opens up,music,new territories for,nime09,play,playful physical motions into,playground,radical collaboration,real-time,the realm of artistic},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_293.pdf},
  DOI                      = {10.5281/zenodo.1177685}
}

@InProceedings{Stearns2009,
  Title                    = {AANN: Artificial Analog Neural Network},
  Author                   = {Stearns, Phillip},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {341--341},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_341.pdf},
  DOI                      = {10.5281/zenodo.1177687}
}

@InProceedings{Steiner2009,
  Title                    = {Firmata : Towards Making Microcontrollers Act Like Extensions of the Computer},
  Author                   = {Steiner, Hans-Christoph},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {125--130},
  Abstract                 = {Firmata is a generic protocol for communicating with microcontrollers from software on a host computer. The central goal is to make the microcontroller an extension of theprogramming environment on the host computer in a manner that feels natural in that programming environment. Itwas designed to be open and flexible so that any programming environment can support it, and simple to implementboth on the microcontroller and the host computer to ensurea wide range of implementations. The current reference implementation is a library for Arduino/Wiring and is includedwith Arduino software package since version 0012. Thereare matching software modules for a number of languages,like Pd, OpenFrameworks, Max/MSP, and Processing.},
  Keywords                 = {arduino,microcontroller,nime09,processing,pure data},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_125.pdf},
  DOI                      = {10.5281/zenodo.1177689}
}

@InProceedings{Todoroff2009,
  Title                    = {''Extension du Corps Sonore'' --- Dancing Viola},
  Author                   = {Todoroff, Todor and Bettens, Fr\'{e}d\'{e}ric and Reboursi\`{e}re, Lo\''{\i}c and Chu, Wen-Yang},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {141--146},
  Abstract                 = {''Extension du corps sonore'' is long-term project initiatedby Musiques Nouvelles [4], a contemporary music ensemble in Mons. It aims at giving instrumental music performers an extended control over the sound of their instrument byextending the understanding of the sound body from the instrument only to the combination of the instrument and thewhole body of the performer. The development started atARTeM and got the benefit of a three month numediartresearch project [1] that focused on three axes of research:pre-processing of sensor data, gesture recognition and mapping through interpolation. The objectives were the development of computing methods and flexible Max/MSP externals to be later integrated in the ARTeM software framework for the concerts with viola player Dominica Eyckmans. They could be used in a variety of other artistic worksand will be made available on the numediart website [1],where more detailed information can be found in the Quarterly Progress Scientific Report #4.},
  Keywords                 = {Sensor data pre-processing, gesture recognition, mapping, interpolation, extension du corps sonore },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_141.pdf},
  DOI                      = {10.5281/zenodo.1177691}
}

@InProceedings{Toenjes2009,
  Title                    = {Natural Materials on Stage : Custom Controllers for Aesthetic Effect},
  Author                   = {Toenjes, John},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {52--53},
  Abstract                 = {This article describes the implications of design and materials of computer controllers used in the context of interactive dance performance. Size, shape, and layout all influence audience perception of the performer, and materials imply context for further interpretation of the interactive performance work. It describes the construction of the "Control/Recorder" and the "VideoLyre", two custom computer control surfaces made for Leonardo's Chimes, a work by Toenjes, Marchant and Smith, and how these controllers contribute to theatrical aesthetic intent. },
  Keywords                 = {control surface, interface, tactile, natural, organic, interactive dance. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_052.pdf},
  DOI                      = {10.5281/zenodo.1177693}
}

@InProceedings{Torre2009,
  Title                    = {MOLITVA --- Composition for Voice, Live Electronics, Pointing-At Glove Device and {3-D} Setup of Speakers},
  Author                   = {Torre, Giuseppe and Sazdov, Robert and Konczewska, Dorota},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {330--330},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_330.pdf},
  DOI                      = {10.5281/zenodo.1177695}
}

@InProceedings{Wang2009,
  Title                    = {Designing Smule's Ocarina : The iPhone's Magic Flute},
  Author                   = {Wang, Ge},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {303--307},
  Abstract                 = {The Smule Ocarina is a wind instrument designed for the iPhone, fully leveraging its wide array of technologies: microphone input (for breath input), multitouch (for fingering), accelerometer, real-time sound synthesis, highperformance graphics, GPS/location, and persistent data connection. In this mobile musical artifact, the interactions of the ancient flute-like instrument are both preserved and transformed via breath-control and multitouch finger-holes, while the onboard global positioning and persistent data connection provide the opportunity to create a new social experience, allowing the users of Ocarina to listen to one another. In this way, Ocarina is also a type of social instrument that enables a different, perhaps even magical, sense of global connectivity. },
  Keywords                 = {chuck,design,in,in real-time,interface,iphone,mobile music,multitouch,nime09,ocarina,pulsing waves,social,sonically and onscreen and,sound synthesis takes place,the breath is visualized},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_303.pdf},
  DOI                      = {10.5281/zenodo.1177697}
}

@InProceedings{Wang2009a,
  Title                    = {PLOrk Beat Science 2.0},
  Author                   = {Wang, Ge and Fiebrink, Rebecca},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {334--334},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_334.pdf},
  DOI                      = {10.5281/zenodo.1177699}
}

@InProceedings{Wechsler2009,
  Title                    = {The Oklo Phenomenon},
  Author                   = {Wechsler, Robert},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {320--320},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_320.pdf},
  DOI                      = {10.5281/zenodo.1177701}
}

@InProceedings{Weinberg2009,
  Title                    = {ZooZBeat : a Gesture-based Mobile Music Studio},
  Author                   = {Weinberg, Gil and Beck, Andrew and Godfrey, Mark},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {312--315},
  Abstract                 = {ZooZBeat is a gesture-based mobile music studio. It is designed to provide users with expressive and creative access to music making on the go. ZooZBeat users shake the phone or tap the screen to enter notes. The result is quantized, mapped onto a musical scale, and looped. Users can then use tilt and shake movements to manipulate and share their creation in a group. Emphasis is placed on finding intuitive metaphors for mobile music creation and maintaining a balance between control and ease-of-use that allows non-musicians to begin creating music with the application immediately. },
  Keywords                 = {mobile music, gestural control },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_312.pdf},
  DOI                      = {10.5281/zenodo.1177703}
}

@InProceedings{Weinberg2009a,
  Title                    = {The Creation of a Multi-Human, Multi-Robot Interactive Jam Session},
  Author                   = {Weinberg, Gil and Blosser, Brian and Mallikarjuna, Trishul and Raman, Aparna},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {70--73},
  Abstract                 = {This paper presents an interactive and improvisational jam session, including human players and two robotic musicians. The project was developed in an effort to create novel and inspiring music through human-robot collaboration. The jam session incorporates Shimon, a newly-developed socially-interactive robotic marimba player, and Haile, a perceptual robotic percussionist developed in previous work. The paper gives an overview of the musical perception modules, adaptive improvisation modes and human-robot musical interaction models that were developed for the session. The paper also addresses the musical output that can be created from increased interconnections in an expanded multiple-robot multiplehuman ensemble, and suggests directions for future work. },
  Keywords                 = {Robotic musicianship, Shimon, Haile. },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_070.pdf},
  DOI                      = {10.5281/zenodo.1177705}
}

@InProceedings{Wessel2009,
  Title                    = {Hands On --- A New Work from SLABS Controller and Generative Algorithms},
  Author                   = {Wessel, David},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {335--335},
  Keywords                 = {nime09},
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_335.pdf},
  DOI                      = {10.5281/zenodo.1177707}
}

@InProceedings{Wiley2009,
  Title                    = {Multi-Laser Gestural Interface --- Solutions for Cost-Effective and Open Source Controllers},
  Author                   = {Wiley, Meason and Kapur, Ajay},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2009},
  Address                  = {Pittsburgh, PA, United States},
  Pages                    = {43--44},
  Abstract                 = {This paper describes a cost-effective, modular, open source framework for a laser interface design that is open to community development, interaction and user modification. The following paper highlights ways in which we are implementing the multi-laser gestural interface in musical, visual, and robotic contexts. },
  Keywords                 = {Lasers, photocell sensor, UltraSound, Open Source controller design, digital gamelan, digital tanpura },
  Url                      = {http://www.nime.org/proceedings/2009/nime2009_043.pdf},
  DOI                      = {10.5281/zenodo.1177709}
}
