%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Alexander Refsum Jensenius at 2014-12-29 13:36:41 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Dahlstedt:2012,
	Abstract = {I present a novel low-tech multidimensional gestural con-troller, based on the resistive properties of a 2D field of pencil markings on paper. A set of movable electrodes (+, -, ground) made from soldered stacks of coins create a dynamic voltage potential field in the carbon layer, and an-other set of movable electrodes tap voltages from this field. These voltages are used to control complex sound engines in an analogue modular synthesizer. Both the voltage field and the tap electrodes can be moved freely. The design was inspired by previous research in complex mappings for advanced digital instruments, and provides a similarly dynamic playing environment for analogue synthesis. The interface is cheap to build, and provides flexible control over a large set of parameters. It is musically satisfying to play, and allows for a wide range of playing techniques, from wild exploration to subtle expressions. I also present an inven-tory of the available playing techniques, motivated by the interface design, musically, conceptually and theatrically. The performance aspects of the interface are also discussed. The interface has been used in a number of performances in Sweden and Japan in 2011, and is also used by other musicians.},
	Address = {Ann Arbor, Michigan},
	Author = {Palle Dahlstedt},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {gestural interface, 2d, analog synthesis, performance, improvisation},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Pencil Fields: An Expressive Low-Tech Performance Interface for Analog Synthesis},
	Url = {http://www.nime.org/proceedings/2012/nime2012_275.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_275.pdf}}

@inproceedings{Reboursiere:2012,
	Abstract = {In this paper we present a series of algorithms developed to detect the following guitar playing techniques : bend, hammer-on, pull-off, slide, palm muting and harmonic. Detection of playing techniques can be used to control exter-nal content (i.e audio loops and effects, videos, light events, etc.), as well as to write real-time score or to assist guitar novices in their learning process. The guitar used is a Godin Multiac with an under-saddle RMC hexaphonic piezo pickup (one pickup per string, i.e six mono signals).},
	Address = {Ann Arbor, Michigan},
	Author = {Lo{\"\i}c Reboursi{\`e}re and Otso L{\"a}hdeoja and Thomas Drugman and St{\'e}phane Dupont and C{\'e}cile Picard-Limpens and Nicolas Riche},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Guitar audio analysis, playing techniques, hexaphonic pickup, controller, augmented guitar},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Left and right-hand guitar playing techniques detection},
	Url = {http://www.nime.org/proceedings/2012/nime2012_213.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_213.pdf}}

@inproceedings{Vamvakousis:2012,
	Abstract = {In this paper we describe the EyeHarp, a new gaze-controlled musical instrument, and the new features we recently added to its design. In particular, we report on the EyeHarp new controls, the arpeggiator, the new remote eye-tracking device, and the EyeHarp capacity to act as a MIDI controller for any VST plugin virtual instrument. We conducted an evaluation of the EyeHarp Temporal accuracy by monitor-ing 10 users while performing a melody task, and comparing their gaze control accuracy with their accuracy using a com-puter keyboard. We report on the results of the evaluation.},
	Address = {Ann Arbor, Michigan},
	Author = {Zacharias Vamvakousis and Rafael Ramirez},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Eye-tracking systems, music interfaces, gaze interaction},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Temporal Control In the EyeHarp Gaze-Controlled Musical Interface},
	Url = {http://www.nime.org/proceedings/2012/nime2012_215.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_215.pdf}}

@inproceedings{Wang:2012,
	Abstract = {We have added a dynamic bio-mechanical mapping layer that contains a model of the human vocal tract with tongue muscle activations as input and tract geometry as output to a real time gesture controlled voice synthesizer system used for musical performance and speech research. Using this mapping layer, we conducted user studies comparing controlling the model muscle activations using a 2D set of force sensors with a position controlled kinematic input space that maps directly to the sound. Preliminary user evaluation suggests that it was more difficult to using force input but the resultant output sound was more intelligible and natural compared to the kinematic controller. This result shows that force input is a potentially feasible for browsing through a vowel space for an articulatory voice synthesis system, although further evaluation is required.},
	Address = {Ann Arbor, Michigan},
	Author = {Johnty Wang and Nicolas d'Alessandro and Sidney Fels and Robert Pritchard},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Gesture, Mapping, Articulatory, Speech, Singing, Synthesis},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Investigation of Gesture Controlled Articulatory Vocal Synthesizer using a Bio-Mechanical Mapping Layer},
	Url = {http://www.nime.org/proceedings/2012/nime2012_291.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_291.pdf}}

@inproceedings{Shear:2012,
	Abstract = {The Electromagnetically Sustained Rhodes Piano is an orig-inal Rhodes Piano modified to provide control over the amplitude envelope of individual notes through aftertouch pressure. Although there are many opportunities to shape the amplitude envelope before loudspeaker amplification, they are all governed by the ever-decaying physical vibra-tions of the tone generating mechanism. A single-note proof of concept for electromagnetic control over this vibrating mechanism was presented at NIME 2011.
In the past year, virtually every aspect of the system has been improved. We use a different vibration sensor that is immune to electromagnetic interference, thus eliminat-ing troublesome feedback. For control, we both reduce cost and gain continuous position sensing throughout the entire range of key motion in addition to aftertouch pressure. Finally, the entire system now fits within the space constraints presented by the original piano, allowing it to be installed on adjacent notes.},
	Address = {Ann Arbor, Michigan},
	Author = {Greg Shear and Matthew Wright},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Rhodes, piano, mechanical synthesizer, electromagnetic, sustain, feedback},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Further Developments in the Electromagnetically Sustained Rhodes Piano},
	Url = {http://www.nime.org/proceedings/2012/nime2012_284.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_284.pdf}}

@inproceedings{Menzies:2012,
	Abstract = {The Highland piping tradition requires the performer to learn and accurately reproduce a diverse array of ornaments, which can be a daunting prospect to the novice piper. This paper presents a system which analyses a player's technique using sensor data obtained from an electronic bagpipe chanter interface. Automatic recognition of a broad range of piping embellishments allows real-time visual feedback to be generated, enabling the learner to ensure that they are practicing each movement correctly.
The electronic chanter employs a robust and responsive infrared (IR) sensing strategy, and uses audio samples from acoustic recordings to produce a high quality bagpipe sound. Moreover, the continuous nature of the IR sensors offers the controller a considerable degree of flexibility, indicating sig-nificant potential for the inclusion of extended and novel techniques for musical expression in the future.},
	Address = {Ann Arbor, Michigan},
	Author = {Duncan Menzies and Andrew McPherson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Great Highland Bagpipe, continuous infrared sensors, ornament recognition, practice tool, SuperCollider, OSC.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {An Electronic Bagpipe Chanter for Automatic Recognition of Highland Piping Ornamentation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_200.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_200.pdf}}

@inproceedings{Britt:2012,
	Abstract = {The EMvibe is an augmented vibraphone that allows for continuous control over the amplitude and spectrum of in-dividual notes. The system uses electromagnetic actuators to induce vibrations in the vibraphone's aluminum tone bars. The tone bars and the electromagnetic actuators are coupled via neodymium magnets affixed to each bar. The acoustic properties of the vibraphone allowed us to develop a very simple, low-cost and powerful amplification solution that requires no heat sinking. The physical design is meant to be portable and robust, and the system can be easily installed on any vibraphone without interfering with normal performance techniques. The system supports multiple in-terfacing solutions, affording the performer and composer the ability to interact with the EMvibe in different ways depending on the musical context.},
	Address = {Ann Arbor, Michigan},
	Author = {N. Cameron Britt and Jeff Snyder and Andrew McPherson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Vibraphone, augmented instrument, electromagnetic actuation},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The EMvibe: An Electromagnetically Actuated Vibraphone},
	Url = {http://www.nime.org/proceedings/2012/nime2012_101.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_101.pdf}}

@inproceedings{Mitchell:2012,
	Abstract = {This paper presents a toolbox of gestural control mechanisms which are available when the input sensing apparatus is a pair of data gloves fitted with orientation sensors. The toolbox was developed in advance of a live music performance in which the mapping from gestural input to audio output was to be developed rapidly in collaboration with the performer. The paper begins with an introduction to the associated literature before introducing a range of continuous, discrete and combined control mechanisms, enabling a flexible range of mappings to be explored and modified easily. An application of the toolbox within a live music performance is then described with an evaluation of the system with ideas for future developments.},
	Address = {Ann Arbor, Michigan},
	Author = {Thomas Mitchell and Sebastian Madgwick and Imogen Heap},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Computer Music, Gestural Control, Data Gloves},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Musical Interaction with Hand Posture and Orientation: A Toolbox of Gestural Control Mechanisms},
	Url = {http://www.nime.org/proceedings/2012/nime2012_272.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_272.pdf}}

@inproceedings{Gillian:2012,
	Abstract = {This paper presents Digito, a gesturally controlled virtual musical instrument. Digito is controlled through a number of intricate hand gestures, providing both discrete and continuous control of Digito's sound engine; with the fine-grain hand gestures captured by a 3D depth sensor and recognized using computer vision and machine learning algorithms. We describe the design and initial iterative development of Digito, the hand and finger tracking algorithms and gesture recognition algorithms that drive the system, and report the insights gained during the initial development cycles and user testing of this gesturally controlled virtual musical instrument.},
	Address = {Ann Arbor, Michigan},
	Author = {Nicholas Gillian and Joseph A. Paradiso},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Gesture Recognition, Virtual Musical Instrument},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Digito: A Fine-Grain Gesturally Controlled Virtual Musical Instrument},
	Url = {http://www.nime.org/proceedings/2012/nime2012_248.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_248.pdf}}

@inproceedings{Park:2012,
	Abstract = {This paper describes an interactive gestural microphone for vocal performance named Voicon. Voicon is a non-invasive and gesture-sensitive microphone which allows vocal performers to use natural gestures to create vocal augmentations and modifications by using embedded sensors in a microphone. Through vocal augmentation and modulation, the performers can easily generate desired amount of the vibrato and achieve wider vocal range. These vocal en-hancements will deliberately enrich the vocal performance both in its expressiveness and the dynamics. Using Voicon, singers can generate additional vibrato, control the pitch and activate customizable vocal effect by simple and intuitive gestures in live and recording context.},
	Address = {Ann Arbor, Michigan},
	Author = {Yongki Park and Hoon Heo and Kyogu Lee},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Gesture, Microphone, Vocal Performance, Performance In-terface},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Voicon: An Interactive Gestural Microphone For Vocal Performance},
	Url = {http://www.nime.org/proceedings/2012/nime2012_199.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_199.pdf}}

@inproceedings{Bosi:2012,
	Abstract = {Tangible tabletop musical interfaces allowing for a collabo-rative real-time interaction in live music performances are one of the promising fields in NIMEs. At present, this kind of interfaces present at least some of the following charac-teristics that limit their musical use: latency in the inter-action, and partial or complete lack of responsiveness to gestures such as tapping, scrubbing or pressing force. Our current research is exploring ways of improving the quality of interaction with this kind of interfaces, and in particular with the tangible tabletop instrument Reactable . In this paper we present a system based on a circular array of me-chanically intercoupled force sensing resistors used to obtain a low-latency, affordable, and easily embeddable hardware system able to detect surface impacts and pressures on the tabletop perimeter. We also consider the option of com-pleting this detected gestural information with the sound information coming from a contact microphone attached to the mechanical coupling layer, to control physical modelling synthesis of percussion instruments.},
	Address = {Ann Arbor, Michigan},
	Author = {Mathieu Bosi and Sergi Jord{\`a}},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {tangible tabletop interfaces, force sensing resistor, mechanical coupling, fast low-noise analog to digital conversion, low-latency sensing, micro controller, multimodal systems, complementary sensing.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Towards fast multi-point force and hit detection in tabletops using mechanically intercoupled force sensing resisors},
	Url = {http://www.nime.org/proceedings/2012/nime2012_257.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_257.pdf}}

@inproceedings{McPherson:2012,
	Abstract = {Capacitive touch sensing is increasingly used in musical con-trollers, particularly those based on multi-touch screen interfaces. However, in contrast to the venerable piano-style keyboard, touch screen controllers lack the tactile feedback many performers find crucial. This paper presents an augmentation system for acoustic and electronic keyboards in which multi-touch capacitive sensors are added to the surface of each key. Each key records the position of fingers on the surface, and by combining this data with MIDI note onsets and aftertouch from the host keyboard, the system functions as a multidimensional polyphonic controller for a wide variety of synthesis software. The paper will discuss general capacitive touch sensor design, keyboard-specific implementation strategies, and the development of a flexible mapping engine using OSC and MIDI.},
	Address = {Ann Arbor, Michigan},
	Author = {Andrew McPherson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {augmented instruments, keyboard, capacitive sensing, multitouch},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {TouchKeys: Capacitive Multi-Touch Sensing on a Physical Keyboard},
	Url = {http://www.nime.org/proceedings/2012/nime2012_195.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_195.pdf}}

@inproceedings{Dahl:2012,
	Abstract = {Composing music for ensembles of computer-based instruments, such as laptop orchestra or mobile phone orchestra, is a multi-faceted and challenging endeavor whose parameters and criteria for success are ill-defined. In the design community, tasks with these qualities are known as wicked problems. This paper frames composing for computer-based ensemble as a design task, shows how Buchanan's four domains of design are present in the task, and discusses its wicked properties. The themes of visibility, risk, and embodiment, as formulated by Klemmer, are shown to be implicitly present in this design task. Composers are encouraged to address them explicitly and to take advantage of the practices of prototyping and iteration.},
	Address = {Ann Arbor, Michigan},
	Author = {Luke Dahl},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Design, laptop orchestra, mobile phone orchestra, instrument design, interaction design, composition},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Wicked Problems and Design Considerations in Composing for Laptop Orchestra},
	Url = {http://www.nime.org/proceedings/2012/nime2012_259.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_259.pdf}}

@inproceedings{Booth:2012,
	Abstract = {In this paper, we argue that the design of New Interfaces for Musical Expression has much to gain from the study of interaction in ensemble laptop performance contexts using ethnographic techniques. Inspired by recent third-stream research in the field of human computer interaction, we describe a recent ethnomethodologically-informed study of the Birmingham Laptop Ensemble (BiLE), and detail our approach to thick description of the group's working practices. Initial formal analysis of this material sheds light on the fluidity of composer, performer and designer roles within the ensemble and shows how confluences of these roles constitute member's differing viewpoints. We go on to draw out a number of strands of interaction that highlight the essentially complex, socially constructed and value driven nature of the group's practice and conclude by reviewing the implications of these factors on the design of software tools for laptop ensembles.},
	Address = {Ann Arbor, Michigan},
	Author = {Graham Booth and Michael Gurevich},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Laptop Performance, Ethnography, Ethnomethodology, Human Computer Interaction.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Collaborative composition and socially constituted instruments: Ensemble laptop performance through the lens of ethnography},
	Url = {http://www.nime.org/proceedings/2012/nime2012_136.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_136.pdf}}

@inproceedings{Smith:2012,
	Abstract = {Machine learning models are useful and attractive tools forthe interactive computer musician, enabling a breadth of interfaces and instruments. With current consumer hardwareit becomes possible to run advanced machine learning algorithms in demanding performance situations, yet expertiseremains a prominent entry barrier for most would-be users.Currently available implementations predominantly employsupervised machine learning techniques, while the adaptive,self-organizing capabilities of unsupervised models are notgenerally available. We present a free, new toolbox of unsupervised machine learning algorithms implemented in Max5 to support real-time interactive music and video, aimedat the non-expert computer artist.},
	Address = {Ann Arbor, Michigan},
	Author = {Benjamin D. Smith and Guy E. Garnett},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, unsupervised machine learning, adaptive resonance theory, self-organizing maps, Max 5},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Unsupervised Play: Machine Learning Toolkit for Max},
	Url = {http://www.nime.org/proceedings/2012/nime2012_68.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_68.pdf}}

@inproceedings{Derbinsky:2012,
	Abstract = {This paper presents a system for mobile percussive collaboration. We show that reinforcement learning can incrementally learn percussive beat patterns played by humans and supports realtime collaborative performance in the absence of one or more performers. This work leverages an existing integration between urMus and Soar and addresses multiple challenges involved in the deployment of machine-learning algorithms for mobile music expression, including tradeoffs between learning speed & quality; interface design for human collaborators; and real-time performance and improvisation.},
	Address = {Ann Arbor, Michigan},
	Author = {Nate Derbinsky and Georg Essl},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Mobile music, machine learning, cognitive architecture},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Exploring Reinforcement Learning for Mobile Percussive Collaboration},
	Url = {http://www.nime.org/proceedings/2012/nime2012_241.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_241.pdf}}

@inproceedings{Nash:2012,
	Abstract = {This paper presents concepts, models, and empirical findings relating to liveness and flow in the user experience of systems mediated by notation. Results from an extensive two-year field study of over 1,000 sequencer and tracker users, combining interaction logging, user surveys, and a video study, are used to illustrate the properties of notations and interfaces that facilitate greater immersion in musical activities and domains, borrowing concepts from programming to illustrate the role of visual and musical feedback, from the notation and domain respectively. The Cognitive Dimensions of Notations framework and Csikszentmihalyi's flow theory are combined to demonstrate how non-realtime, notation-mediated interaction can support focused, immersive, energetic, and intrinsically-rewarding musical experiences, and to what extent they are supported in the interfaces of music production software. Users are shown to maintain liveness through a rapid, iterative edit-audition cycle that integrates audio and visual feedback.},
	Address = {Ann Arbor, Michigan},
	Author = {Chris Nash and Alan Blackwell},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {notation, composition, liveness, flow, feedback, sequencers, DAWs, soundtracking, performance, user studies, programming},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Liveness and Flow in Notation Use},
	Url = {http://www.nime.org/proceedings/2012/nime2012_217.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_217.pdf}}

@inproceedings{Clay:2012,
	Abstract = {The augmented ballet project aims at gathering research from several fields and directing them towards a same application case: adding virtual elements (visual and acoustic) to a dance live performance, and allowing the dancer to interact with them. In this paper, we describe a novel interaction that we used in the frame of this project: using the dancer's movements to recognize the emotions he expresses, and use these emotions to generate musical audio flows evolving in real-time. The originality of this interaction is threefold. First, it covers the whole interaction cycle from the input (the dancer's movements) to the output (the generated music). Second, this interaction isn't direct but goes through a high level of abstraction: dancer's emotional expression is recognized and is the source of music generation. Third, this interaction has been designed and validated through constant collaboration with a choreographer, culminating in an augmented ballet performance in front of a live audience.},
	Address = {Ann Arbor, Michigan},
	Author = {Alexis Clay and Nadine Couture and Myriam Desainte-Catherine and Pierre-Henri Vulliard and Joseph Larralde and Elodie Decarsin},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Interactive sonification, motion, gesture and music, interaction, live performance, musical human-computer interaction},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Movement to emotions to music: using whole body emotional expression as an interaction for electronic music generation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_180.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_180.pdf}}

@inproceedings{Nymoen:2012,
	Abstract = {The paper presents an analysis of the quality of motion data from an iPod Touch (4th gen.). Acceleration and orientation data derived from internal sensors of an iPod is com-pared to data from a high end optical infrared marker-based motion capture system (Qualisys) in terms of latency, jitter, accuracy and precision. We identify some rotational drift in the iPod, and some time lag between the two systems. Still, the iPod motion data is quite reliable, especially for describing relative motion over a short period of time.},
	Address = {Ann Arbor, Michigan},
	Author = {Kristian Nymoen and Arve Voldsund and St{\aa}le A. Skogstad and Alexander Refsum Jensenius and Jim Torresen},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Comparing Motion Data from an iPod Touch to a High-End Optical Infrared Marker-Based Motion Capture System},
	Url = {http://www.nime.org/proceedings/2012/nime2012_198.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_198.pdf}}

@inproceedings{Weitzner:2012,
	Abstract = {massMobile is a client-server system for mass audience participation in live performances using smartphones. It was designed to flexibly adapt to a variety of participatory performance needs and to a variety of performance venues. It allows for real time bi-directional communication between performers and audiences utilizing existing wireless 3G, 4G, or WiFi networks. In this paper, we discuss the goals, design, and implementation of the framework, and we describe several projects realized with massMobile.},
	Address = {Ann Arbor, Michigan},
	Author = {Nathan Weitzner and Jason Freeman and Stephen Garrett and Yan-Ling Chen},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {audience participation, network music, smartphone, performance, mobile},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {massMobile -an Audience Participation Framework},
	Url = {http://www.nime.org/proceedings/2012/nime2012_128.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_128.pdf}}

@inproceedings{Allison:2012,
	Abstract = {Aural -of or relateing to the ear or hearing
Aura -an invisible breath, emanation, or radiation AR -Augmented Reality
AuRal is an environmental audio system in which individual participants form ad hoc ensembles based on geolocation and affect the overall sound of the music associated with the location that they are in.
The AuRal environment binds physical location and the choices of multiple, simultaneous performers to act as the generative force of music tied to the region. Through a mobile device interface, musical participants, or agents, have a degree of input into the generated music essentially defining the sound of a given region. The audio landscape is superimposed onto the physical one. The resultant musical experience is not tied simply to the passage of time, but through the incorporation of participants over time and spatial proximity, it becomes an aural location as much as a piece of music. As a result, walking through the same location at different times results in unique collaborative listening experiences.},
	Address = {Ann Arbor, Michigan},
	Author = {Jesse Allison and Christian Dell},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {AuRal, sonic environment, distributed performance system, mobile music, android, ruby on rails, supercollider},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {AuRal: A Mobile Interactive System for Geo-Locative Audio Synthesis},
	Url = {http://www.nime.org/proceedings/2012/nime2012_301.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_301.pdf}}

@inproceedings{Kimura:2012,
	Abstract = {As a 2010 Artist in Residence in Musical Research at IRCAM, Mari Kimura used the Augmented Violin to develop new compositional approaches, and new ways of creating interactive performances [1]. She contributed her empirical and historical knowledge of violin bowing technique, working with the Real Time Musical Interactions Team at IRCAM. Thanks to this residency, her ongoing long-distance collaboration with the team since 2007 dramatically accelerated, and led to solving several compositional and calibration issues of the Gesture Follower (GF) [2]. Kimura was also the first artist to develop projects between the two teams at IRCAM, using OMAX (Musical Representation Team) with GF. In the past year, the performance with Augmented Violin has been expanded in larger scale interactive audio/visual projects as well. In this paper, we report on the various techniques developed for the Augmented Violin and compositions by Kimura using them, offering specific examples and scores.},
	Address = {Ann Arbor, Michigan},
	Author = {Mari Kimura and Nicolas Rasamimanana and Fr{\'e}d{\'e}ric Bevilacqua and Norbert Schnell and Bruno Zamborlin and Emmanuel Fl{\'e}ty},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Augmented Violin, Gesture Follower, Interactive Performance},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Extracting Human Expression For Interactive Composition with the Augmented Violin},
	Url = {http://www.nime.org/proceedings/2012/nime2012_279.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_279.pdf}}

@inproceedings{Vigliensoni:2012,
	Abstract = {This paper presents a comparison of three-dimensional (3D) position tracking systems in terms of some of their performance parameters such as static accuracy and precision, update rate, and shape of the space they sense. The underlying concepts and characteristics of position tracking tech-nologies are reviewed, and four position tracking systems (Vicon, Polhemus, Kinect, and Gametrak), based on dif-ferent technologies, are empirically compared according to their performance parameters and technical specifications. Our results show that, overall, the Vicon was the position tracker with the best performance.},
	Address = {Ann Arbor, Michigan},
	Author = {Gabriel Vigliensoni and Marcelo M. Wanderley},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Position tracker, comparison, touch-less, gestural control},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Quantitative Comparison of Position Trackers for the Development of a Touch-less Musical Interface},
	Url = {http://www.nime.org/proceedings/2012/nime2012_155.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_155.pdf}}

@inproceedings{Schiesser:2012,
	Abstract = {An augmented bass clarinet is developed in order to extend the performance and composition potential of the instru-ment. Four groups of sensors are added: key positions, inertial movement, mouth pressure and trigger switches. The instrument communicates wirelessly with a receiver setup which produces an OSC data stream, usable by any appli-cation on a host computer.
The SABRe projects intention is to be neither tied to its inventors nor to one single player but to offer a reference design for a larger community of bass clarinet players and composers. For this purpose, several instruments are made available and a number of composer residencies, workshops, presentations and concerts are organized. These serve for evaluation and improvement purposes in order to build a robust and user friendly extended musical instrument, that opens new playing modalities.},
	Address = {Ann Arbor, Michigan},
	Author = {S{\'e}bastien Schiesser and Jan C. Schacher},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {augmented instrument, bass clarinet, sensors, air pressure, gesture, OSC},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {SABRe: The Augmented Bass Clarinet},
	Url = {http://www.nime.org/proceedings/2012/nime2012_193.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_193.pdf}}

@inproceedings{Trail:2012,
	Abstract = {The Gyil is a pentatonic African wooden xylophone with 14-15 keys. The work described in this paper has been motivated by three applications: computer analysis of Gyil performance, live improvised electro-acoustic music incorporating the Gyil, and hybrid sampling and physical mod-eling. In all three of these cases, detailed information about what is played on the Gyil needs to be digitally captured in real-time. We describe a direct sensing apparatus that can be used to achieve this. It is based on contact microphones and is informed by the specific characteristics of the Gyil. An alternative approach based on indirect acquisition is to apply polyphonic transcription on the signal acquired by a microphone without requiring the instrument to be modified. The direct sensing apparatus we have developed can be used to acquire ground truth for evaluating different approaches to polyphonic transcription and help create a ``surrogate'' sensor. Some initial results comparing different strategies to polyphonic transcription are presented.},
	Address = {Ann Arbor, Michigan},
	Author = {Shawn Trail and Tiago Fernandes Tavares and Dan Godlovitch and George Tzanetakis},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {hyperinstruments, indirect acquisition, surrogate sensors, computational ethnomusicology, physical modeling, perfor-mance analysis},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Direct and surrogate sensing for the Gyil african xylophone},
	Url = {http://www.nime.org/proceedings/2012/nime2012_222.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_222.pdf}}

@inproceedings{Overholt:2012,
	Abstract = {The Create USB Interface is an open source microcontroller board that can be programmed in C, BASIC, or Arduino languages. The latest version is called the CUI32Stem, and it is designed to work `hand-in-hand' with the GROVE prototyping system that includes a wide range of sensors and actuators. It utilizes a high-performance Microchip{\textregistered} PIC32 microcontroller unit to allow programmable user interfaces. Its development and typical uses are described, focusing on musical interaction design scenarios. Several options for wireless connectivity are described as well, enabling the CUI32Stem to pair with a smartphone and/or a normal computer. Finally, SeeedStudio's GROVE system is explained, which provides a prototyping system comprised of various elements that incorporate simple plugs, allowing the CUI32Stem to easily connect to the growing collection of open source GROVE transducers.},
	Address = {Ann Arbor, Michigan},
	Author = {Dan Overholt},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Musical Interaction Design, NIME education, Microcontroller, Arduino language, StickOS BASIC, Open Sound Control, Microchip PIC32, Wireless, Zigflea, Wifi, 802.11g, Bluetooth, CUI32, CUI32Stem},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Musical Interaction Design with the CUI32Stem: Wireless Options and the GROVE system for prototyping new interfaces},
	Url = {http://www.nime.org/proceedings/2012/nime2012_194.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_194.pdf}}

@inproceedings{Snyder:2012,
	Abstract = {This paper presents the JD-1, a digital controller for analog modular synthesizers. The JD-1 features a capacitive touch-sensing keyboard that responds to continuous variations in finger contact, high-accuracy polyphonic control-voltage outputs, a built-in sequencer, and digital interfaces for connection to MIDI and OSC devices. Design goals include interoperability with a wide range of synthesizers, very high-resolution pitch control, and intuitive control of the sequencer from the keyboard.},
	Address = {Ann Arbor, Michigan},
	Author = {Jeff Snyder and Andrew McPherson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {keyboard, sequencer, analog synthesizer, capacitive touch sensing},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The JD-1: an Implementation of a Hybrid Keyboard/Sequencer Controller for Analog Synthesizers},
	Url = {http://www.nime.org/proceedings/2012/nime2012_187.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_187.pdf}}

@inproceedings{Ouzounian:2012,
	Abstract = {Music for Sleeping & Waking Minds (2011-2012) is a new, overnight work in which four performers fall asleep while wearing custom designed EEG sensors which monitor their brainwave activity. The data gathered from the EEG sensors is applied in real time to different audio and image signal processing functions, resulting in continuously evolving multi-channel sound environment and visual projection. This material serves as an audiovisual description of the individual and collective neurophysiological state of the ensemble. Audiences are invited to experience the work in different states of attention: while alert and asleep, resting and awakening.},
	Address = {Ann Arbor, Michigan},
	Author = {Gascia Ouzounian and R. Benjamin Knapp and Eric Lyon and Luke DuBois},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {EEG, sleep, dream, biosignals, bio art, consciousness, BCI},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Music for Sleeping & Waking Minds (paper submission)},
	Url = {http://www.nime.org/proceedings/2012/nime2012_229.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_229.pdf}}

@inproceedings{McPherson:2012a,
	Abstract = {There is growing interest in the field of augmented musical instruments, which extend traditional acoustic instruments using new sensors and actuators. Several designs use electromagnetic actuation to induce vibrations in the acoustic mechanism, manipulating the traditional sound of the in-strument without external speakers. This paper presents techniques and guidelines for the use of electromagnetic actuation in augmented instruments, including actuator design and selection, interfacing with the instrument, and cir-cuits for driving the actuators. The material in this pa-per forms the basis of the magnetic resonator piano, an electromagnetically-augmented acoustic grand piano now in its second design iteration. In addition to discussing applications to the piano, this paper aims to provide a toolbox to accelerate the design of new hybrid acoustic-electronic instruments.},
	Address = {Ann Arbor, Michigan},
	Author = {Andrew McPherson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {augmented instruments, electromagnetic actuation, circuit design, hardware},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Techniques and Circuits for Electromagnetic Instrument Actuation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_117.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_117.pdf}}

@inproceedings{Levy:2012,
	Abstract = {OMax is an improvisation software based on a graph representation encoding the pattern repetitions and structures of a sequence, built incrementally and in real-time from a live Midi or Audio source. We present in this paper a totally rewritten version of the software. The new design leads to refine the spectral listening of OMax and to consider different methods to build the symbolic alphabet labeling our symbolic units. The very modular and versatile architecture makes possible new musical configurations and we tried the software with different styles and musical situations. A novel visualization is proposed, which displays the current state of the learnt knowledge and allows to notice, both on the fly and a posteriori, points of musical interest and higher level structures.},
	Address = {Ann Arbor, Michigan},
	Author = {Benjamin Levy and Georges Bloch and Gerard Assayag},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {OMax, Improvisation, Machine Learning, Machine Listen-ing, Visualization, Sequence Model, Software Architecture},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {OMaxist Dialectics: Capturing, Visualizing and Expanding Improvisations},
	Url = {http://www.nime.org/proceedings/2012/nime2012_87.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_87.pdf}}

@inproceedings{Pugliese:2012,
	Abstract = {In this paper strategies for augmenting the social dimension of collaborative music making, in particular in the form of bodily and situated interaction are presented. Mobile instruments are extended by means of relational descriptors democratically controlled by the group and mapped to sound parameters. A qualitative evaluation approach is described and a user test with participants playing in groups of three conducted. The results of the analysis show core-categories such as familiarity with instrument and situation, shift of focus in activity, family of interactions and different categories of the experience emerging from the interviews. Our evaluation shows the suitability of our approach but also the need for iterating on our design on the basis of the perspectives brought forth by the users. This latter observation confirms the importance of conducting a thorough interview session followed by data analysis on the line of grounded theory.},
	Address = {Ann Arbor, Michigan},
	Author = {Roberto Pugliese and Koray Tahiroglu and Callum Goddard and James Nesfield},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Collaborative music making, evaluation methods, mobile music, human-human interaction.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Augmenting human-human interaction in mobile group improvisation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_108.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_108.pdf}}

@inproceedings{Pardo:2012,
	Abstract = {Potential users of audio production software, such as parametric audio equalizers, may be discouraged by the complexity of the interface. A new approach creates a personalized on-screen slider that lets the user manipulate the audio in terms of a descriptive term (e.g. "warm"), without the user needing to learn or use the interface of an equalizer. This system learns mappings by presenting a sequence of sounds to the user and correlating the gain in each frequency band with the user's preference rating. The system speeds learning through transfer learning. Results on a study of 35 participants show how an effective, personalized audio manipulation tool can be automatically built after only three ratings from the user.},
	Address = {Ann Arbor, Michigan},
	Author = {Bryan Pardo and David Little and Darren Gergle},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Human computer interaction, music, multimedia production, transfer learning},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Towards Speeding Audio EQ Interface Building with Transfer Learning},
	Url = {http://www.nime.org/proceedings/2012/nime2012_74.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_74.pdf}}

@inproceedings{Murphy:2012,
	Abstract = {A problem with many contemporary musical robotic percussion systems lies in the fact that solenoids fail to respond lin-early to linear increases in input velocity. This nonlinearity forces performers to individually tailor their compositions to specific robotic drummers. To address this problem, we introduce a method of pre-performance calibration using metaheuristic search techniques. A variety of such techniques are introduced and evaluated and the results of the optimized solenoid-based percussion systems are presented and compared with output from non-calibrated systems.},
	Address = {Ann Arbor, Michigan},
	Author = {Jim Murphy and Ajay Kapur and Dale Carnegie},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {musical robotics, human-robot interaction},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Better Drumming Through Calibration: Techniques for Pre-Performance Robotic Percussion Optimization},
	Url = {http://www.nime.org/proceedings/2012/nime2012_100.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_100.pdf}}

@inproceedings{Yuksel:2012,
	Abstract = {In this work, a comprehensive study is performed on the relationship between audio, visual and emotion by applying the principles of cognitive emotion theory into digital creation. The study is driven by an audiovisual emotion library project that is named AVIEM, which provides an interactive interface for experimentation and evaluation of the perception and creation processes of audiovisuals. AVIEM primarily consists of separate audio and visual libraries and grows with user contribution as users explore different combinations between them. The library provides a wide range of experimentation possibilities by allowing users to create audiovisual relations and logging their emotional responses through its interface. Besides being a resourceful tool of experimentation, AVIEM aims to become a source of inspiration, where digitally created abstract virtual environments and soundscapes can elicit target emotions at a preconscious level, by building genuine audiovisual relations that would engage the viewer on a strong emotional stage. Lastly, various schemes are proposed to visualize information extracted through AVIEM, to improve the navigation and designate the trends and dependencies among audiovisual relations.},
	Address = {Ann Arbor, Michigan},
	Author = {Kamer Ali Yuksel and Sinan Buyukbas and Elif Ayiter},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Designing emotive audiovisuals, cognitive emotion theory, audiovisual perception and interaction, synaesthesia},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {An Interface for Emotional Expression in Audio-Visuals},
	Url = {http://www.nime.org/proceedings/2012/nime2012_60.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_60.pdf}}

@inproceedings{Chacin:2012,
	Abstract = {This paper is an in depth exploration of the fashion object and device, the Play-A-Grill. It details inspirations, socio-cultural implications, technical function and operation, and potential applications for the Play-A-Grill system.},
	Address = {Ann Arbor, Michigan},
	Author = {Aisen Caro Chacin},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Digital Music Players, Hip Hop, Rap, Music Fashion, Grills, Mouth Jewelry, Mouth Controllers, and Bone Conduction Hearing.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Play-A-Grill: Music To Your Teeth},
	Url = {http://www.nime.org/proceedings/2012/nime2012_48.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_48.pdf}}

@inproceedings{Kim:2012,
	Abstract = {In this paper we introduce an interactive mobile music performance system using the digital compass of mobile phones. Compass-based interface can detect the aiming orientation of performers on stage, allowing us to obtain information on interactions between performers and use it for both musical mappings and visualizations on screen for the audience. We document and discuss the result of a compass-based mobile music performance, Where Are You Standing, and present an algorithm for a new app to track down the performers' positions in real-time.},
	Address = {Ann Arbor, Michigan},
	Author = {Bongjun Kim and Woon Seung Yeo},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Mobile music, mobile phone, smartphone, compass, magnetometer, aiming gesture, musical mapping, musical sonification},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Interactive Mobile Music Performance with Digital Compass},
	Url = {http://www.nime.org/proceedings/2012/nime2012_170.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_170.pdf}}

@inproceedings{Lehrman:2012,
	Abstract = {George Antheil's notorious Ballet m{\'e}canique (1924-1925) was originally scored for percussion ensemble, sound effects, and 16 pianolas. He was never able to perform the piece with those forces, however, due to his inability to synchronize multiple pianolas. Thus all performances of the piece in his lifetime, and for decades after, were done with a single pianola or player piano.*
The author traces the origin of the concept of synchronizing multiple pianolas, and explains the attendant technological issues. He examines attempts to synchronize mechanical pianos and other time-based devices at the time of Ballet m{\'e}canique's composition, and suggests that Antheil's vision for his piece was not as farfetched as has long been thought.},
	Address = {Ann Arbor, Michigan},
	Author = {Paul Lehrman},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Antheil, Stravinsky, player piano, pianola, mechanical instruments, synchronization},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Multiple Pianolas in Antheil's Ballet m{\'e}canique},
	Url = {http://www.nime.org/proceedings/2012/nime2012_25.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_25.pdf}}

@inproceedings{Migneco:2012,
	Abstract = {Platforms for mobile computing and gesture recognitionprovide enticing interfaces for creative expression on virtualmusical instruments. However, sound synthesis on thesesystems is often limited to sample-based synthesizers, whichlimits their expressive capabilities. Source-filter models areadept for such interfaces since they provide flexible, algorithmic sound synthesis, especially in the case of the guitar.In this paper, we present a data-driven approach for modeling guitar excitation signals using principal componentsderived from a corpus of excitation signals. Using thesecomponents as features, we apply nonlinear principal components analysis to derive a feature space that describesthe expressive attributes characteristic to our corpus. Finally, we propose using the reduced dimensionality space asa control interface for an expressive guitar synthesizer.},
	Address = {Ann Arbor, Michigan},
	Author = {Raymond Migneco and Youngmoo Kim},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Source-filter models, musical instrument synthesis, PCA, touch musical interfaces},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Component-Based Approach for Modeling Plucked-Guitar Excitation Signals},
	Url = {http://www.nime.org/proceedings/2012/nime2012_63.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_63.pdf}}

@inproceedings{Stead:2012,
	Abstract = {We describe a system that allows non-programmers to specify the grammar for a novel graphic score notation of their own design, defining performance notations suitable for drawing in live situations on a surface such as a whiteboard. Thescore can be interpreted via the camera of a smartphone,interactively scanned over the whiteboard to control the parameters of synthesisers implemented in Overtone. The visual grammar of the score, and its correspondence to the sound parameters, can be defined by the user with a simple visual condition-action language. This language can be edited on the touchscreen of an Android phone, allowing the grammar to be modified live in performance situations.Interactive scanning of the score is visible to the audience asa performance interface, with a colour classifier and visual feature recogniser causing the grammar-specified events to be sent using OSC messages via Wi-Fi from the hand-held smartphone to an audio workstation.},
	Address = {Ann Arbor, Michigan},
	Author = {Alistair G. Stead and Alan F. Blackwell and Samual Aaron},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Graphic Notation, Disposable Notation, Live Coding, Com-puter Vision, Mobile Music},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Graphic Score Grammars for End-Users},
	Url = {http://www.nime.org/proceedings/2012/nime2012_77.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_77.pdf}}

@inproceedings{Nort:2012,
	Abstract = {In this paper we discuss aspects of our work in develop-ing performance systems that are geared towards human-machine co-performance with a particular emphasis on improvisation. We present one particular system, FILTER, which was created in the context of a larger project related to artificial intelligence and performance, and has been tested in the context of our electro-acoustic performance trio. We discuss how this timbrally rich and highly non-idiomatic musical context has challenged the design of the system, with particular emphasis on the mapping of machine listening parameters to higher-level behaviors of the system in such a way that spontaneity and creativity are encouraged while maintaining a sense of novel dialogue.},
	Address = {Ann Arbor, Michigan},
	Author = {Doug Van Nort and Jonas Braasch and Pauline Oliveros},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Electroacoustic Improvisation, Machine Learning, Mapping, Sonic Gestures, Spatialization},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Mapping to musical actions in the FILTER system},
	Url = {http://www.nime.org/proceedings/2012/nime2012_235.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_235.pdf}}

@inproceedings{Magnus:2012,
	Abstract = {The purpose of the Musician Assistance and Score Distribution (MASD) system is to assist novice musicians with playing in an orchestra, concert band, choir or other musical ensemble. MASD helps novice musicians in three ways. It removes the confusion that results from page turns, aides a musician's return to the proper location in the music score after the looking at the conductor and notifies musicians of conductor instructions. MASD is currently verified by evaluating the time between sending beats or conductor information and this information being rendered for the musician. Future work includes user testing of this system.
There are three major components to the MASD system. These components are Score Distribution, Score Rendering and Information Distribution. Score Distribution passes score information to clients and is facilitated by the Internet Communication Engine (ICE). Score Rendering uses the GUIDO Library to display the musical score. Information Distribution uses ICE and the IceStorm service to pass beat and instruction information to musicians.},
	Address = {Ann Arbor, Michigan},
	Author = {Nathan Magnus and David Gerhard},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {score distribution, score-following, score rendering, musician assistance},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Musician Assistance and Score Distribution (MASD)},
	Url = {http://www.nime.org/proceedings/2012/nime2012_237.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_237.pdf}}

@inproceedings{Lai:2012,
	Abstract = {This paper addresses the issue of engaging the audience with new musical instruments in live performance context. We introduce design concerns that we consider influential to enhance the communication flow between the audience and the performer. We also propose and put in practice a design approach that considers the use of performance space as a way to engage with the audience. A collaborative project, Sound Gloves, presented here exemplifies such a concept by dissolving the space between performers and audience. Our approach resulted in a continuous interaction between audience and performers, in which the social dynamics was changed in a positive way in a live performance context of NIMEs. Such an approach, we argue, may be considered as one way to further engage and interact with the audience.},
	Address = {Ann Arbor, Michigan},
	Author = {Chi-Hsia Lai and Koray Tahiroglu},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, wearable electronics, performance, design approach},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Design Approach to Engage with Audience with Wearable Musical Instruments: Sound Gloves},
	Url = {http://www.nime.org/proceedings/2012/nime2012_197.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_197.pdf}}

@inproceedings{Wierenga:2012,
	Abstract = {In an attempt to utilize the expert pianist's technique and spare bandwidth, a new keyboard-based instrument augmented by sensors suggested by the examination of existing acoustic instruments is introduced. The complete instrument includes a keyboard, various pedals and knee levers, several bowing controllers, and breath and embouchure sensors connected to an Arduino microcontroller that sends sensor data to a laptop running Max/MSP, where custom software maps the data to synthesis algorithms. The audio is output to a digital amplifier powering a transducer mounted on a resonator box to which several of the sensors are attached. Careful sensor selection and mapping help to facilitate performance mode.},
	Address = {Ann Arbor, Michigan},
	Author = {Red Wierenga},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Gesture, controllers, Digital Musical Instrument, keyboard},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A New Keyboard-Based, Sensor-Augmented Instrument For Live Performance},
	Url = {http://www.nime.org/proceedings/2012/nime2012_211.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_211.pdf}}

@inproceedings{Han:2012,
	Abstract = {Virtual Pottery is an interactive audiovisual piece that uses hand gesture to create 3D pottery objects and sound shape. Using the OptiTrack motion capture (Rigid Body) system at TransLab in UCSB, performers can take a glove with attached trackers, move the hand in x, y, and z axis and create their own sound pieces. Performers can also manipulate their pottery pieces in real time and change arrangement on the musical score interface in order to create a continuous musical composition. In this paper we address the relationship between body, sound and 3D shapes. We also describe the origin of Virtual Pottery, its design process, discuss its aesthetic value and musical sound synthesis system, and evaluate the overall experience.},
	Address = {Ann Arbor, Michigan},
	Author = {Yoon Chung Han and Byeong-jun Han},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Virtual Pottery, virtual musical instrument, sound synthesis, motion and gesture, pottery, motion perception, interactive sound installation.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Virtual Pottery: An Interactive Audio-Visual Installation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_216.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_216.pdf}}

@inproceedings{Tanaka:2012,
	Abstract = {Mobile devices represent a growing research field within NIME, and a growing area for commercial music software. They present unique design challenges and opportunities, which are yet to be fully explored and exploited. In this paper, we propose using a survey method combined with qualitative analysis to investigate the way in which people use mobiles musically. We subsequently present as an area of future research our own PDplayer, which provides a completely self contained end application in the mobile device, potentially making the mobile a more viable and expressive tool for musicians.},
	Address = {Ann Arbor, Michigan},
	Author = {Atau Tanaka and Adam Parkinson and Zack Settel and Koray Tahiroglu},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, Mobile Music, Pure Data},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Survey and Thematic Analysis Approach as Input to the Design of Mobile Music GUIs},
	Url = {http://www.nime.org/proceedings/2012/nime2012_240.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_240.pdf}}

@inproceedings{Fyans:2012,
	Abstract = {A study is presented examining the participatory design of digital musical interactions. The study takes into consideration the entire ecology of digital musical interactions including the designer, performer and spectator. A new instrument is developed through iterative participatory design involving a group of performers. Across the study the evolution of creative practice and skill development in an emerging community of practice is examined and a spectator study addresses the cognition of performance and the perception of skill with the instrument. Observations are presented regarding the cognition of a novel interaction and evolving notions of skill. The design process of digital musical interactions is reflected on focusing on involvement of the spectator in design contexts.},
	Address = {Ann Arbor, Michigan},
	Author = {A. Cavan Fyans and Adnan Marquez-Borbon and Paul Stapleton and Michael Gurevich},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {participatory design, DMIs, skill, cognition, spectator},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Ecological considerations for participatory design of DMIs},
	Url = {http://www.nime.org/proceedings/2012/nime2012_253.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_253.pdf}}

@inproceedings{Grosshauser:2012,
	Abstract = {From a technical point of view, instrumental music mak-ing involves audible, visible and hidden playing parameters. Hidden parameters like force, pressure and fast movements, happening within milliseconds are particularly difficult to capture. Here, we present data focusing on movement coordination parameters of the left hand fingers with the bow hand in violinists and between two violinists in group playing. Data was recorded with different position sensors, a micro camcorder fixed on a violin and an acceleration sensor placed on the bow. Sensor measurements were obtained at a high sampling rate, gathering the data with a small mi-crocontroller unit, connected with a laptop computer. To capture bow's position, rotation and angle directly on the bow to string contact point, the micro camcorder was fixed near the bridge. Main focuses of interest were the changes of the left hand finger, the temporal synchronization between left hand fingers with the right hand, the close up view to the bow to string contact point and the contact of the left hand finger and/or string to the fingerboard. Seven violinists, from beginners to master class students played scales in different rhythms, speeds and bowings and music excerpts of free choice while being recorded. One measure-ment with 2 violinists was made to see the time differences between two musicians while playing together. For simple integration of a conventional violin into electronic music environments, left hand sensor data were exemplary converted to MIDI and OSC.},
	Address = {Ann Arbor, Michigan},
	Author = {Tobias Grosshauser and Victor Candia and Horst Hildebrand and Gerhard Tr{\"o}ster},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Strings, violin, coordination, left, finger, right, hand},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Sensor Based Measurements of Musicians' Synchronization Issues},
	Url = {http://www.nime.org/proceedings/2012/nime2012_256.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_256.pdf}}

@inproceedings{Melo:2012,
	Abstract = {This paper describes the conceptualization and development of an open source tool for controlling the sound of a saxophone via the gestures of its performer. The motivation behind this work is the need for easy access tools to explore, compose and perform electroacoustic music in Colombian music schools and conservatories. This work led to the adaptation of common hardware to be used as a sensor attached to an acoustic instrument and the development of software applications to record, visualize and map performers gesture data into signal processing parameters. The scope of this work suggested that focus was to be made on a specific instrument so the saxophone was chosen. Gestures were selected in an iterative process with the performer, although a more ambitious strategy to figure out main gestures of an instruments performance was first defined. Detailed gesture-to-sound processing mappings are exposed in the text. An electroacoustic musical piece was successfully rehearsed and recorded using the Gest-O system.},
	Address = {Ann Arbor, Michigan},
	Author = {Jonh Melo and Daniel G{\'o}mez and Miguel Vargas},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Electroacoustic music, saxophone, expanded instrument, gesture.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Gest-O: Performer gestures used to expand the sounds of the saxophone},
	Url = {http://www.nime.org/proceedings/2012/nime2012_262.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_262.pdf}}

@inproceedings{Muller:2012,
	Abstract = {This paper discusses the utilization of human skin as a tangible interface for musical expression and collaborative performance. We present an overview of existing different instrument designs that include the skin as the main input. As a further development of a previous exploration [16] we outline the setup and interaction methods of `Skintimacy', an instrument that appropriates the skin for low voltage power transmission in multi-player interaction. Observations deriving from proof-of-concept exploration and performances using the instrument are brought into the reflection and discussion concerning the capabilities and limitations of skin as an input surface.},
	Address = {Ann Arbor, Michigan},
	Author = {Alexander M{\"u}ller-Rakow and Jochen Fuchs},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Skin-based instruments, skin conductivity, collaborative interfaces, embodiment, intimacy, multi-player performance},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Human Skin as an Interface for Musical Expression},
	Url = {http://www.nime.org/proceedings/2012/nime2012_177.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_177.pdf}}

@inproceedings{Trappe:2012,
	Abstract = {In this paper we present our project to make sound synthesis and music controller construction accessible to children in a technology design workshop. We present the work we have carried out to develop a graphical user interface, and give account of the workshop we conducted in collaboration with a local primary school. Our results indicate that the production of audio events by means of digital synthesis and algorithmic composition provides a rich and interesting field to be discovered for pedagogical workshops taking a Constructionist approach.},
	Address = {Ann Arbor, Michigan},
	Author = {Christoph Trappe},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Child Computer Interaction, Constructionism, Sound and Music Computing, Human-Computer Interface Design, Mu-sic Composition and Generation, Interactive Audio Sys-tems, Technology Design Activities.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Making Sound Synthesis Accessible for Children},
	Url = {http://www.nime.org/proceedings/2012/nime2012_181.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_181.pdf}}

@inproceedings{Skogstad:2012,
	Abstract = {In this paper we present the Dance Jockey System, a system developed for using a full body inertial motion capture suit (Xsens MVN) in music/dance performances. We present different strategies for extracting relevant postures and actions from the continuous data, and how these postures and actions can be used to control sonic and musical features. The system has been used in several public performances, and we believe it has great potential for further exploration. However, to overcome the current practical and technical challenges when working with the system, it is important to further refine tools and software in order to facilitate making of new performance pieces.},
	Address = {Ann Arbor, Michigan},
	Author = {St{\aa}le A. Skogstad and Kristian Nymoen and Yago de Quay and Alexander Refsum Jensenius},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Developing the Dance Jockey System for Musical Interaction with the Xsens MVN Suit},
	Url = {http://www.nime.org/proceedings/2012/nime2012_182.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_182.pdf}}

@inproceedings{OSullivan:2012,
	Abstract = {Development of new musical interfaces often requires experimentation with the mapping of available controller inputs to output parameters. Useful mappings for a particular application may be complex in nature, with one or more inputs being linked to one or more outputs. Existing development environments are commonly used to program such mappings, while code libraries provide powerful data-stream manipulation. However, room exists for a standalone application with a simpler graphical user interface for dynamically patching between inputs and outputs. This paper presents an early prototype version of a software tool that allows the user to route control signals in real time, using various messaging formats. It is cross-platform and runs as a standalone application in desktop and Android OS versions. The latter allows the users of mobile devices to experiment with mapping signals to and from physical computing components using the inbuilt multi-touch screen. Potential uses therefore include real-time mapping during performance in a more expressive manner than facilitated by existing tools.},
	Address = {Ann Arbor, Michigan},
	Author = {Liam O'Sullivan and Dermot Furlong and Frank Boland},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Mapping, Software Tools, Android.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Introducing CrossMapper: Another Tool for Mapping Musical Control Parameters},
	Url = {http://www.nime.org/proceedings/2012/nime2012_189.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_189.pdf}}

@inproceedings{Donnarumma:2012,
	Abstract = {Performing music with a computer and loudspeakers represents always a challenge. The lack of a traditional instrument requires the performer to study idiomatic strategies by which musicianship becomes apparent. On the other hand, the audience needs to decode those strategies, so to achieve an understanding and appreciation of the music being played. The issue is particularly relevant to the performance of music that results from the mediation between biological signals of the human body and physical performance.
The present article tackles this concern by demonstrating a new model of musical performance; what I define biophysical music. This is music generated and played in real time by amplifying and processing the acoustic sound of a performer's muscle contractions. The model relies on an original and open source technology made of custom biosensors and a related software framework. The succesfull application of these tools is discussed in the practical context of a solo piece for sensors, laptop and loudspeakers. Eventually, the compositional strategies that characterize the piece are discussed along with a systematic description of the relevant mapping techniques and their sonic outcome.},
	Address = {Ann Arbor, Michigan},
	Author = {Marco Donnarumma},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Muscle sounds, biophysical music, augmented body, realtime performance, human-computer interaction, embodiment.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Music for Flesh II: informing interactive music performance with the viscerality of the body system},
	Url = {http://www.nime.org/proceedings/2012/nime2012_133.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_133.pdf}}

@inproceedings{Zamorano:2012,
	Abstract = {This paper introduces Simpletones, an interactive sound system that enables a sense of musical collaboration for non-musicians. Participants can easily create simple sound compositions in real time by collaboratively operating physical artifacts as sound controllers. The physical configuration of the artifacts requires coordinated actions between participants to control sound (thus requiring, and emphasizing collaboration).
Simpletones encourages playful human-to-human interaction by introducing a simple interface and a set of basic rules [1]. This enables novices to focus on the collaborative aspects of making music as a group (such as synchronization and taking collective decisions through non-verbal communication) to ultimately engage a state of group flow[2].
This project is relevant to a contemporary discourse on musical expression because it allows novices to experience the social aspects of group music making, something that is usually reserved only for trained performers [3].},
	Address = {Ann Arbor, Michigan},
	Author = {Francisco Zamorano},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Collaboration, Artifacts, Computer Vision, Color Tracking, State of Flow.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Simpletones: A System of Collaborative Physical Controllers for Novices},
	Url = {http://www.nime.org/proceedings/2012/nime2012_258.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_258.pdf}}

@inproceedings{Henriques:2012,
	Abstract = {The Sonik Spring is a portable and wireless digital instrument, created for real-time synthesis and control of sound. It brings together different types of sensory input, linking gestural motion and kinesthetic feedback to the production of sound. The interface consists of a 15-inch spring with unique flexibility, which allows multiple degrees of variation in its shape and length. The design of the instrument is described and its features discussed. Three performance modes are detailed highlighting the instrument's expressive potential and wide range of functionality. },
	Address = {Ann Arbor, Michigan},
	Author = {Tomas Henriques},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Interface for sound and music, Gestural control of sound, Kinesthetic and visual feedback},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {SONIK SPRING},
	Url = {http://www.nime.org/proceedings/2012/nime2012_20.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_20.pdf}}

@inproceedings{Troyer:2012,
	Abstract = {We introduce a prototype of a new tangible step sequencerthat transforms everyday objects into percussive musicalinstruments. DrumTop adapts our everyday task-orientedhand gestures with everyday objects as the basis of musicalinteraction, resulting in an easily graspable musical interfacefor musical novices. The sound, tactile, and visual feedbackcomes directly from everyday objects as the players programdrum patterns and rearrange the objects on the tabletopinterface. DrumTop encourages the players to explore themusical potentiality of their surroundings and be musicallycreative through rhythmic interactions with everyday objects. The interface consists of transducers that trigger ahit, causing the objects themselves to produce sound whenthey are in close contact with the transducers. We discusshow we designed and implemented our current DrumTopprototype and describe how players interact with the interface. We then highlight the players' experience with Drumtop and our plans for future work in the fields of musiceducation and performance.},
	Address = {Ann Arbor, Michigan},
	Author = {Akito van Troyer},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Tangible User Interfaces, Playful Experience, Percussion, Step Sequencer, Transducers, Everyday Objects},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {DrumTop: Playing with Everyday Objects},
	Url = {http://www.nime.org/proceedings/2012/nime2012_70.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_70.pdf}}

@inproceedings{Oliver:2012,
	Abstract = {There is some evidence that structured training can benefit cochlear implant (CI) users' appraisal of music as well as their music perception abilities. There are currently very limited music training resources available for CI users to explore. This demonstration will introduce delegates to the `Interactive Music Awareness Program' (IMAP) for cochlear implant users, which was developed in response to the need for a client-centered, structured, interactive, creative, open-ended, educational and challenging music (re)habilitation resource.},
	Address = {Ann Arbor, Michigan},
	Author = {Benjamin R. Oliver and Rachel M. van Besouw and David R. Nicholls},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {music, cochlear implants, perception, rehabilitation, auditory training, interactive learning, client-centred software},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The `Interactive Music Awareness Program' (IMAP) for Cochlear Implant Users},
	Url = {http://www.nime.org/proceedings/2012/nime2012_109.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_109.pdf}}

@inproceedings{Schwarz:2012,
	Abstract = {Corpus-based concatenative synthesis is a fairly recent sound synthesis method, based on descriptor analysis of any number of existing or live-recorded sounds, and synthesis by selection of sound segments from the database matching given sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a new interface for musical expression. The interesting outcome of such an examination is that the actual instrument is the space of sound characteristics, through which the performer navigates with gestures captured by various input devices. We will take a look at different types of interaction modes and controllers (positional, inertial, audio analysis) and the gestures they afford, and provide a critical assessment of their musical and expressive capabilities, based on several years of musical experience, performing with the CataRT system for real-time CBCS.},
	Address = {Ann Arbor, Michigan},
	Author = {Diemo Schwarz},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {CataRT, corpus-based concatenative synthesis, gesture},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Sound Space as Musical Instrument: Playing Corpus-Based Concatenative Synthesis},
	Url = {http://www.nime.org/proceedings/2012/nime2012_120.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_120.pdf}}

@inproceedings{McGee:2012,
	Abstract = {SenSynth is an open-source mobile application that allows for arbitrary, dynamic mapping between several sensors and sound synthesis parameters. In addition to synthesis techniques commonly found on mobile devices, SenSynth includes a scanned synthesis source for the audification of sensor data. Using SenSynth, we present a novel instrument based on the audification of accelerometer data and introduce a new means of mobile synthesis control via a wearable magnetic ring. SenSynth also employs a global pitch quantizer so one may adjust the level of virtuosity required to play any instruments created via mapping.},
	Address = {Ann Arbor, Michigan},
	Author = {Ryan McGee and Daniel Ashbrook and Sean White},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {mobile music, sonification, audification, mobile sensors},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {SenSynth: a Mobile Application for Dynamic Sensor to Sound Mapping},
	Url = {http://www.nime.org/proceedings/2012/nime2012_149.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_149.pdf}}

@inproceedings{Leeuw:2012,
	Abstract = {This short paper follows an earlier NIME paper [1] describing the invention and construction of the Electrumpet. Revisions and playing experience are both part of the current paper.
The Electrumpet can be heard in the performance given by Hans Leeuw and Diemo Schwarz at this NIME conference.},
	Address = {Ann Arbor, Michigan},
	Author = {Hans Leeuw},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, Electrumpet, live-electronics, hybrid instruments.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The electrumpet, additions and revisions},
	Url = {http://www.nime.org/proceedings/2012/nime2012_271.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_271.pdf}}

@inproceedings{Carlson:2012,
	Abstract = {Borderlands is a new interface for composing and performing with granular synthesis. The software enables flexible, realtime improvisation and is designed to allow users to engage with sonic material on a fundamental level, breaking free of traditional paradigms for interaction with this technique. The user is envisioned as an organizer of sound, simultaneously assuming the roles of curator, performer, and listener. This paper places the software within the context of painterly interfaces and describes the user interaction design and synthesis methodology.},
	Address = {Ann Arbor, Michigan},
	Author = {Chris Carlson and Ge Wang},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Granular synthesis, painterly interfaces, improvisation, organized sound, NIME, CCRMA},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Borderlands -An Audiovisual Interface for Granular Synthesis},
	Url = {http://www.nime.org/proceedings/2012/nime2012_152.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_152.pdf}}

@inproceedings{FASCIANI:2012,
	Abstract = {Sound generators and synthesis engines expose a large set of parameters, allowing run-time timbre morphing and exploration of sonic space. However, control over these high-dimensional interfaces is constrained by the physical limitations of performers. In this paper we propose the exploitation of vocal gesture as an extension or alternative to traditional physical controllers. The approach uses dynamic aspects of vocal sound to control variations in the timbre of the synthesized sound. The mapping from vocal to synthesis parameters is automatically adapted to information extracted from vocal examples as well as to the relationship between parameters and timbre within the synthesizer. The mapping strategy aims to maximize the breadth of the explorable perceptual sonic space over a set of the synthesizer's real-valued parameters, indirectly driven by the voice-controlled interface.},
	Address = {Ann Arbor, Michigan},
	Author = {STEFANO FASCIANI and LONCE WYSE},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Voice Interface for Sound Generators: adaptive and automatic mapping of gestures to sound},
	Url = {http://www.nime.org/proceedings/2012/nime2012_57.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_57.pdf}}

@inproceedings{Ariza:2012,
	Abstract = {This paper demonstrates the practical benefits and performance opportunities of using the dual-analog gamepad as a controller for real-time live electronics. Numerous diverse instruments and interfaces, as well as detailed control mappings, are described. Approaches to instrument and preset switching are also presented. While all of the instrument implementations presented are made available through the Martingale Pd library, resources for other synthesis languages are also described. },
	Address = {Ann Arbor, Michigan},
	Author = {Christopher Ariza},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Controllers, live electronics, dual-analog, gamepad, joystick, computer music, instrument, interface},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Dual-Analog Gamepad as a Practical Platform for Live Electronics Instrument and Interface Design},
	Url = {http://www.nime.org/proceedings/2012/nime2012_73.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_73.pdf}}

@inproceedings{Patricio:2012,
	Abstract = {This article proposes a wireless handheld multimedia digital instrument, which allows one to compose and perform digital music for films in real-time. Not only does it allow the performer and the audience to follow the film images in question, but also the relationship between the gestures performed and the sound generated. Furthermore, it allows one to have an effective control over the sound, and consequently achieve great musical expression. In addition, a method for calibrating the multimedia digital instrument, devised to overcome the lack of a reliable reference point of the accelerometer and a process to obtain a video score are presented. This instrument has been used in a number of concerts (Portugal and Brazil) so as to test its robustness.},
	Address = {Ann Arbor, Michigan},
	Author = {Pedro Patr{\'\i}cio},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {MuDI -Multimedia Digital Instrument for Composing and Performing Digital Music for Films in Real-time},
	Url = {http://www.nime.org/proceedings/2012/nime2012_64.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_64.pdf}}

@inproceedings{t-Klooster:2012,
	Abstract = {This paper describes the development of the Emotion Light, an interactive biofeedback artwork where the user listens to a piece of electronic music whilst holding a semi-transparent sculpture that tracks his/her bodily responses and translates these into changing light patterns that emerge from the sculpture. The context of this work is briefly described and the questions it poses are derived from interviews held with audience members.},
	Address = {Ann Arbor, Michigan},
	Author = {Adinda Rosa van 't Klooster},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Interactive biofeedback artwork, music and emotion, novel interfaces, practice based research, bodily response, heart rate, biosignals, affective computing, aesthetic interaction, mediating body, biology inspired system},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The body as mediator of music in the Emotion Light},
	Url = {http://www.nime.org/proceedings/2012/nime2012_167.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_167.pdf}}

@inproceedings{Luhtala:2012,
	Abstract = {This paper introduces `The Aesthetic Experience Prism', a framework for studying how components of aesthetic experience materialize in the model's of interaction of novel musical interfaces as well as how the role of aesthetics could be made more explicit in the processes of designing interaction for musical technologies. The Aesthetic Experience Prism makes use of Arthur Danto's framework of aesthetic experience that consists of three conceptual entities: (1) metaphor; (2) expression; and (3) style. In this paper we present key questions driving the research, theoretical background, artistic research approach and user research activities.
In the DIYSE project a proof-of-concept music creation system prototype was developed in a collaborative design setting. The prototype provides means to the performer to create music with minimum effort while allowing for versatile interaction. We argue that by using an artistic research approach specifically targeting designing for aesthetic experience we were able to transform the knowledge from early design ideas to resulting technology products in which model's of interaction metaphors, expression and style are in an apparent role.},
	Address = {Ann Arbor, Michigan},
	Author = {Matti Luhtala and Ilkka Niemel{\"a}inen and Johan Plomp and Markku Turunen and Julius Tuomisto},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Aesthetics, Interaction Design, Artistic Research, Exploration},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Studying Aesthetics in a Musical Interface Design Process Through `Aesthetic Experience Prism'},
	Url = {http://www.nime.org/proceedings/2012/nime2012_226.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_226.pdf}}

@inproceedings{Harriman:2012,
	Abstract = {This paper provides an overview of a new method for approaching beat sequencing. As we have come to know them drum machines provide means to loop rhythmic patterns over a certain interval. Usually with the option to specify different beat divisions. What I developed and propose for consideration is a rethinking of the traditional drum machine confines. The Sinkapater is an untethered beat sequencer in that the beat division, and the loop length can be arbitrarily modified for each track. The result is the capability to create complex syncopated patterns which evolve over time as different tracks follow their own loop rate. To keep cohesion all channels can be locked to a master channel forcing a loop to be an integer number of "Master Beats". Further a visualization mode enables exploring the patterns in another new way. Using synchronized OpenGL a 3-Dimensional environment visualizes the beats as droplets falling from faucets of varying heights determined by the loop length. Waves form in the bottom as beats splash into the virtual "sink". By combining compelling visuals and a new approach to sequencing a new way of exploring beats and experiencing music has been created.},
	Address = {Ann Arbor, Michigan},
	Author = {Jiffer Harriman},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, proceedings, drum machine, sequencer, visualization},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Sinkapater -An Untethered Beat Sequencer},
	Url = {http://www.nime.org/proceedings/2012/nime2012_308.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_308.pdf}}

@inproceedings{Frisson:2012,
	Abstract = {This paper presents the LoopJam installation which allows participants to interact with a sound map using a 3D com-puter vision tracking system. The sound map results from similarity-based clustering of sounds. The playback of these sounds is controlled by the positions or gestures of partic-ipants tracked with a Kinect depth-sensing camera. The beat-inclined bodily movements of participants in the in-stallation are mapped to the tempo of played sounds, while the playback speed is synchronized by default among all sounds. We presented and tested an early version of the in-stallation to three exhibitions in Belgium, Italy and France. The reactions among participants ranged between curiosity and amusement.},
	Address = {Ann Arbor, Michigan},
	Author = {Christian Frisson and St{\'e}phane Dupont and Julien Leroy and Alexis Moinet and Thierry Ravet and Xavier Siebert and Thierry Dutoit},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Interactive music systems and retrieval, user interaction and interfaces, audio similarity, depth sensors},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {LoopJam: turning the dance floor into a collaborative instrumental map},
	Url = {http://www.nime.org/proceedings/2012/nime2012_260.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_260.pdf}}

@inproceedings{Kikukawa:2012,
	Abstract = {We developed original solenoid actuator units with several built-in sensors, and produced a box-shaped musical inter-face ``PocoPoco'' using 16 units of them as a universal input/output device. We applied up-and-down movement of the solenoid-units and user's intuitive input to musical interface. Using transformation of the physical interface, we can apply movement of the units to new interaction design. At the same time we intend to suggest a new interface whose movement itself can attract the user.},
	Address = {Ann Arbor, Michigan},
	Author = {Yuya Kikukawa and Takaharu Kanai and Tatsuhiko Suzuki and Toshiki Yoshiike and Tetsuaki Baba and Kumiko Kushiyama},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {musical interface, interaction design, tactile, moving, kinetic},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {PocoPoco: A Kinetic Musical Interface With Electro-Magnetic Levitation Units},
	Url = {http://www.nime.org/proceedings/2012/nime2012_232.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_232.pdf}}

@inproceedings{Yang:2012,
	Abstract = {We augment the piano keyboard with a 3D gesture space using Microsoft Kinect for sensing and top-down projection for visual feedback. This interface provides multi-axial gesture controls to enable continuous adjustments to multiple acoustic parameters such as those on the typical digital synthesizers. We believe that using gesture control is more visceral and aesthetically pleasing, especially during concert performance where the visibility of the performer's action is important. Our system can also be used for other types of gesture interaction as well as for pedagogical applications.},
	Address = {Ann Arbor, Michigan},
	Author = {Qi Yang and Georg Essl},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, piano, depth camera, musical instrument, gesture, tabletop projection},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Augmented Piano Performance using a Depth Camera},
	Url = {http://www.nime.org/proceedings/2012/nime2012_203.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_203.pdf}}

@inproceedings{Schlei:2012,
	Abstract = {This paper describes the design and realization of TC-11, a software instrument based on programmable multi-point controllers. TC-11 is a modular synthesizer for the iPad that uses multi-touch and device motion sensors for control. It has a robust patch programming interface that centers around multi-point controllers, providing powerful flexibility. This paper details the origin, design principles, programming implementation, and performance result of TC-11.},
	Address = {Ann Arbor, Michigan},
	Author = {Kevin Schlei},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {TC-11, iPad, multi-touch, multi-point, controller mapping, synthesis programming},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {TC-11: A Programmable Multi-Touch Synthesizer for the iPad},
	Url = {http://www.nime.org/proceedings/2012/nime2012_230.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_230.pdf}}

@inproceedings{Parson:2012,
	Abstract = {With the advent of high resolution digital video projection and high quality spatial sound systems in modern planetariums, the planetarium can become the basis for a unique set of virtual musical instrument capabilities that go well beyond packaged multimedia shows. The dome, circular speaker and circular seating arrangements provide means for skilled composers and performers to create a virtual reality in which attendees are immersed in the composite instrument. This initial foray into designing an audio-visual computerbased instrument for improvisational performance in a planetarium builds on prior, successful work in mapping the rules and state of two-dimensional computer board games to improvised computer music. The unique visual and audio geometries of the planetarium present challenges and opportunities. The game tessellates the dome in mobile, colored hexagons that emulate both atoms and musical scale intervals in an expanding universe. Spatial activity in the game maps to spatial locale and instrument voices in the speakers, in essence creating a virtual orchestra with a string section, percussion section, etc. on the dome. Future work includes distribution of game play via mobile devices to permit attendees to participate in a performance. This environment is open-ended, with great educational and aesthetic potential.},
	Address = {Ann Arbor, Michigan},
	Author = {Dale Parson and Phillip Reed},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {aleatory music, algorithmic improvisation, computer game, planetarium},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Planetarium as a Musical Instrument},
	Url = {http://www.nime.org/proceedings/2012/nime2012_47.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_47.pdf}}

@inproceedings{Shahar:2012,
	Abstract = {SoundStrand is a tangible music composition tool. It demonstrates a paradigm developed to enable music composition through the use of tangible interfaces. This paradigm attempts to overcome the contrast between the relatively small of amount degrees of freedom usually demonstrated by tangible interfaces and the vast number of possibilities that musical composition presents.
SoundStrand is comprised of a set of physical objects called cells, each representing a musical phrase. Cells can be sequentially connected to each other to create a musical theme. Cells can also be physically manipulated to access a wide range of melodic, rhythmic and harmonic variations. The SoundStrand software assures that as the cells are manipulated, the melodic flow, harmonic transitions and rhythmic patterns of the theme remain musically plausible while preserving the user's intentions.},
	Address = {Ann Arbor, Michigan},
	Author = {Eyal Shahar},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Tangible, algorithmic, composition, computer assisted},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {SoundStrand: a Tangible Interface for Composing Music with Limited Degrees of Freedom},
	Url = {http://www.nime.org/proceedings/2012/nime2012_125.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_125.pdf}}

@inproceedings{Marier:2012,
	Abstract = {A new method for interpolating between presets is described. The interpolation algorithm called Intersecting N-Spheres Interpolation is simple to compute and its generalization to higher dimensions is straightforward. The current imple-mentation in the SuperCollider environment is presented as a tool that eases the design of many-to-many mappings for musical interfaces. Examples of its uses, including such mappings in conjunction with a musical interface called the sponge, are given and discussed.},
	Address = {Ann Arbor, Michigan},
	Author = {Martin Marier},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Mapping, Preset, Interpolation, Sponge, SuperCollider},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Designing Mappings for Musical Interfaces Using Preset Interpolation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_159.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_159.pdf}}

@inproceedings{Refsum:2012,
	Abstract = {We report on the Music Ball Project, a longterm, exploratory project focused on creating novel instruments/controllers with a spherical shape as the common denominator. Be-sides a simple and attractive geometrical shape, balls afford many different types of use, including play. This has made our music balls popular among widely different groups of people, from toddlers to seniors, including those that would not otherwise engage with a musical instrument. The pa-per summarises our experience of designing, constructing and using a number of music balls of various sizes and with di{\O}erent types of sound-producing elements.},
	Address = {Ann Arbor, Michigan},
	Author = {Jensenius Alexander Refsum and Voldsund Arve},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {music balls, instruments, controllers, inexpensive},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Music Ball Project: Concept, Design, Development, Performance},
	Url = {http://www.nime.org/proceedings/2012/nime2012_161.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_161.pdf}}

@inproceedings{Rotondo:2012,
	Abstract = {In this paper we explore the concept of instruments which
are played by more than one person, and present two case
studies. We designed, built and performed with Feedb{\o}rk,
a two-player instrument comprising two iPads which form
a video feedback loop, and Barrel, a nine-player instrument
made up of eight Gametrak controllers fastened to a steel
industrial barrel. By splitting the control of these instruments into distinct but interdependent roles, we allow each
individual to easily play a part while retaining a rich complexity of output for the whole system. We found that the
relationships between those roles had a significant effect on
how the players communicated with each other, and on how
the performance was perceived by the audience.},
	Address = {Ann Arbor, Michigan},
	Author = {Michael Rotondo and Nick Kruge and Ge Wang},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Many-Person Instruments for Computer Music Performance},
	Url = {http://www.nime.org/proceedings/2012/nime2012_171.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_171.pdf}}

@inproceedings{Kapur:2012,
	Abstract = {In this paper, we introduce Kritaanjli, a robotic harmo-nium. Details concerning the design, construction, and use of Kritaanjli are discussed. After an examination of related work, quantitative research concerning the hardware chosen in the construction of the instrument is shown, as is a thor-ough exposition of the design process and use of CAD/CAM techniques in the design lifecycle of the instrument. Addi-tionally, avenues for future work and compositional prac-tices are focused upon, with particular emphasis placed on human/robot interaction, pedagogical techniques afforded by the robotic instrument, and compositional avenues made accessible through the use of Kritaanjli.},
	Address = {Ann Arbor, Michigan},
	Author = {Ajay Kapur and Jim Murphy and Dale Carnegie},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Musical Robotics, pedagogy, North Indian Classical Music, augmented instruments},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Kritaanjali: A Robotic Harmonium for Performance, Pedogogy and Research},
	Url = {http://www.nime.org/proceedings/2012/nime2012_99.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_99.pdf}}

@inproceedings{Beilharz:2012,
	Abstract = {In site-specific installation or situated media, a significant part of the "I" in NIME is the environment, the site and the implicit features of site such as humans, weather, materials, natural acoustics, etc. These could be viewed as design constraints, or features, even agency determining the outcome of responsive sound installation works. This paper discusses the notion of interface in public (especially outdoor) installation, starting with the authors' Sculpture by the Sea Windtraces work using this recent experience as the launch-pad, with reference to ways in which others have approached it (focusing on sensor, weather-activated outdoor installations in a brief traverse of related cases, e.g. works by Garth Paine, James Bulley and Daniel Jones, and David Bowen). This is a dialogical paper on the topic of interface and `site' as the aetiology of interaction/interface/instrument and its type of response (e.g. to environment and audience). While the focus here is on outdoor factors (particularly the climatic environment), indoor site-specific installation also experiences the effects of ambient noise, acoustic context, and audience as integral agents in the interface and perception of the work, its musical expression. The way in which features of the situation are integrated has relevance for others in the NIME community in the design of responsive spaces, art installation, and large-scale or installed instruments in which users, participants, acoustics play a significant role.},
	Address = {Ann Arbor, Michigan},
	Author = {Kirsty Beilharz and Aengus Martin},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, site-specific installation, outdoor sound installation},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The `Interface' in Site-Specific Sound Installation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_175.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_175.pdf}}

@inproceedings{Jackson:2012,
	Address = {Ann Arbor, Michigan},
	Author = {Jay Alan Jackson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Bubble Drum-agog-ing: Polyrhythm Games \& Other Inter Activities},
	Url = {http://www.nime.org/proceedings/2012/nime2012_8.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_8.pdf}}

@inproceedings{Trail:2012a,
	Abstract = {Hyper-instruments extend traditional acoustic instruments with sensing technologies that capture digitally subtle and sophisticated aspects of human performance. They leverage the long training and skills of performers while simultaneously providing rich possibilities for digital control. Many existing hyper-instruments suffer from being one of a kind instruments that require invasive modifications to the underlying acoustic instrument. In this paper we focus on the pitched percussion family and describe a non-invasive sensing approach for extending them to hyper-instruments. Our primary concern is to retain the technical integrity of the acoustic instrument and sound production methods while being able to intuitively interface the computer. This is accomplished by utilizing the Kinect sensor to track the position of the mallets without any modification to the instrument which enables easy and cheap replication of the pro-posed hyper-instrument extensions. In addition we describe two approaches to higher-level gesture control that remove the need for additional control devices such as foot pedals and fader boxes that are frequently used in electro-acoustic performance. This gesture control integrates more organically with the natural flow of playing the instrument providing user selectable control over filter parameters, synthesis, sampling, sequencing, and improvisation using a commer-cially available low-cost sensing apparatus.},
	Address = {Ann Arbor, Michigan},
	Author = {Shawn Trail and Michael Dean and Gabrielle Odowichuk and Tiago Fernandes Tavares and Peter Driessen and W. Andrew Schloss and George Tzanetakis},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Non-invasive sensing and gesture control for pitched percussion hyper-instruments using the Kinect},
	Url = {http://www.nime.org/proceedings/2012/nime2012_297.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_297.pdf}}

@inproceedings{Lee:2012c,
	Abstract = {This research aims to improve the correspondence between music and dance, and explores the use of human respiration pattern for musical applications with focus on the motional aspect of breathing. While respiration is frequently considered as an indicator of the metabolic state of human body that contains meaningful information for medicine or psychology, motional aspect of respiration has been relatively unnoticed in spite of its strong correlation with muscles and the brain.
This paper introduces an interactive system to control music playback for dance performances based on the respiration pattern of the dancer. A wireless wearable sensor device detects the dancer's respiration, which is then utilized to modify the dynamic of music. Two different respiration-dynamic mappings were designed and evaluated through public performances and private tests by professional choreographers. Results from this research suggest a new conceptual approach to musical applications of respiration based on the technical characteristics of music and dance.},
	Address = {Ann Arbor, Michigan},
	Author = {Jeong-seob Lee and Woon Seung Yeo},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Music, dance, respiration, correspondence, wireless interface, interactive performance},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Real-time Modification of Music with Dancer's Respiration Pattern},
	Url = {http://www.nime.org/proceedings/2012/nime2012_309.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_309.pdf}}

@inproceedings{Castet:2012,
	Abstract = {This paper presents ongoing work on methods dedicated torelations between composers and performers in the contextof experimental music. The computer music community hasover the last decade paid a strong interest on various kindsof gestural interfaces to control sound synthesis processes.The mapping between gesture and sound parameters hasspecially been investigated in order to design the most relevant schemes of sonic interaction. In fact, this relevanceresults in an aesthetic choice that encroaches on the process of composition. This work proposes to examine therelations between composers and performers in the contextof the new interfaces for musical expression. It aims to define a theoretical and methodological framework clarifyingthese relations. In this project, this paper is the first experimental study about the use of physical models as gesturalmaps for the production of textural sounds.},
	Address = {Ann Arbor, Michigan},
	Author = {Julien Castet},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Simulation, Interaction, Sonic textures},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Performing experimental music by physical simulation},
	Url = {http://www.nime.org/proceedings/2012/nime2012_30.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_30.pdf}}

@inproceedings{Lu:2012,
	Abstract = {WIS platform is a wireless interactive sensor platform de-signed to support dynamic and interactive applications. The platform consists of a capture system which includes multi-ple on-body Zigbee compatible motion sensors, a processing unit and an audio-visual display control unit. It has a com-plete open architecture and provides interfaces to interact with other user-designed applications. Therefore, WIS plat-form is highly extensible. Through gesture recognitions by on-body sensor nodes and data processing, WIS platform can offer real-time audio and visual experiences to the users. Based on this platform, we set up a multimedia installation that presents a new interaction model between the partic-ipants and the audio-visual environment. Furthermore, we are also trying to apply WIS platform to other installations and performances.},
	Address = {Ann Arbor, Michigan},
	Author = {Jia-Liang Lu and Da-Lei Fang and Yi Qin and Jiu-Qiang Tang},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Interactive, Audio-visual experience},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Wireless Interactive Sensor Platform for Real-Time Audio-Visual Experience},
	Url = {http://www.nime.org/proceedings/2012/nime2012_98.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_98.pdf}}

@inproceedings{Brent:2012,
	Abstract = {This paper introduces the Gesturally Extended Piano---an augmented instrument controller that relies on information drawn from performer motion tracking in order to control real-time audiovisual processing and synthesis. Specifically, the positions, heights, velocities, and relative distances and angles of points on the hands and forearms are followed. Technical details and installation of the tracking system are covered, as well as strategies for interpreting and mapping the resulting data in relation to synthesis parameters. Design factors surrounding mapping choices and the interrelation between mapped parameters are also considered.},
	Address = {Ann Arbor, Michigan},
	Author = {William Brent},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Augmented instruments, controllers, motion tracking, mapping},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Gesturally Extended Piano},
	Url = {http://www.nime.org/proceedings/2012/nime2012_102.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_102.pdf}}

@inproceedings{Pineyro:2012,
	Abstract = {The Electric Slide Organistrum (Figure 1) is an acoustic stringed instrument played through a video capture system. The vibration of the instrument string is generated electro-magnetically and the pitch variation is achieved by movements carried out by the player in front of a video camera. This instrument results from integrating an ancient technique for the production of sounds as it is the vibration of a string on a soundbox and actual human-computer interaction technology such as motion detection.},
	Address = {Ann Arbor, Michigan},
	Author = {Martin Pi{\~n}eyro},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Gestural Interface, eBow, Pickup, Bowed string, Electromagnetic actuation},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Electric Slide Organistrum},
	Url = {http://www.nime.org/proceedings/2012/nime2012_114.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_114.pdf}}

@inproceedings{Leeuw:2012a,
	Abstract = {This position paper likes to stress the role and importance of performance based education in NIME like subjects. It describes the `klankontwerp' learning line at the `school of the arts Utrecht' in its department Music Technology.
Our educational system also reflects the way that we could treat performance in the NIME community as a whole. The importance of performing with our instruments other then in the form of a mere demonstration should get more emphasis.},
	Address = {Ann Arbor, Michigan},
	Author = {Hans Leeuw and Jorrit Tamminga},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, education, position paper, live electronics, performance},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {NIME Education at the HKU, Emphasizing performance},
	Url = {http://www.nime.org/proceedings/2012/nime2012_247.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_247.pdf}}

@inproceedings{Bukvic:2012,
	Abstract = {In the following paper we propose a new tiered granularity approach to developing modules or abstractions in the Pd-L2Ork visual multimedia programming environment with the specific goal of devising creative environments that scale their educational scope and difficulty to encompass several stages within the context of primary and secondary (K-12) education. As part of a preliminary study, the team designed modules targeting 4th and 5th grade students, the primary focus being exploration of creativity and collaborative learning. The resulting environment infrastructure -coupled with the Boys & Girls Club of Southwest Virginia Satellite Linux Laptop Orchestra -offers opportunities for students to design and build original instruments, master them through a series of rehearsals, and ultimately utilize them as part of an ensemble in a performance of a predetermined piece whose parameters are coordinated by instructor through an embedded networked module. The ensuing model will serve for the assessment and development of a stronger connection with content-area standards and the development of creative thinking and collaboration skills.},
	Address = {Ann Arbor, Michigan},
	Author = {Ivica Bukvic and Liesl Baum and Bennett Layman and Kendall Woodard},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Granular, Learning Objects, K-12, Education, L2Ork, PdL2Ork},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Rating = {2},
	Title = {Granular Learning Objects for Instrument Design and Collaborative Performance in K-12 Education},
	Url = {http://www.nime.org/proceedings/2012/nime2012_315.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_315.pdf}}

@inproceedings{Jackson:2012a,
	Abstract = {This paper describes the bubble drum set, along with several polyrhythm games and interactive music activities that have been developed to show its potential for use as an input controller. The bubble drum set combines various sizes of colorful exercise balls, held in place or suspended with conventional drum hardware and thus creating a trap kit configuration in which the spherical surfaces can be struck and stroked from varying angles using sticks, brushes, or even by hands alone. The acoustic properties of these fitness balls are surprisingly rich, capable of producing subtle differences in timbre while being responsive over a wide dynamic range. The entire set has been purposefully designed to provide a player with the means to achieve a rigorous and healthy physical workout, in addition to the achieving beneficial cognitive and sensory stimulation that comes from playing music with a sensitive and expressive instrument.},
	Address = {Ann Arbor, Michigan},
	Author = {Jay Alan Jackson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Bubble Drum-agog-ing: Polyrhythm Games & Other Inter Activities},
	Url = {http://www.nime.org/proceedings/2012/nime2012_8.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_8.pdf}}

@inproceedings{Endo:2012,
	Abstract = {Tweet Harp is a musical instrument using Twitter and a laser harp. This instrument features the use of the human voice speaking tweets in Twitter as sounds for music. It is played by touching the six harp strings of laser beams. Tweet Harp gets the latest tweets from Twitter in real-time, and it creates music like a song with unexpected words. It also creates animation displaying the texts at the same time. The audience can visually enjoy this performance by sounds synchronized with animation. If the audience has a Twitter account, they can participate in the performance by tweeting.},
	Address = {Ann Arbor, Michigan},
	Author = {Ayaka Endo and Takuma Moriyama and Yasuo Kuhara},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Twitter, laser harp, text, speech, voice, AppleScript, Quartz Composer, Max/MSP, TTS, Arduino},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Tweet Harp: Laser Harp Generating Voice and Text of Real-time Tweets in Twitter},
	Url = {http://www.nime.org/proceedings/2012/nime2012_66.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_66.pdf}}

@inproceedings{Astrinaki:2012,
	Abstract = {In this paper, we describe our pioneering work in developing speech synthesis beyond the Text-To-Speech paradigm. We introduce tangible speech synthesis as an alternate way of envisioning how artificial speech content can be produced. Tangible speech synthesis refers to the ability, for a given system, to provide some physicality and interactivity to important speech production parameters. We present MAGE, our new software platform for high-quality reactive speech synthesis, based on statistical parametric modeling and more particularly hidden Markov models. We also introduce a new HandSketch-based musical instrument. This instrument brings pen and posture based interaction on the top of MAGE, and demonstrates a first proof of concept.},
	Address = {Ann Arbor, Michigan},
	Author = {Maria Astrinaki and Nicolas d'Alessandro and Thierry Dutoit},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {speech synthesis, Hidden Markov Models, tangible interaction, software library, MAGE, HTS, performative},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {MAGE --A Platform for Tangible Speech Synthesis},
	Url = {http://www.nime.org/proceedings/2012/nime2012_164.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_164.pdf}}

@inproceedings{dAlessandro:2012,
	Abstract = {We present the integration of two musical interfaces into a new music-making system that seeks to capture the expe-rience of a choir and bring it into the mobile space. This system relies on three pervasive technologies that each support a different part of the musical experience. First, the mobile device application for performing with an artificial voice, called ChoirMob. Then, a central composing and conducting application running on a local interactive display, called Vuzik. Finally, a network protocol to synchronize the two. ChoirMob musicians can perform music together at any location where they can connect to a Vuzik central conducting device displaying a composed piece of music. We explored this system by creating a chamber choir of ChoirMob performers, consisting of both experienced musicians and novices, that performed in rehearsals and live concert scenarios with music composed using the Vuzik interface.},
	Address = {Ann Arbor, Michigan},
	Author = {Nicolas d'Alessandro and Aura Pon and Johnty Wang and David Eagle and Ehud Sharlin and Sidney Fels},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {singing synthesis, mobile music, interactive display, interface design, OSC, ChoirMob, Vuzik, social music, choir},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Digital Mobile Choir: Joining Two Interfaces towards Composing and Performing Collaborative Mobile Music},
	Url = {http://www.nime.org/proceedings/2012/nime2012_310.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_310.pdf}}

@inproceedings{Lee:2012a,
	Abstract = {This paper describes recent extensions to LOLC, a text-based environment for collaborative improvisation for laptop ensembles, which integrate acoustic instrumental musicians into the environment. Laptop musicians author short commands to create, transform, and share pre-composed musical fragments, and the resulting notation is digitally displayed, in real time, to instrumental musicians to sight-read in performance. The paper describes the background and motivations of the project, outlines the design of the original LOLC environment and describes its new real-time notation components in detail, and explains the use of these new components in a musical composition, SGLC, by one of the authors.},
	Address = {Ann Arbor, Michigan},
	Author = {Sang Won Lee and Jason Freeman and Andrew Collela},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Real-Time Music Notation, Collaborative Improvisation, and Laptop Ensembles},
	Url = {http://www.nime.org/proceedings/2012/nime2012_62.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_62.pdf}}

@inproceedings{Hochenbaum:2012,
	Abstract = {In this paper we present a multimodal system for analyzing drum performance. In the first example we perform automatic drum hand recognition utilizing a technique for automatic labeling of training data using direct sensors, and only indirect sensors (e.g. a microphone) for testing. Left/Right drum hand recognition is achieved with an average accuracy of 84.95% for two performers. Secondly we provide a study investigating multimodality dependent performance metrics analysis.},
	Address = {Ann Arbor, Michigan},
	Author = {Jordan Hochenbaum and Ajay Kapur},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Multimodality, Drum stroke identification, surrogate sensors, surrogate data training, machine learning, music information retrieval, performance metrics},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Drum Stroke Computing: Multimodal Signal Processing for Drum Stroke Identification and Performance Metrics},
	Url = {http://www.nime.org/proceedings/2012/nime2012_82.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_82.pdf}}

@inproceedings{Johnston:2012,
	Abstract = {Mapping between musical interfaces, and sound engines, is integral to the nature of an interface [3]. Traditionally, musical applications for touch surfaces have directly mapped touch coordinates to control parameters. However, recent work [9] is looking at new methods of control that use relational multi-point analysis. Instead of directly using touch coordinates, which are related to a global screen space, an initial touch is used as an `anchor' to create a local coordinate space in which subsequent touches can be located and compared. This local coordinate space frees touches from being locked to one single relationship, and allows for more complex interaction between touch events. So far, this method has only been implemented on Apple computer's small capacitive touch pads. Additionally, there has yet to be a user study that directly compares [9] against mappings of touch events within global coordinate spaces. With this in mind, we have developed and evaluated two interfaces with the aim of determining and quantifying some of these differences within the context of our custom large multi-touch surfaces [1].},
	Address = {Ann Arbor, Michigan},
	Author = {Blake Johnston and Owen Vallis and Ajay Kapur},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Multi-Touch, User Study, Relational-point interface},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Comparative User Study of Two Methods of Control on a Multi-Touch Surface for Musical Expression},
	Url = {http://www.nime.org/proceedings/2012/nime2012_94.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_94.pdf}}

@inproceedings{Lee:2012b,
	Abstract = {Tok! is a collaborative acoustic instrument application for iOS devices aimed at real time percussive music making in a colocated setup. It utilizes the mobility of hand-held devices and transforms them into drumsticks to tap on flat surfaces and produce acoustic music. Tok! is also networked and consists of a shared interactive music score to which the players tap their phones, creating a percussion ensemble. Through their social interaction and real-time modifications to the music score, and through their creative selection of tapping surfaces, the players can collaborate and dynamically create interesting rhythmic music with a variety of timbres.},
	Address = {Ann Arbor, Michigan},
	Author = {Sang Won Lee and Ajay Srinivasamurthy and Gregoire Tronel and Weibin Shen and Jason Freeman},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Mobile Phones, Collaboration, Social Interaction, Acoustic Musical Instrument},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Tok! : A Collaborative Acoustic Instrument using Mobile Phones},
	Url = {http://www.nime.org/proceedings/2012/nime2012_61.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_61.pdf}}

@inproceedings{El-Shimy:2012,
	Abstract = {In this paper, we discuss the design and testing of a reactive envi-ronment for musical performance. Driven by the interpersonal in-teractions amongst musicians, our system gives users, i.e., several musicians playing together in a band, real-time control over certain aspects of their performance, enabling them to change volume lev-els dynamically simply by moving around. It differs most notably from the majority of ventures into the design of novel musical in-terfaces and installations in its multidisciplinary approach, draw-ing on techniques from Human-Computer Interaction, social sci-ences and ludology. Our User-Centered Design methodology was central to producing an interactive environment that enhances tra-ditional performance with novel functionalities. During a formal experiment, musicians reported finding our system exciting and enjoyable. We also introduce some additional interactions that can further enhance the interactivity of our reactive environment. In describing the particular challenges of working with such a unique and creative user as the musician, we hope that our approach can be of guidance to interface developers working on applications of a creative nature.},
	Address = {Ann Arbor, Michigan},
	Author = {Dalia El-Shimy and Thomas Hermann and Jeremy Cooperstock},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Reactive Environment for Dynamic Volume Control},
	Url = {http://www.nime.org/proceedings/2012/nime2012_88.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_88.pdf}}

@inproceedings{Wyse:2012,
	Abstract = {The upper limit of frequency sensitivity for vibrotactile stimulation of the fingers and hand is commonly accepted as 1 kHz. However, during the course of our research to develop a full-hand vibrotactile musical communication device for the hearing-impaired, we repeatedly found evidence suggesting sensitivity to higher frequencies. Most of the studies on which vibrotactile sensitivity are based have been conducted using sine tones delivered by point-contact actuators. The current study was designed to investigate vibrotactile sensitivity using complex signals and full, open-hand contact with a flat vibrating surface representing more natural environmental conditions. Sensitivity to frequencies considerably higher than previously reported was demonstrated for all the signal types tested. Furthermore, complex signals seem to be more easily detected than sine tones, especially at low frequencies. Our findings are applicable to a general understanding of sensory physiology, and to the development of new vibrotactile display devices for music and other applications.},
	Address = {Ann Arbor, Michigan},
	Author = {Lonce Wyse and Suranga Nanayakkara and Paul Seekings and Sim Heng Ong and Elizabeth Taylor},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Haptic Sensitivity, Hearing-impaired, Vibrotactile Threshold},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Palm-area sensitivity to vibrotactile stimuli above 1 kHz},
	Url = {http://www.nime.org/proceedings/2012/nime2012_105.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_105.pdf}}

@inproceedings{Manousakis:2012,
	Abstract = {This paper presents the system and technology developed for the distributed, micro-telematic, interactive sound art installation, The Network Is A Blind Space. The piece uses sound to explore the physical yet invisible electromagnetic spaces created by Wireless Local Area Networks (WLANs). To this end, the author created a framework for indoor WiFi localization, providing a variety of control data for various types of `musical echolocation'. This data, generated mostly by visitors exploring the installation while holding WiFi-enabled devices, is used to convey the hidden properties of wireless networks as dynamic spaces through an artistic experience.},
	Address = {Ann Arbor, Michigan},
	Author = {Stelios Manousakis},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Network music, mobile music, distributed music, interactivity, sound art installation, collaborative instrument, site-specific, electromagnetic signals, WiFi, trilateration, traceroute, echolocation, SuperCollider, Pure Data, RjDj, mapping},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Network spaces as collaborative instruments: WLAN trilateration for musical echolocation in sound art},
	Url = {http://www.nime.org/proceedings/2012/nime2012_142.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_142.pdf}}

@inproceedings{Nesfield:2012,
	Abstract = {A general strategy for encouraging embodied engagement within musical interface design is introduced. A pair of ex-ample implementations of this strategy are described, one tangible and one graphical. As part of a potentially larger set within our general approach, two separate relationships are described termed `decay and contribution' and `instability and adjustment', which are heavily dependent on the action requirements and timeliness of the interaction. By suggesting this process occurs on a timescale of less than one second it is hoped attentiveness and engagement can be en-couraged to the possible benefit of future developments in digital musical instrument design.},
	Address = {Ann Arbor, Michigan},
	Author = {James Nesfield},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {engagement, embodiment, flow, decay, instability, design, NIME},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Strategies for Engagement in Computer-Mediated Musical Performance},
	Url = {http://www.nime.org/proceedings/2012/nime2012_162.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_162.pdf}}

@inproceedings{Lympouridis:2012,
	Abstract = {Through a series of collaborative research projects usingOrient, a wireless, inertial sensor-based motion capture system,I have studied the requirements of musicians, dancers,performers and choreographers and identified various design strategies for the realization of Whole Body Interactive (WBI)performance systems. The acquired experience and knowledge led to the design and development of EnActor, prototypeWhole Body Interaction Design software. The software has been realized as a collection of modules that were proved valuable for the design of interactive performance systems that are directly controlled by the body.This paper presents EnActor's layout as a blueprint for the design and development of more sophisticated descendants.Complete video archive of my research projects in WBI performance systems at: http://www.inter-axions.com},
	Address = {Ann Arbor, Michigan},
	Author = {Vangelis Lympouridis},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Whole Body Interaction, Motion Capture, Interactive Performance Systems, Interaction Design, Software Prototype},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {EnActor: A Blueprint for a Whole Body Interaction Design Software Platform},
	Url = {http://www.nime.org/proceedings/2012/nime2012_169.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_169.pdf}}

@inproceedings{Barbosa:2012,
	Abstract = {The authors propose the development of a more complete Digital Music Instrument (DMI) evaluation methodology, which provides structured tools for the incremental development of prototypes based on user feedback. This paper emphasizes an important but often ignored stakeholder present in the context of musical performance: the audience. We demonstrate the practical application of an audience focused methodology through a case study (`Illusio'), discuss the obtained results and possible improvements for future works.},
	Address = {Ann Arbor, Michigan},
	Author = {Jer{\^o}nimo Barbosa and Filipe Calegario and Ver{\^o}nica Teichrieb and Geber Ramalho and Patrick McGlynn},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Empirical methods, quantitative, usability testing and evaluation, digital musical instruments, evaluation methodology, Illusio},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Considering Audience's View Towards an Evaluation Methodology for Digital Musical Instruments},
	Url = {http://www.nime.org/proceedings/2012/nime2012_174.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_174.pdf}}

@inproceedings{Torresen:2012,
	Abstract = {We present a new wireless transceiver board for the CUI32 sensor interface, aimed at creating a solution that is flexible, reliable, and with little power consumption. Communica-tion with the board is based on the ZigFlea protocol and it has been evaluated on a CUI32 using the StickOS oper-ating system. Experiments show that the total sensor data collection time is linearly increasing with the number of sensor samples used. A data rate of 0.8 kbit/s is achieved for wirelessly transmitting three axes of a 3D accelerometer. Although this data rate is low compared to other systems, our solution benefits from ease-of-use and stability, and is useful for applications that are not time-critical.},
	Address = {Ann Arbor, Michigan},
	Author = {Jim Torresen and {\O}yvind N. Hauback and Dan Overholt and Alexander Refsum Jensenius},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {wireless sensing, CUI32, StickOS, ZigBee, ZigFlea},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Development and Evaluation of a ZigFlea-based Wireless Transceiver Board for CUI32},
	Url = {http://www.nime.org/proceedings/2012/nime2012_205.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_205.pdf}}

@inproceedings{Makelberge:2012,
	Abstract = {"Perfect Take" is a public installation out of networked acoustic instruments that let composers from all over the world exhibit their MIDI-works by means of the Internet. The primary aim of this system is to offer composers a way to have works exhibited and recorded in venues and with technologies not accessible to him/her under normal circumstances. The Secondary aim of this research is to highlight experience design as a complement to interaction design, and a shift of focus from functionality of a specific gestural controller, towards the environments, events and processes that they are part of.},
	Address = {Ann Arbor, Michigan},
	Author = {Nicolas Makelberge and {\'A}lvaro Barbosa and Andr{\'e} Perrotta and Lu{\'\i}s Sarmento Ferreira},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, Networked Music, MIDI, Disklavier, music collaboration, creativity},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Perfect Take: Experience design and new interfaces for musical expression},
	Url = {http://www.nime.org/proceedings/2012/nime2012_208.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_208.pdf}}

@inproceedings{Gong:2012,
	Abstract = {This paper describes a novel music control sensate surface, which enables integration between any musical instruments with a v ersatile, customizable, and essentially cost-effective user interface. This sensate surface is based on c onductive inkjet printing technology which allows capacitive sensor electrodes and connections between electronics components to be printed onto a large roll of flexible substrate that is unrestricted in length. The high dynamic range capacitive sensing electrodes can not only infer touch, but near-range, non-contact gestural nuance in a music performance. With this sensate surface, users can ``cut'' out their desired shapes, ``paste'' the number of inputs, and customize their controller interface, which can then send signals wirelessly to effects or software synthesizers. We seek to find a solution for integrating the form factor of traditional music controllers seamlessly on top of one's music instrument and meanwhile adding expressiveness to the music performance by sensing and incorporating movements and gestures to manipulate the musical output. We present an example of implementation on an electric ukulele and provide several design examples to demonstrate the versatile capabilities of this system.},
	Address = {Ann Arbor, Michigan},
	Author = {Nan-Wei Gong and Nan Zhao and Joseph Paradiso},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Sensate surface, music controller skin, customizable controller surface, flexible electronics},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Customizable Sensate Surface for Music Control},
	Url = {http://www.nime.org/proceedings/2012/nime2012_201.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_201.pdf}}

@inproceedings{Subramanian:2012,
	Abstract = {This paper describes a recent addition to LOLC, a text-based environment for collaborative improvisation for laptop ensembles, incorporating a machine musician that plays along with human performers. The machine musician LOLbot analyses the patterns created by human performers and the composite music they create as they are layered in performance. Based on user specified settings, LOLbot chooses appropriate patterns to play with the ensemble, either to add contrast to the existing performance or to be coherent with the rhythmic structure of the performance. The paper describes the background and motivations of the project, outlines the design of the original LOLC environment and describes the architecture and implementation of LOLbot.},
	Address = {Ann Arbor, Michigan},
	Author = {Sidharth Subramanian and Jason Freeman and Scott McCoid},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Machine Musicianship, Live Coding, Laptop Orchestra},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {LOLbot: Machine Musicianship in Laptop Ensembles},
	Url = {http://www.nime.org/proceedings/2012/nime2012_119.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_119.pdf}}

@inproceedings{Henson:2012,
	Abstract = {This paper introduces the concept of Kugelschwung, a digital musical instrument centrally based around the use of pendulums and lasers to create unique and highly interactive electronic ambient soundscapes. Here, we explore the underlying design and physical construction of the instrument, as well as its implementation and feasibility as an instrument in the real world. To conclude, we outline potential expansions to the instrument, describing how its range of applications can be extended to accommodate a variety of musical styles.},
	Address = {Ann Arbor, Michigan},
	Author = {Jamie Henson and Benjamin Collins and Alexander Giles and Kathryn Webb and Matthew Livingston and Thomas Mortensson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {laser, pendulums, instrument design, electronic, sampler, soundscape, expressive performance},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Kugelschwung -a Pendulum-based Musical Instrument},
	Url = {http://www.nime.org/proceedings/2012/nime2012_131.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_131.pdf}}

@inproceedings{Hattwick:2012,
	Abstract = {The configurability and networking abilities of digital musical instruments increases the possibilities for collaboration in musical performances. Computer music ensembles such as laptop orchestras are becoming increasingly common and provide laboratories for the exploration of these possibilities. However, much of the literature regarding the creation of DMIs has been focused on individual expressivity, and their potential for collaborative performance has been under-utilized. This paper makes the case for the benefits of an approach to digital musical instrument design that begins with their collaborative potential, examines several frameworks and sets of principles for the creation of digital musical instruments, and proposes a dimension space representation of collaborative approaches which can be used to evaluate and guide future DMI creation. Several examples of DMIs and compositions are then evaluated and discussed in the context of this dimension space.},
	Address = {Ann Arbor, Michigan},
	Author = {Ian Hattwick and Marcelo Wanderley},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {dimension space, collaborative, digital musical instrument, dmi, digital music ensemble, dme},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {A Dimension Space for Evaluating Collaborative Musical Performance Systems},
	Url = {http://www.nime.org/proceedings/2012/nime2012_150.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_150.pdf}}

@inproceedings{Bergsland:2012,
	Abstract = {As a part of the research project Voice Meetings, a solo live-electronic vocal performance was presented for 63 students. Through a mixed method approach applying both written and oral response, feedback from one blindfolded and one seeing audience group was collected and analyzed.
There were marked differences between the groups regarding focus, in that the participants in blindfolded group tended to focus on fewer aspects, have a heightened focus and be less distracted than the seeing group. The seeing group, on its part, focused more on the technological instruments applied in the performance, the performer herself and her actions. This study also shows that there were only minor differences between the groups regarding the experience of skill and control, and argues that this observation can be explained by earlier research on skill in NIMEs.},
	Address = {Ann Arbor, Michigan},
	Author = {Andreas Bergsland and Tone {\AA}se},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Performance, audience reception, acousmatic listening, live-electronics, voice, qualitative research},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Using a seeing/blindfolded paradigm to study audience experiences of live-electronic performances with voice},
	Url = {http://www.nime.org/proceedings/2012/nime2012_168.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_168.pdf}}

@inproceedings{Tache:2012,
	Abstract = {Force-feedback and physical modeling technologies now allow to achieve the same kind of relation with virtual instruments as with acoustic instruments, but the design of such elaborate models needs guidelines based on the study of the human sensory-motor system and behaviour. This article presents a qualitative study of a simulated instrumental interaction in the case of the virtual bowed string, using both waveguide and mass-interaction models. Subjects were invited to explore the possibilities of the simulations and to express themselves verbally at the same time, allowing us to identify key qualities of the proposed systems that determine the construction of an intimate and rich relationship with the users.},
	Address = {Ann Arbor, Michigan},
	Author = {Olivier Tache and Stephen Sinclair and Jean-Loup Florens and Marcelo Wanderley},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Instrumental interaction, presence, force-feedback, physical modeling, simulation, haptics, bowed string.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Exploring audio and tactile qualities of instrumentality with bowed string simulations},
	Url = {http://www.nime.org/proceedings/2012/nime2012_243.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_243.pdf}}

@inproceedings{Hollinger:2012,
	Abstract = {A modular and reconfigurable hardware platform for analog optoelectronic signal acquisition is presented. Its intended application is for fiber optic sensing in electronic musical interfaces, however the flexible design enables its use with a wide range of analog and digital sensors. Multiple gain and multiplexing stages as well as programmable analog and digital hardware blocks allow for the acquisition, processing, and communication of single-ended and differential signals. Along with a hub board, multiple acquisition boards can be connected to modularly extend the system's capabilities to suit the needs of the application. Fiber optic sensors and their application in DMIs are briefly discussed, as well as the use of the hardware platform with specific musical interfaces.},
	Address = {Ann Arbor, Michigan},
	Author = {Avrum Hollinger and Marcelo M. Wanderley},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {fiber optic sensing, analog signal acquisition, musical interface, MRI-compatible},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Optoelectronic Acquisition and Control Board for Musical Applications},
	Url = {http://www.nime.org/proceedings/2012/nime2012_228.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_228.pdf}}

@inproceedings{Giordano:2012,
	Abstract = {Force-feedback devices can provide haptic feedback duringinteraction with physical models for sound synthesis. However, low-end devices may not always provide high-fidelitydisplay of the acoustic characteristics of the model. This article describes an enhanced handle for the Phantom Omnicontaining a vibration actuator intended to display the highfrequency portion of the synthesized forces. Measurementsare provided to show that this approach achieves a morefaithful representation of the acoustic signal, overcominglimitations in the device control and dynamics.},
	Address = {Ann Arbor, Michigan},
	Author = {Marcello Giordano and Stephen Sinclair and Marcelo M. Wanderley},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Haptics, force feedback, bowing, audio, interaction},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Bowing a vibration-enhanced force feedback device},
	Url = {http://www.nime.org/proceedings/2012/nime2012_37.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_37.pdf}}

@inproceedings{Surges:2012,
	Abstract = {This paper describes three hardware devices for integrating modular synthesizers with computers, each with a different approach to the relationship between hardware and software. The devices discussed are the USB-Octomod, an 8-channel OSC-compatible computer-controlled control-voltage generator, the tabulaRasa, a hardware table-lookup oscillator synthesis module with corresponding waveform design software, and the pucktronix.snake.corral, a dual 8x8 computer-controlled analog signal routing matrix. The devices make use of open-source hardware and software, and are designed around affordable micro-controllers and integrated circuits. },
	Address = {Ann Arbor, Michigan},
	Author = {Greg Surges},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {DIY Hybrid Analog/Digital Modular Synthesis},
	Url = {http://www.nime.org/proceedings/2012/nime2012_9.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_9.pdf}}

@inproceedings{Mayton:2012,
	Abstract = {We present Patchwerk, a networked synthesizer module with tightly coupled web browser and tangible interfaces. Patchwerk connects to a pre-existing modular synthesizer using the emerging cross-platform HTML5 WebSocket standard to enable low-latency, high-bandwidth, concurrent control of analog signals by multiple users. Online users control physical outputs on a custom-designed cabinet that reflects their activity through a combination of motorized knobs and LEDs, and streams the resultant audio. In a typical installation, a composer creates a complex physical patch on the modular synth that exposes a set of analog and digital parameters (knobs, buttons, toggles, and triggers) to the web-enabled cabinet. Both physically present and online audiences can control those parameters, simultane-ously seeing and hearing the results of each other's actions. By enabling collaborative interaction with a massive analog synthesizer, Patchwerk brings a broad audience closer to a rare and historically important instrument. Patchwerk is available online at http://synth.media.mit.edu.},
	Address = {Ann Arbor, Michigan},
	Author = {Brian Mayton and Gershon Dublon and Nicholas Joliat and Joseph A. Paradiso},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Modular synthesizer, HTML5, tangible interface, collaborative musical instrument},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Patchwork: Multi-User Network Control of a Massive Modular Synthesizer},
	Url = {http://www.nime.org/proceedings/2012/nime2012_293.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_293.pdf}}

@inproceedings{Jaimovich:2012,
	Abstract = {In order to further understand our emotional reaction to music, a museum-based installation was designed to collect physiological and self-report data from people listening to music. This demo will describe the technical implementation of this installation as a tool for collecting large samples of data in public spaces. The Emotion in Motion terminal is built upon a standard desktop computer running Max/MSP and using sensors that measure physiological indicators of emotion that are connected to an Arduino. The terminal has been installed in museums and galleries in Europe and the USA, helping create the largest database of physiology and self-report data while listening to music.},
	Address = {Ann Arbor, Michigan},
	Author = {Javier Jaimovich and Miguel Ortiz and Niall Coghlan and R. Benjamin Knapp},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Biosignals, EDA, SC, GSR, HR, POX, Self-Report, Database, Physiological Signals, Max/MSP, FTM, SAM, GEMS},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Emotion in Motion Experiment: Using an Interactive Installation as a Means for Understanding Emotional Response to Music},
	Url = {http://www.nime.org/proceedings/2012/nime2012_254.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_254.pdf}}

@inproceedings{McGlynn:2012,
	Abstract = {This paper contends that the development of expressive performance interfaces using multi-touch technology has been hindered by an over-reliance upon GUI paradigms. Despite offering rich and robust data output and multiple ways to interpret it, approaches towards using multi-touch technology in digit al musical inst rument design have been markedly conservative, showing a strong tendency towards modeling existing hardware. This not only negates many of the benefits of multi-touch technology but also creates specific difficulties in the context of live music performance. A case study of two other interface types that have seen considerable musical use --the XY pad and button grid --illustrates the manner in which the implicit characteristics of a device determine the conditions under which it will favorably perform. Accordingly, this paper proposes an alternative approach to multi-touch which emp hasizes the imp licit strengths of the technology and establishes a philosophy of design around them. Finally, we introduce two toolkits currently being used to assess the validity of this approach.},
	Address = {Ann Arbor, Michigan},
	Author = {Patrick McGlynn and Victor Lazzarini and Gordon Delap and Xiaoyu Chen},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Multi-touch, controllers, mapping, gesture, GUIs, physical interfaces, perceptual & cognitive issues},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Recontextualizing the Multi-touch Surface},
	Url = {http://www.nime.org/proceedings/2012/nime2012_132.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_132.pdf}}

@inproceedings{Smith:2012a,
	Abstract = {Machine learning models are useful and attractive tools for
the interactive computer musician, enabling a breadth of interfaces and instruments. With current consumer hardware
it becomes possible to run advanced machine learning algorithms in demanding performance situations, yet expertise
remains a prominent entry barrier for most would-be users.
Currently available implementations predominantly employ
supervised machine learning techniques, while the adaptive,
self-organizing capabilities of unsupervised models are not
generally available. We present a free, new toolbox of unsupervised machine learning algorithms implemented in Max
5 to support real-time interactive music and video, aimed
at the non-expert computer artist.},
	Address = {Ann Arbor, Michigan},
	Author = {Benjamin D. Smith and Guy E. Garnett},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Unsupervised Play: Machine Learning Toolkit for Max},
	Url = {http://www.nime.org/proceedings/2012/nime2012_68.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_68.pdf}}

@inproceedings{Levinson:2012,
	Abstract = {TedStick is a new wireless musical instrument that processes acoustic sounds resonating within its wooden body and ma-nipulates them via gestural movements. The sounds are transduced by a piezoelectric sensor inside the wooden body, so any tactile contact with TedStick is transmitted as audio and further processed by a computer. The main method for performing with TedStick focuses on extracting diverse sounds from within the resonant properties of TedStick it-self. This is done by holding TedStick in one hand and a standard drumstick in the opposite hand while tapping, rubbing, or scraping the two against each other. Gestural movements of TedStick are then mapped to parameters for several sound effects including pitch shift, delay, reverb and low/high pass filters. Using this technique the hand holding the drumstick can control the acoustic sounds/interaction between the sticks while the hand holding TedStick can fo-cus purely on controlling the sound manipulation and effects parameters.},
	Address = {Ann Arbor, Michigan},
	Author = {Cory Levinson},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {tangible user interface, piezoelectric sensors, gestural per-formance, digital sound manipulation},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {TedStick: A Tangible Electrophonic Drumstick},
	Url = {http://www.nime.org/proceedings/2012/nime2012_96.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_96.pdf}}

@inproceedings{Hattwick:2012a,
	Abstract = {The Physical Computing Ensemble was created in order to determine the viability of an approach to musical performance which focuses on the relationships and interactions of the performers. Three performance systems utilizing gestural controllers were designed and implemented, each with a different strategy for performer interaction.
These strategies took advantage of the opportunities for collaborative performance inherent in digital musical instruments due to their networking abilities and reconfigurable software. These characteristics allow for the easy implementation of varying approaches to collaborative performance. Ensembles who utilize digital musical instruments provide a fertile environment for the design, testing, and utilization of collaborative performance systems.
The three strategies discussed in this paper are the parameterization of musical elements, turn-based collaborative control of sound, and the interaction of musical systems created by multiple performers. Design principles, implementation, and a performance using these strategies are discussed, and the conclusion is drawn that performer interaction and collaboration as a primary focus for system design, composition, and performance is viable.},
	Address = {Ann Arbor, Michigan},
	Author = {Ian Hattwick and Kojiro Umezaki},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Collaborative performance, interaction, digital musical instruments, gestural controller, digital music ensemble, Wii},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Approaches to Interaction in a Digital Music Ensemble},
	Url = {http://www.nime.org/proceedings/2012/nime2012_153.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_153.pdf}}

@inproceedings{Hansen:2012,
	Abstract = {This paper presents the results of user interaction with two explorative music environments (sound system A and B) that were inspired from the Banda Linda music tradition in two different ways. The sound systems adapted to how a team of two players improvised and made a melody together in an interleaved fashion: Systems A and B used a fuzzy logic algorithm and pattern recognition to respond with modifications of a background rhythms. In an experiment with a pen tablet interface as the music instrument, users aged 10-13 were to tap tones and continue each other's melody. The sound systems rewarded users sonically, if they managed to add tones to their mutual melody in a rapid turn taking manner with rhythmical patterns. Videos of experiment sessions show that user teams contributed to a melody in ways that resemble conversation. Interaction data show that each sound system made player teams play in different ways, but players in general had a hard time adjusting to a non-Western music tradition. The paper concludes with a comparison and evaluation of the two sound systems. Finally it proposes a new approach to the design of collaborative and shared music environments that is based on ''listening applications''.},
	Address = {Ann Arbor, Michigan},
	Author = {Anne-Marie Skriver Hansen and Hans J{\o}rgen Andersen and Pirkko Raudaskoski},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Music improvisation, novices, social learning, interaction studies, interaction design.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Two Shared Rapid Turn Taking Sound Interfaces for Novices},
	Url = {http://www.nime.org/proceedings/2012/nime2012_123.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_123.pdf}}

@inproceedings{Roberts:2012,
	Address = {Ann Arbor, Michigan},
	Annote = {Designing mobile interfaces for computer-based musical performance is generally a time-consuming task that can be exasperating for performers. Instead of being able to experiment freely with physical interfaces' affordances, performers must spend time and attention on non-musical tasks including network configuration, development environments for the mobile devices, defining OSC address spaces, and handling the receipt of OSC in the environment that will control and produce sound. Our research seeks to overcome such obstacles by minimizing the code needed to both generate and read the output of interfaces on mobile devices. For iOS and Android devices, our implementation extends the application Control to use a simple set of OSC messages to define interfaces and automatically route output. On the desktop, our implementations in Max/MSP/Jitter, LuaAV, and Su-perCollider allow users to create mobile widgets mapped to sonic parameters with a single line of code. We believe the fluidity of our approach will encourage users to incorporate mobile devices into their everyday performance practice.},
	Author = {Charles Roberts and Graham Wakefield and Matt Wright},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, OSC, Zeroconf, iOS, Android, Max/MSP/Jitter, LuaAV, SuperCollider, Mobile},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Mobile Controls On-The-Fly: An Abstraction for Distributed NIMEs},
	Url = {http://www.nime.org/proceedings/2012/nime2012_303.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_303.pdf}}

@inproceedings{Buschert:2012,
	Abstract = {Musician Maker is a system to allow novice players the opportunity to create expressive improvisational music. While the system plays an accompaniment background chord progression, each participant plays some kind of controller to make music through the system. The program takes the signals from the controllers and adjusts the pitches somewhat so that the players are limited to notes which fit the chord progression. The various controllers are designed to be very easy and intuitive so anyone can pick one up and quickly be able to play it. Since the computer is making sure that wrong notes are avoided, even inexperienced players can immediately make music and enjoy focusing on some of the more expressive elements and thus become musicians.},
	Address = {Ann Arbor, Michigan},
	Author = {John Buschert},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Musical Instrument, Electronic, Computer Music, Novice, Controller},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Musician Maker: Play expressive music without practice},
	Url = {http://www.nime.org/proceedings/2012/nime2012_36.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_36.pdf}}

@inproceedings{Han:2012a,
	Abstract = {FutureGrab is a new wearable musical instrument for live performance that is highly intuitive while still generating an interesting sound by subtractive synthesis. Its sound effects resemble the human vowel pronunciation, which were mapped to hand gestures that are similar to the mouth shape of human to pronounce corresponding vowel. FutureGrab also provides all necessary features for a lead musical instrument such as pitch control, trigger, glissando and key adjustment. In addition, pitch indicator was added to give visual feedback to the performer, which can reduce the mistakes during live performances. This paper describes the motivation, system design, mapping strategy and implementation of FutureGrab, and evaluates the overall experience.},
	Address = {Ann Arbor, Michigan},
	Author = {Yoonchang Han and Jinsoo Na and Kyogu Lee},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Wearable musical instrument, Pure Data, gestural synthesis, formant synthesis, data-glove, visual feedback, subtractive synthesis},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {FutureGrab: A wearable subtractive synthesizer using hand gesture},
	Url = {http://www.nime.org/proceedings/2012/nime2012_209.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_209.pdf}}

@inproceedings{Carey:2012,
	Abstract = {This paper presents the author's derivations system, an interactive performance system for solo improvising instrumentalist. The system makes use of a combination of real-time audio analysis, live sampling and spectral re-synthesis to build a vocabulary of possible performative responses to live instrumental input throughout an improvisatory performance. A form of timbral matching is employed to form a link between the live performer and an expanding database of musical materials. In addition, the system takes into account the unique nature of the rehearsal/practice space in musical performance through the implementation of performer-configurable cumulative rehearsal databases into the final design. This paper discusses the system in detail with reference to related work in the field, making specific reference to the system's interactive potential both inside and outside of a real-time performance context.},
	Address = {Ann Arbor, Michigan},
	Author = {Benjamin Carey},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Interactivity, performance systems, improvisation},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Designing for Cumulative Interactivity: The _derivations System},
	Url = {http://www.nime.org/proceedings/2012/nime2012_292.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_292.pdf}}

@inproceedings{Senturk:2012,
	Abstract = {Meaning crossword of sound, Crossole is a musical meta-instrument where the music is visualized as a set of virtual blocks that resemble a crossword puzzle. In Crossole, the chord progressions are visually presented as a set of virtual blocks. With the aid of the Kinect sensing technology, a performer controls music by manipulating the crossword blocks using hand movements. The performer can build chords in the high level, traverse over the blocks, step into the low level to control the chord arpeggiations note by note, loop a chord progression or map gestures to various processing algorithms to enhance the timbral scenery.},
	Address = {Ann Arbor, Michigan},
	Author = {Sertan {\c S}ent{\"u}rk and Sang Won Lee and Avinash Sastry and Anosh Daruwalla and Gil Weinberg},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Kinect, meta-instrument, chord progression, body gesture},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Crossole: A Gestural Interface for Composition, Improvisation and Performance using Kinect},
	Url = {http://www.nime.org/proceedings/2012/nime2012_185.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_185.pdf}}

@inproceedings{Savary:2012,
	Abstract = {Dirty Tangible Interfaces (DIRTI) are a new concept in interface design that forgoes the dogma of repeatability in favor of a richer and more complex experience, constantly evolving, never reversible, and infinitely modifiable. We built a prototype based on granular or liquid interaction material placed in a glass dish, that is analyzed by video tracking for its 3D relief. This relief, and the dynamic changes applied to it by the user, are interpreted as activation profiles to drive corpus-based concatenative sound synthesis, allowing one or more players to mold sonic landscapes and to plow through them in an inherently collaborative, expressive, and dynamic experience.},
	Address = {Ann Arbor, Michigan},
	Author = {Matthieu Savary and Diemo Schwarz and Denis Pellerin},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Tangible interface, Corpus-based concatenative synthesis, Non-standard interaction},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {DIRTI ---Dirty Tangible Interfaces},
	Url = {http://www.nime.org/proceedings/2012/nime2012_212.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_212.pdf}}

@inproceedings{Schlessinger:2012,
	Abstract = {We have developed a prototype wireless microphone that provides vocalists with control over their vocal effects directly from the body of the microphone. A wireless microphone has been augmented with six momentary switches, one fader, and three axes of motion and position sensors, all of which provide MIDI output from the wireless receiver. The MIDI data is used to control external vocal effects units such as live loopers, reverbs, distortion pedals, etc. The goal was to to provide dramatically increased expressive control to vocal performances, and address some of the shortcomings of pedal-controlled effects. The addition of gestural controls from the motion sensors opens up new performance possibilities such as panning the voice simply by pointing the microphone in one direction or another. The result is a hybrid microphone-musical instrument which has recieved extremely positive results from vocalists in numerous infor-mal workshops.},
	Address = {Ann Arbor, Michigan},
	Author = {Dan Moses Schlessinger},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {NIME, Sennheiser, Concept Tahoe, MIDI, control, microphone},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Concept Tahoe: Microphone Midi Control},
	Url = {http://www.nime.org/proceedings/2012/nime2012_202.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_202.pdf}}

@inproceedings{Choi:2012,
	Abstract = {The Deckle Group1 is an ensemble that designs, builds and performs on electroacoustic drawing boards. These draw-ing surfaces are augmented with Satellite CCRMA Beagle-Boards and Arduinos2.[1] Piezo microphones are used in conjunction with other sensors to produce sounds that are coupled tightly to mark-making gestures. Position tracking is achieved with infra-red object tracking, conductive fabric and a magnetometer.},
	Address = {Ann Arbor, Michigan},
	Author = {Hongchan Choi and John Granzow and Joel Sadler},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Deckle, BeagleBoard, Drawing, Sonification, Performance, Audiovisual, Gestural Interface},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Deckle Project : A Sketch of Three Sensors},
	Url = {http://www.nime.org/proceedings/2012/nime2012_214.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_214.pdf}}

@inproceedings{Gerhard:2012,
	Abstract = {The Instant Instrument Anywhere (IIA) is a small device which can be attached to any metal object to create an electronic instrument. The device uses capacitive sensing to detect proximity of the player's body to the metal object, and sound is generated through a surface transducer which can be attached to any flat surface. Because the capacitive sensor can be any shape or size, absolute capacitive thresholding is not possible since the baseline capacitance will change. Instead, we use a differential-based moving sum threshold which can rapidly adjust to changes in the environment or be re-calibrated to a new metal object. We show that this dynamic threshold is effective in rejecting environmental noise and rapidly adapting to new objects. We also present details for constructing Instant Instruments Anywhere, including using smartphone as the synthesis engine and power supply.},
	Address = {Ann Arbor, Michigan},
	Author = {David Gerhard and Brett Park},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Capacitive Sensing, Arduino},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Instant Instrument Anywhere: A Self-Contained Capacitive Synthesizer},
	Url = {http://www.nime.org/proceedings/2012/nime2012_223.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_223.pdf}}

@inproceedings{Fyfe:2012,
	Abstract = {Message mapping between control interfaces and sound engines is an important task that could benefit from tools that streamline development. A new Open Sound Control (OSC) namespace called Nexus Data Exchange Format (NDEF) streamlines message mapping by offering developers the ability to manage sound engines as network nodes and to query those nodes for the messages in their OSC address spaces. By using NDEF, developers will have an eas-ier time managing nodes and their messages, especially for scenarios in which a single application or interface controls multiple sound engines. NDEF is currently implemented in the JunctionBox interaction toolkit but could easily be implemented in other toolkits.},
	Address = {Ann Arbor, Michigan},
	Author = {Lawrence Fyfe and Adam Tindale and Sheelagh Carpendale},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {OSC, namespace, interaction, node},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Node and Message Management with the JunctionBox Interaction Toolkit},
	Url = {http://www.nime.org/proceedings/2012/nime2012_299.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_299.pdf}}

@inproceedings{Lee:2012,
	Abstract = {Empatheater is a video playing system that is controlled by multimodal interaction. As the video is played, the user must interact and emulate predefined ``events'' for the video to continue on. The user is given the illusion of playing an active role in the unraveling video content and can empathize with the performer. In this paper, we report about user experiences with Empatheater when applied to musical video contents.},
	Address = {Ann Arbor, Michigan},
	Author = {Myunghee Lee and Youngsun Kim and Gerard Kim},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Music video, Empathy, Interactive video, Musical event, Multimodal interaction.},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {Empathetic Interactive Music Video Experience},
	Url = {http://www.nime.org/proceedings/2012/nime2012_179.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_179.pdf}}

@inproceedings{Freed:2012,
	Abstract = {The Fingerphone, a reworking of the Stylophone in conductive paper, is presented as an example of new design approaches for sustainability and playability of electronic musical instruments.},
	Address = {Ann Arbor, Michigan},
	Author = {Adrian Freed},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	Keywords = {Stylophone, Conductive Paper, Pressure Sensing, Touch Sensing, Capacitive Sensing, Plurifunctionality, Fingerphone, Sustainable Design},
	Month = {May 21-23},
	Publisher = {University of Michigan},
	Title = {The Fingerphone: a Case Study of Sustainable Instrument Redesign},
	Url = {http://www.nime.org/proceedings/2012/nime2012_264.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2012/nime2012_264.pdf}}
