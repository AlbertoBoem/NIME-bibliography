@inproceedings{Aaron2011,
author = {Aaron, Samuel and Blackwell, Alan and Hoadley, Richard and Regan, Tim},
url = {http://www.nime.org/proceedings/2011/nime2011_381.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Principled Approach to Developing New Languages for Live Coding},
year = {2011},
abstract = {This paper introduces Improcess, a novel cross-disciplinarycollaborative project focussed on the design and development of tools to structure the communication between performer and musical process. We describe a 3-tiered architecture centering around the notion of a Common MusicRuntime, a shared platform on top of which inter-operatingclient interfaces may be combined to form new musical instruments. This approach allows hardware devices such asthe monome to act as an extended hardware interface withthe same power to initiate and control musical processesas a bespoke programming language. Finally, we reflect onthe structure of the collaborative project itself, which offers an opportunity to discuss general research strategy forconducting highly sophisticated technical research within aperforming arts environment such as the development of apersonal regime of preparation for performance.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Improvisation, live coding, controllers, monome, collabora- tion, concurrency, abstractions },
pages = {381--386},
}
@inproceedings{Ahola2011,
author = {Ahola, Tom and Tahiroglu, Koray and Ahmaniemi, Teemu and Belloni, Fabio and Ranki, Ville},
url = {http://www.nime.org/proceedings/2011/nime2011_433.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Raja - A Multidisciplinary Artistic Performance},
year = {2011},
abstract = {Motion-based interactive systems have long been utilizedin contemporary dance performances. These performancesbring new insight to sound-action experiences in multidisciplinary art forms. This paper discusses the related technology within the framework of the dance piece, Raja. The performance set up of Raja gives a possibility to use two complementary tracking systems and two alternative choices formotion sensors in real-time audio-visual synthesis.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {raja, performance, dance, motion sensor, accelerometer, gyro, positioning, sonification, pure data, visualization, Qt },
pages = {433--436},
}
@inproceedings{Albin2011,
author = {Albin, Aaron and Sent\"{u}rk, Sertan and {Van Troyer}, Akito and Blosser, Brian and Jan, Oliver and Weinberg, Gil},
url = {http://www.nime.org/proceedings/2011/nime2011_112.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Beatscape , a Mixed Virtual-Physical Environment for Musical Ensembles},
year = {2011},
abstract = {A mixed media tool was created that promotes ensemblevirtuosity through tight coordination and interdepence inmusical performance. Two different types of performers interact with a virtual space using Wii remote and tangibleinterfaces using the reacTIVision toolkit [11]. One group ofperformers uses a tangible tabletop interface to place andmove sound objects in a virtual environment. The soundobjects are represented by visual avatars and have audiosamples associated with them. A second set of performersmake use of Wii remotes to create triggering waves thatcan collide with those sound objects. Sound is only produced upon collision of the waves with the sound objects.What results is a performance in which users must negotiate through a physical and virtual space and are positionedto work together to create musical pieces.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {reacTIVision, processing, ensemble, mixed media, virtual- ization, tangible, sample },
pages = {112--115},
}
@inproceedings{Ando2011,
author = {Ando, Daichi},
url = {http://www.nime.org/proceedings/2011/nime2011_076.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Improving User-Interface of Interactive EC for Composition-Aid by means of Shopping Basket Procedure},
year = {2011},
abstract = {The use of Interactive Evolutionary Computation(IEC) issuitable to the development of art-creation aid system forbeginners. This is because of important features of IEC,like the ability of optimizing with ambiguous evaluationmeasures, and not requiring special knowledge about artcreation. With the popularity of Consumer Generated Media, many beginners in term of art-creation are interestedin creating their own original art works. Thus developing ofuseful IEC system for musical creation is an urgent task.However, user-assist functions for IEC proposed in pastworks decrease the possibility of getting good unexpectedresults, which is an important feature of art-creation withIEC. In this paper, The ,
,
author proposes a new IEC evaluation process named "Shopping Basket" procedure IEC.In the procedure, an user-assist function called SimilarityBased Reasoning allows for natural evaluation by the user.The function reduces user's burden without reducing thepossibility of unexpected results. The ,
,
author performs anexperiment where subjects use the new interface to validateit. As a result of the experiment, the ,
,
author concludes thethe new interface is better to motivate users to composewith IEC system than the old interface.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Interactive Evolutionary Computation, User-Interface, Com- position Aid },
pages = {76--79},
}
@inproceedings{Angel2011,
author = {Angel, Claudia R.},
url = {http://www.nime.org/proceedings/2011/nime2011_421.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Creating Interactive Multimedia Works with Bio-data},
year = {2011},
abstract = {This paper deals with the usage of bio-data from performers to create interactive multimedia performances or installations. It presents this type of research in some art works produced in the last fifty years (such as Lucier's Music for a Solo Performance, from 1965), including two interactive performances of my ,
,
authorship, which use two different types of bio-interfaces: on the one hand, an EMG (Electromyography) and on the other hand, an EEG (electroencephalography). The paper explores the interaction between the human body and real-time media (audio and visual) by the usage of bio-interfaces. This research is based on biofeedback investigations pursued by the psychologist Neal E. Miller in the 1960s, mainly based on finding new methods to reduce stress. However, this article explains and shows examples in which biofeedback research is used for artistic purposes only. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Live electronics, Butoh, performance, biofeedback, interactive sound and video.  },
pages = {421--424},
}
@inproceedings{Baath2011,
author = {B\aa\aath, Rasmus and Strandberg, Thomas and Balkenius, Christian},
url = {http://www.nime.org/proceedings/2011/nime2011_441.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Eye Tapping : How to Beat Out an Accurate Rhythm using Eye Movements},
year = {2011},
abstract = {The aim of this study was to investigate how well subjectsbeat out a rhythm using eye movements and to establishthe most accurate method of doing this. Eighteen subjectsparticipated in an experiment were five different methodswere evaluated. A fixation based method was found to bethe most accurate. All subjects were able to synchronizetheir eye movements with a given beat but the accuracywas much lower than usually found in finger tapping studies. Many parts of the body are used to make music but sofar, with a few exceptions, the eyes have been silent. The research presented here provides guidelines for implementingeye controlled musical interfaces. Such interfaces would enable performers and artists to use eye movement for musicalexpression and would open up new, exiting possibilities.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Rhythm, Eye tracking, Sensorimotor synchronization, Eye tapping },
pages = {441--444},
}
@inproceedings{Barenca2011,
author = {Barenca, Adri\'{a}n and Torre, Giuseppe},
url = {http://www.nime.org/proceedings/2011/nime2011_232.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Manipuller : Strings Manipulation and Multi- Dimensional Force Sensing},
year = {2011},
abstract = {The Manipuller is a novel Gestural Controller based on strings manipulation and multi-dimensional force sensing technology. This paper describes its motivation, design and operational principles along with some of its musical applications. Finally the results of a preliminary usability test are presented and discussed. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {1,and force sensors within,force sensing,gestural,gestural controller,manipulation,strings,strings and force sensing,the integration of strings},
pages = {232--235},
}
@inproceedings{Beck2011,
author = {Beck, Stephen D. and Branton, Chris and Maddineni, Sharath},
url = {http://www.nime.org/proceedings/2011/nime2011_207.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Tangible Performance Management of Grid-based Laptop Orchestras},
year = {2011},
abstract = {Laptop Orchestras (LOs) have recently become a very popular mode of musical expression. They engage groups ofperformers to use ordinary laptop computers as instrumentsand sound sources in the performance of specially createdmusic software. Perhaps the biggest challenge for LOs isthe distribution, management and control of software acrossheterogeneous collections of networked computers. Software must be stored and distributed from a central repository, but launched on individual laptops immediately beforeperformance. The GRENDL project leverages proven gridcomputing frameworks and approaches the Laptop Orchestra as a distributed computing platform for interactive computer music. This allows us to readily distribute softwareto each laptop in the orchestra depending on the laptop'sinternal configuration, its role in the composition, and theplayer assigned to that computer. Using the SAGA framework, GRENDL is able to distribute software and managesystem and application environments for each composition.Our latest version includes tangible control of the GRENDLenvironment for a more natural and familiar user experience.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {laptop orchestra, tangible interaction, grid computing },
pages = {207--210},
}
@inproceedings{Berdahl2011,
author = {Berdahl, Edgar and Chafe, Chris},
url = {http://www.nime.org/proceedings/2011/nime2011_322.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Autonomous New Media Artefacts ( AutoNMA )},
year = {2011},
abstract = {The purpose of this brief paper is to revisit the question oflongevity in present experimental practice and coin the termautonomous new media artefacts (AutoNMA), which arecomplete and independent of external computer systems,so they can be operable for a longer period of time andcan be demonstrated at a moment's notice. We argue thatplatforms for prototyping should promote the creation ofAutoNMA to make extant the devices which will be a partof the future history of new media.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {autonomous, standalone, Satellite CCRMA, Arduino },
pages = {322--323},
}
@inproceedings{Berdahl2011a,
author = {Berdahl, Edgar and Ju, Wendy},
url = {http://www.nime.org/proceedings/2011/nime2011_173.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform},
year = {2011},
abstract = {This paper describes a new Beagle Board-based platform forteaching and practicing interaction design for musical applications. The migration from desktop and laptop computerbased sound synthesis to a compact and integrated control, computation and sound generation platform has enormous potential to widen the range of computer music instruments and installations that can be designed, and improvesthe portability, autonomy, extensibility and longevity of designed systems. We describe the technical features of theSatellite CCRMA platform and contrast it with personalcomputer-based systems used in the past as well as emergingsmart phone-based platforms. The advantages and tradeoffs of the new platform are considered, and some projectwork is described.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {arduino,beagle board,instruments omap,linux,microcontrollers,music controllers,nime,pd,pedagogy,texas},
pages = {173--178},
}
@inproceedings{Bergsland2011,
author = {Bergsland, Andreas},
url = {http://www.nime.org/proceedings/2011/nime2011_523.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {phrases from Paul Lansky ’ s Six Fantasies},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {LPC, software instrument, analysis, modeling, csound  },
pages = {523--526},
}
@inproceedings{Berthaut2011,
author = {Berthaut, Florent and Katayose, Haruhiro and Wakama, Hironori and Totani, Naoyuki and Sato, Yuichi},
url = {http://www.nime.org/proceedings/2011/nime2011_044.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {First Person Shooters as Collaborative Multiprocess Instruments},
year = {2011},
abstract = {First Person Shooters are among the most played computer videogames. They combine navigation, interaction and collaboration in3D virtual environments using simple input devices, i.e. mouseand keyboard. In this paper, we study the possibilities broughtby these games for musical interaction. We present the Couacs, acollaborative multiprocess instrument which relies on interactiontechniques used in FPS together with new techniques adding theexpressiveness required for musical interaction. In particular, theFaders For All game mode allows musicians to perform patternbased electronic compositions.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {the couacs, fps, first person shooters, collaborative, 3D interaction, multiprocess instrument },
pages = {44--47},
}
@inproceedings{Beyer2011,
author = {Beyer, Gilbert and Meier, Max},
url = {http://www.nime.org/proceedings/2011/nime2011_507.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Music Interfaces for Novice Users : Composing Music on a Public Display with Hand Gestures},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Interactive music, public displays, user experience, out-of- home media, algorithmic composition, soft constraints  },
pages = {507--510},
}
@inproceedings{Bisig2011,
author = {Bisig, Daniel and Schacher, Jan C. and Neukom, Martin},
url = {http://www.nime.org/proceedings/2011/nime2011_260.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Flowspace – A Hybrid Ecosystem},
year = {2011},
abstract = {In this paper an audio-visual installation is discussed, which combines interactive, immersive and generative elements. After introducing some of the challenges in the field of Generative Art and placing the work within its research context, conceptual reflections are made about the spatial, behavioural, perceptual and social issues that are raised within the entire installation. A discussion about the artistic content follows, focussing on the scenography and on working with flocking algorithms in general, before addressing three specific pieces realised for the exhibition. Next the technical implementation for both hardand software are detailed before the idea of a hybrid ecosystem gets discussed and further developments outlined.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Generative Art, Interactive Environment, Immersive Installation, Swarm Simulation, Hybrid Ecosystem  },
pages = {260--263},
}
@inproceedings{Bokesoy2011,
author = {B\"{o}kesoy, Sinan and Adler, Patrick},
url = {http://www.nime.org/proceedings/2011/nime2011_052.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {1city1001vibrations : Development of a Interactive Sound Installation with Robotic Instrument Performance},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Sound installation, robotic music, interactive systems  },
pages = {52--55},
}
@inproceedings{Bokowiec2011,
author = {Bokowiec, Mark A.},
url = {http://www.nime.org/proceedings/2011/nime2011_040.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {V'OCT (Ritual): An Interactive Vocal Work for Bodycoder System and 8 Channel Spatialization},
year = {2011},
abstract = {V'OCT(Ritual) is a work for solo vocalist/performer and Bodycoder System, composed in residency at Dartington College of Arts (UK) Easter 2010. This paper looks at the technical and compositional methodologies used in the realization of the work, in particular, the choices made with regard to the mapping of sensor elements to various spatialization functions. Kinaesonics will be discussed in relation to the coding of real-time one-to-one mapping of sound to gesture and its expression in terms of hardware and software design. Four forms of expressivity arising out of interactive work with the Bodycoder system will be identified. How sonic (electro-acoustic), programmed, gestural (kinaesonic) and in terms of the V'Oct(Ritual) vocal expressivities are constructed as pragmatic and tangible elements within the compositional practice will be discussed and the subsequent importance of collaboration with a performer will be exposed. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Bodycoder, Kinaesonics, Expressivity, Gestural Control,  Interactive Performance Mechanisms, Collaboration.   },
pages = {40--43},
}
@inproceedings{Brandtsegg2011,
author = {Brandtsegg, \O yvind and Saue, Sigurd and Johansen, Thom},
url = {http://www.nime.org/proceedings/2011/nime2011_316.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Modulation Matrix for Complex Parameter Sets},
year = {2011},
abstract = {The article describes a flexible mapping technique realized as a many-to-many dynamic mapping matrix. Digital sound generation is typically controlled by a large number of parameters and efficient and flexible mapping is necessary to provide expressive control over the instrument. The proposed modulation matrix technique may be seen as a generic and selfmodifying mapping mechanism integrated in a dynamic interpolation scheme. It is implemented efficiently by taking advantage of its inherent sparse matrix structure. The modulation matrix is used within the Hadron Particle Synthesizer, a complex granular module with 200 synthesis parameters and a simplified performance control structure with 4 expression parameters. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Mapping, granular synthesis, modulation, live performance   },
pages = {316--319},
}
@inproceedings{Bryan2011,
author = {Bryan, Nicholas J. and Wang, Ge},
url = {http://www.nime.org/proceedings/2011/nime2011_179.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Two Turntables and a Mobile Phone},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Digital scratching, mobile music, digital DJ, smartphone, turntable, turntablism, record player, accelerometer, gyro- scope, vinyl emulation software },
pages = {179--184},
}
@inproceedings{Bullock2011,
author = {Bullock, Jamie and Beattie, Daniel and Turner, Jerome},
url = {http://www.nime.org/proceedings/2011/nime2011_387.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Integra Live : a New Graphical User Interface for Live Electronic Music},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {live electronics,software,usability,user experience},
pages = {387--392},
}
@inproceedings{Cappelen2011,
author = {Cappelen, Birgitta and Anderson, Anders-Petter},
url = {http://www.nime.org/proceedings/2011/nime2011_511.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Expanding the Role of the Instrument},
year = {2011},
abstract = {The traditional role of the musical instrument is to be the working tool of the professional musician. On the instrument the musician performs music for the audience to listen to. In this paper we present an interactive installation, where we expand the role of the instrument to motivate musicking and cocreation between diverse users. We have made an open installation, where users can perform a variety of actions in several situations. By using the abilities of the computer, we have made an installation, which can be interpreted to have many roles. It can both be an instrument, a co-musician, a communication partner, a toy, a meeting place and an ambient musical landscape. The users can dynamically shift between roles, based on their abilities, knowledge and motivation. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {design,genre,interaction,interactive installation,music instrument,musicking,narrative,open,role,sound art},
pages = {511--514},
}
@inproceedings{Caramiaux2011,
author = {Caramiaux, Baptiste and Bevilacqua, Fr\'{e}d\'{e}ric and Schnell, Norbert},
url = {http://www.nime.org/proceedings/2011/nime2011_329.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sound Selection by Gestures},
year = {2011},
abstract = {This paper presents a prototypical tool for sound selection driven by users' gestures. Sound selection by gesturesis a particular case of "query by content" in multimediadatabases. Gesture-to-Sound matching is based on computing the similarity between both gesture and sound parameters' temporal evolution. The tool presents three algorithms for matching gesture query to sound target. Thesystem leads to several applications in sound design, virtualinstrument design and interactive installation.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Query by Gesture, Time Series Analysis, Sonic Interaction },
pages = {329--330},
}
@inproceedings{Caramiaux2011a,
author = {Caramiaux, Baptiste and Susini, Patrick and Bianco, Tommaso and Bevilacqua, Fr\'{e}d\'{e}ric and Houix, Olivier and Schnell, Norbert and Misdariis, Nicolas},
url = {http://www.nime.org/proceedings/2011/nime2011_144.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Gestural Embodiment of Environmental Sounds : an Experimental Study},
year = {2011},
abstract = {In this paper we present an experimental study concerninggestural embodiment of environmental sounds in a listeningcontext. The presented work is part of a project aiming atmodeling movement-sound relationships, with the end goalof proposing novel approaches for designing musical instruments and sounding objects. The experiment is based onsound stimuli corresponding to "causal" and "non-causal"sounds. It is divided into a performance phase and an interview. The experiment is designed to investigate possiblecorrelation between the perception of the "causality" of environmental sounds and different gesture strategies for thesound embodiment. In analogy with the perception of thesounds' causality, we propose to distinguish gestures that"mimic" a sound's cause and gestures that "trace" a sound'smorphology following temporal sound characteristics. Results from the interviews show that, first, our causal soundsdatabase lead to consistent descriptions of the action at theorigin of the sound and participants mimic this action. Second, non-causal sounds lead to inconsistent metaphoric descriptions of the sound and participants make gestures following sound "contours". Quantitatively, the results showthat gesture variability is higher for causal sounds that noncausal sounds.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Embodiment, Environmental Sound Perception, Listening, Gesture Sound Interaction },
pages = {144--148},
}
@inproceedings{Carlson2011,
author = {Carlson, Chris and Marschner, Eli and Mccurry, Hunter},
url = {http://www.nime.org/proceedings/2011/nime2011_138.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Sound Flinger : A Haptic Spatializer},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {arduino,beagleboard,ccrma,force feedback,haptics,jack,linux audio,multi-channel audio,nime,pd,pure data,satellite ccrma,sound spatialization},
pages = {138--139},
}
@inproceedings{Carrascal2011,
author = {Carrascal, Juan P. and Jord\`{a}, Sergi},
url = {http://www.nime.org/proceedings/2011/nime2011_100.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Multitouch Interface for Audio Mixing},
year = {2011},
abstract = {Audio mixing is the adjustment of relative volumes, panning and other parameters corresponding to different soundsources, in order to create a technically and aestheticallyadequate sound sum. To do this, audio engineers employ"panpots" and faders, the standard controls in audio mixers. The design of such devices has remained practically unchanged for decades since their introduction. At the time,no usability studies seem to have been conducted on suchdevices, so one could question if they are really optimizedfor the task they are meant for.This paper proposes a new set of controls that might beused to simplify and/or improve the performance of audiomixing tasks, taking into account the spatial characteristicsof modern mixing technologies such as surround and 3Daudio and making use of multitouch interface technologies.A preliminary usability test has shown promising results.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {audio mixing,control surface,multitouch,touchscreen},
pages = {100--103},
}
@inproceedings{Choe2011,
author = {Choe, Souhwan and Lee, Kyogu},
url = {http://www.nime.org/proceedings/2011/nime2011_533.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {SWAF: Towards a Web Application Framework for Composition and Documentation of Soundscape},
year = {2011},
abstract = {In this paper, we suggest a conceptual model of a Web application framework for the composition and documentation of soundscape and introduce corresponding prototype projects, SeoulSoundMap and SoundScape Composer. We also survey the current Web-based sound projects in terms of soundscape documentation. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {soundscape, web application framework, sound archive, sound map, soundscape composition, soundscape documentation.   },
pages = {533--534},
}
@inproceedings{Comajuncosas2011,
author = {Comajuncosas, Josep M. and Barrachina, Alex and O'Connell, John and Guaus, Enric},
url = {http://www.nime.org/proceedings/2011/nime2011_252.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Nuvolet: 3D Gesture-driven Collaborative Audio Mosaicing},
year = {2011},
abstract = {This research presents a 3D gestural interface for collaborative concatenative sound synthesis and audio mosaicing.Our goal is to improve the communication between the audience and performers by means of an enhanced correlationbetween gestures and musical outcome. Nuvolet consists ofa 3D motion controller coupled to a concatenative synthesis engine. The interface detects and tracks the performers hands in four dimensions (x,y,z,t) and allows them toconcurrently explore two or three-dimensional sound cloudrepresentations of the units from the sound corpus, as wellas to perform collaborative target-based audio mosaicing.Nuvolet is included in the Esmuc Laptop Orchestra catalogfor forthcoming performances.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {concatenative synthesis, audio mosaicing, open-air inter- face, gestural controller, musical instrument, 3D },
pages = {252--255},
}
@inproceedings{Crevoisier2011,
author = {Crevoisier, Alain and Picard-Limpens, C\'{e}cile},
url = {http://www.nime.org/proceedings/2011/nime2011_236.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Mapping Objects with the Surface Editor},
year = {2011},
abstract = {The Surface Editor is a software tool for creating control interfaces and mapping input actions to OSC or MIDI actions very easily and intuitively. Originally conceived to be used with a tactile interface, the Surface Editor has been extended to support the creation of graspable interfaces as well. This paper presents a new framework for the generic mapping of user actions with graspable objects on a surface. We also present a system for detecting touch on thin objects, allowing for extended interactive possibilities. The Surface Editor is not limited to a particular tracking system though, and the generic mapping approach for objects can have a broader use with various input interfaces supporting touch and/or objects. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {NIME, mapping, interaction, user-defined interfaces, tangibles, graspable interfaces.  },
pages = {236--239},
}
@inproceedings{dAlessandro2011,
author = {d'Alessandro, Nicolas and Calderon, Roberto and M\"{u}ller, Stefanie},
url = {http://www.nime.org/proceedings/2011/nime2011_132.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {ROOM \# 81 - Agent-Based Instrument for Experiencing Architectural and Vocal Cues},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {agent,architecture,collaboration,figure 1,installation,instrument,interactive fabric,light,mo-,movements in the installation,space and,tion,voice synthesis},
pages = {132--135},
}
@inproceedings{Dahl2011,
author = {Dahl, Luke and Herrera, Jorge and Wilkerson, Carr},
url = {http://www.nime.org/proceedings/2011/nime2011_272.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {TweetDreams : Making Music with the Audience and the World using Real-time Twitter Data},
year = {2011},
abstract = {TweetDreams is an instrument and musical compositionwhich creates real-time sonification and visualization oftweets. Tweet data containing specified search terms is retrieved from Twitter and used to build networks of associated tweets. These networks govern the creation of melodiesassociated with each tweet and are displayed graphically.Audience members participate in the piece by tweeting,and their tweets are given special musical and visual prominence.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Twitter, audience participation, sonification, data visual- ization, text processing, interaction, multi-user instrument. },
pages = {272--275},
}
@inproceedings{DeJong2011,
author = {de Jong, Staas},
url = {http://www.nime.org/proceedings/2011/nime2011_326.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Making Grains Tangible: Microtouch for Microsound},
year = {2011},
abstract = {This paper proposes a new research direction for the large family of instrumental musical interfaces where sound is generated using digital granular synthesis, and where interaction and control involve the (fine) operation of stiff, flat contact surfaces. First, within a historical context, a general absence of, and clear need for, tangible output that is dynamically instantiated by the grain-generating process itself is identified. Second, to fill this gap, a concrete general approach is proposed based on the careful construction of non-vibratory and vibratory force pulses, in a one-to-one relationship with sonic grains.An informal pilot psychophysics experiment initiating the approach was conducted, which took into account the two main cases for applying forces to the human skin: perpendicular, and lateral. Initial results indicate that the force pulse approach can enable perceivably multidimensional, tangible display of the ongoing grain-generating process. Moreover, it was found that this can be made to meaningfully happen (in real time) in the same timescale of basic sonic grain generation. This is not a trivial property, and provides an important and positive fundament for further developing this type of enhanced display. It also leads to the exciting prospect of making arbitrary sonic grains actual physical manipulanda. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {and others,and today granular,barry truax,curtis roads,granular sound synthesis,instrumental control,tangible display,tangible manipulation},
pages = {326--328},
}
@inproceedings{Derbinsky2011,
author = {Derbinsky, Nate and Essl, Georg},
url = {http://www.nime.org/proceedings/2011/nime2011_104.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Cognitive Architecture in Mobile Music Interactions},
year = {2011},
abstract = {This paper explores how a general cognitive architecture canpragmatically facilitate the development and exploration ofinteractive music interfaces on a mobile platform. To thisend we integrated the Soar cognitive architecture into themobile music meta-environment urMus. We develop anddemonstrate four artificial agents which use diverse learningmechanisms within two mobile music interfaces. We alsoinclude details of the computational performance of theseagents, evincing that the architecture can support real-timeinteractivity on modern commodity hardware.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {cognitive architecture,machine learning,mobile music},
pages = {104--107},
}
@inproceedings{Diakopoulos2011,
author = {Diakopoulos, Dimitri and Kapur, Ajay},
url = {http://www.nime.org/proceedings/2011/nime2011_405.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {HIDUINO : A firmware for building driverless USB-MIDI devices using the Arduino microcontroller},
year = {2011},
abstract = {This paper presents a series of open-source firmwares for the latest iteration of the popular Arduino microcontroller platform. A portmanteau of Human Interface Device and Arduino, the HIDUINO project tackles a major problem in designing NIMEs: easily and reliably communicating with a host computer using standard MIDI over USB. HIDUINO was developed in conjunction with a class at the California Institute of the Arts intended to teach introductory-level human-computer and human-robot interaction within the context of musical controllers. We describe our frustration with existing microcontroller platforms and our experiences using the new firmware to facilitate the development and prototyping of new music controllers. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Arduino, USB, HID, MIDI, HCI, controllers, microcontrollers  },
pages = {405--408},
}
@inproceedings{Dimitrov2011,
author = {Dimitrov, Smilen and Serafin, Stefania},
url = {http://www.nime.org/proceedings/2011/nime2011_211.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Audio Arduino - an ALSA (Advanced Linux Sound Architecture) Audio Driver for FTDI-based Arduinos},
year = {2011},
abstract = {A contemporary PC user, typically expects a sound cardto be a piece of hardware, that: can be manipulated by'audio' software (most typically exemplified by 'media players'); and allows interfacing of the PC to audio reproduction and/or recording equipment. As such, a 'sound card'can be considered to be a system, that encompasses designdecisions on both hardware and software levels - that alsodemand a certain understanding of the architecture of thetarget PC operating system.This project outlines how an Arduino Duemillanoveboard (containing a USB interface chip, manufactured byFuture Technology Devices International Ltd [FTDI]company) can be demonstrated to behave as a full-duplex,mono, 8-bit 44.1 kHz soundcard, through an implementation of: a PC audio driver for ALSA (Advanced LinuxSound Architecture); a matching program for theArduino'sATmega microcontroller - and nothing more than headphones (and a couple of capacitors). The main contributionof this paper is to bring a holistic aspect to the discussionon the topic of implementation of soundcards - also by referring to open-source driver, microcontroller code and testmethods; and outline a complete implementation of an open- yet functional - soundcard system.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {alsa,arduino,audio,driver,linux,sound card},
pages = {211--216},
}
@inproceedings{Donald2011,
author = {Donald, Erika and Duinker, Ben and Britton, Eliot},
url = {http://www.nime.org/proceedings/2011/nime2011_491.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Designing the EP Trio: Instrument Identities, Control and Performance Practice in an Electronic Chamber Music Ensemble},
year = {2011},
abstract = {This paper outlines the formation of the Expanded Performance (EP) trio, a chamber ensemble comprised of electriccello with sensor bow, augmented digital percussion, anddigital turntable with mixer. Decisions relating to physical set-ups and control capabilities, sonic identities, andmappings of each instrument, as well as their roles withinthe ensemble, are explored. The contributions of these factors to the design of a coherent, expressive ensemble andits emerging performance practice are considered. The trioproposes solutions to creation, rehearsal and performanceissues in ensemble live electronics.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Live electronics, digital performance, mapping, chamber music, ensemble, instrument identity },
pages = {491--494},
}
@inproceedings{Engum2011,
author = {Engum, Trond},
url = {http://www.nime.org/proceedings/2011/nime2011_519.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Real-time Control and Creative Convolution},
year = {2011},
abstract = {This paper covers and also describes an ongoing research project focusing on new artistic possibilities by exchanging music technological methods and techniques between two distinct musical genres. Through my background as a guitarist and composer in an experimental metal band I have experienced a vast development in music technology during the last 20 years. This development has made a great impact in changing the procedures for composing and producing music within my genre without necessarily changing the strategies of how the technology is used. The transition from analogue to digital sound technology not only opened up new ways of manipulating and manoeuvring sound, it also opened up challenges in how to integrate and control the digital sound technology as a seamless part of my musical genre. By using techniques and methods known from electro-acoustic/computer music, and adapting them for use within my tradition, this research aims to find new strategies for composing and producing music within my genre. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Artistic research, strategies for composition and production, convolution, environmental sounds, real time control  },
pages = {519--522},
}
@inproceedings{Erkut2011,
author = {Erkut, Cumhur and Jylh\"{a}, Antti and Discioglu, Reha},
url = {http://www.nime.org/proceedings/2011/nime2011_477.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Structured Design and Evaluation Model with Application to Rhythmic Interaction Displays},
year = {2011},
abstract = {We present a generic, structured model for design and evaluation of musical interfaces. This model is developmentoriented, and it is based on the fundamental function of themusical interfaces, i.e., to coordinate the human action andperception for musical expression, subject to human capabilities and skills. To illustrate the particulars of this modeland present it in operation, we consider the previous designand evaluation phase of iPalmas, our testbed for exploringrhythmic interaction. Our findings inform the current design phase of iPalmas visual and auditory displays, wherewe build on what has resonated with the test users, and explore further possibilities based on the evaluation results.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {multimodal displays,rhythmic interaction,sonification,uml},
pages = {477--480},
}
@inproceedings{Fabiani2011,
author = {Fabiani, Marco and Dubus, Ga\"{e}l and Bresin, Roberto},
url = {http://www.nime.org/proceedings/2011/nime2011_116.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {MoodifierLive : Interactive and Collaborative Expressive Music Performance on Mobile Devices},
year = {2011},
abstract = {This paper presents MoodifierLive, a mobile phone application for interactive control of rule-based automatic musicperformance. Five different interaction modes are available,of which one allows for collaborative performances with upto four participants, and two let the user control the expressive performance using expressive hand gestures. Evaluations indicate that the application is interesting, fun touse, and that the gesture modes, especially the one based ondata from free expressive gestures, allow for performanceswhose emotional content matches that of the gesture thatproduced them.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Expressive performance, gesture, collaborative performance, mobile phone },
pages = {116--119},
}
@inproceedings{Flety2011,
author = {Fl\'{e}ty, Emmanuel and Maestracci, C\^{o}me},
url = {http://www.nime.org/proceedings/2011/nime2011_409.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Latency Improvement in Sensor Wireless Transmission Using IEEE 802.15.4},
year = {2011},
abstract = {We present a strategy for the improvement of wireless sensor data transmission latency, implemented in two current projects involving gesture/control sound interaction. Our platform was designed to be capable of accepting accessories using a digital bus. The receiver features a IEEE 802.15.4 microcontroller associated to a TCP/IP stack integrated circuit that transmits the received wireless data to a host computer using the Open Sound Control protocol. This paper details how we improved the latency and sample rate of the said technology while keeping the device small and scalable. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Embedded sensors, gesture recognition, wireless, sound and music computing, interaction, 802.15.4, Zigbee.     },
pages = {409--412},
}
@inproceedings{Forsyth2011,
author = {Forsyth, Jon and Glennon, Aron and Bello, Juan P.},
url = {http://www.nime.org/proceedings/2011/nime2011_487.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Random Access Remixing on the iPad},
year = {2011},
abstract = {Remixing audio samples is a common technique for the creation of electronic music, and there are a wide variety oftools available to edit, process, and recombine pre-recordedaudio into new compositions. However, all of these toolsconceive of the timeline of the pre-recorded audio and theplayback timeline as identical. In this paper, we introducea dual time axis representation in which these two timelines are described explicitly. We also discuss the randomaccess remix application for the iPad, an audio sample editor based on this representation. We describe an initialuser study with 15 high school students that indicates thatthe random access remix application has the potential todevelop into a useful and interesting tool for composers andperformers of electronic music.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {interactive systems, sample editor, remix, iPad, multi-touch },
pages = {487--490},
}
@inproceedings{Franinovic2011,
author = {Franinovic, Karmen},
url = {http://www.nime.org/proceedings/2011/nime2011_448.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Flo)(ps : Negotiating Between Habitual and Explorative Gestures},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {exploration,gesture,habit,sonic interaction design},
pages = {448--452},
}
@inproceedings{Freed2011,
author = {Freed, Adrian and MacCallum, John and Schmeder, Andrew},
url = {http://www.nime.org/proceedings/2011/nime2011_308.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP},
year = {2011},
abstract = {An effective programming style for gesture signal processing is described using a new library that brings efficient run-time polymorphism, functional and instance-based object-oriented programming to Max/MSP. By introducing better support for generic programming and composability Max/MSP becomes a more productive environment for managing the growing scale and complexity of gesture sensing systems for musical instruments and interactive installations. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {composability,delegation,functional programming,gesture signal,max,msp,object,object-,open sound control,oriented programming,processing},
pages = {308--311},
}
@inproceedings{Friberg2011,
author = {Friberg, Anders and K\"{a}llblad, Anna},
url = {http://www.nime.org/proceedings/2011/nime2011_128.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Experiences from Video-Controlled Sound Installations},
year = {2011},
abstract = {This is an overview of the three installations Hoppsa Universum, CLOSE and Flying Carpet. They were all designed as choreographed sound and music installations controlled by the visitors movements. The perspective is from an artistic goal/vision intention in combination with the technical challenges and possibilities. All three installations were realized with video cameras in the ceiling registering the users' position or movement. The video analysis was then controlling different types of interactive software audio players. Different aspects like narrativity, user control, and technical limitations are discussed. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Gestures, dance, choreography, music installation, interactive music.  },
pages = {128--131},
}
@inproceedings{Fyans2011,
author = {Fyans, A. Cavan and Gurevich, Michael},
url = {http://www.nime.org/proceedings/2011/nime2011_495.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Perceptions of Skill in Performances with Acoustic and Electronic Instruments},
year = {2011},
abstract = {We present observations from two separate studies of spectators' perceptions of musical performances, one involvingtwo acoustic instruments, the other two electronic instruments. Both studies followed the same qualitative method,using structured interviews to ascertain and compare spectators' experiences. In this paper, we focus on outcomespertaining to perceptions of the performers' skill, relatingto concepts of embodiment and communities of practice.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {skill, embodiment, perception, effort, control, spectator },
pages = {495--498},
}
@inproceedings{Fyfe2011,
author = {Fyfe, Lawrence and Tindale, Adam and Carpendale, Sheelagh},
url = {http://www.nime.org/proceedings/2011/nime2011_276.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {JunctionBox : A Toolkit for Creating Multi-touch Sound Control Interfaces},
year = {2011},
abstract = {JunctionBox is a new software toolkit for creating multitouch interfaces for controlling sound and music. Morespecifically, the toolkit has special features which make iteasy to create TUIO-based touch interfaces for controllingsound engines via Open Sound Control. Programmers using the toolkit have a great deal of freedom to create highlycustomized interfaces that work on a variety of hardware.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Multi-touch, Open Sound Control, Toolkit, TUIO },
pages = {276--279},
}
@inproceedings{Gallin2011,
author = {Gallin, Emmanuelle and Sirguy, Marc},
url = {http://www.nime.org/proceedings/2011/nime2011_437.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Eobody3: a Ready-to-use Pre-mapped \& Multi-protocol Sensor Interface},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Controller, Sensor, MIDI, USB, Computer Music, USB, OSC, CV, MIDI, DMX, A/D Converter, Interface.  },
pages = {437--440},
}
@inproceedings{Garcia2011,
author = {Garc\'{\i}a, Francisco and Vinceslas, Leny and Tubau, Josep and Maestre, Esteban},
url = {http://www.nime.org/proceedings/2011/nime2011_124.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Acquisition and Study of Blowing Pressure Profiles in Recorder Playing},
year = {2011},
abstract = {This paper presents a study of blowing pressure profilesacquired from recorder playing. Blowing pressure signalsare captured from real performance by means of a a lowintrusiveness acquisition system constructed around commercial pressure sensors based on piezoelectric transducers.An alto recorder was mechanically modified by a luthierto allow the measurement and connection of sensors whilerespecting playability and intrusiveness. A multi-modaldatabase including aligned blowing pressure and sound signals is constructed from real practice, covering the performance space by considering different fundamental frequencies, dynamics, articulations and note durations. Once signals were pre-processed and segmented, a set of temporalenvelope features were defined as a basis for studying andconstructing a simplified model of blowing pressure profilesin different performance contexts.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {blowing,instrumental gesture,multi-modal data,pressure,recorder,wind instrument},
pages = {124--127},
}
@inproceedings{Garcia2011a,
author = {Garcia, J\'{e}r\'{e}mie and Tsandilas, Theophanis and Agon, Carlos and Mackay, Wendy E.},
url = {http://www.nime.org/proceedings/2011/nime2011_361.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {InkSplorer : Exploring Musical Ideas on Paper and Computer},
year = {2011},
abstract = {We conducted three studies with contemporary music composers at IRCAM. We found that even highly computer-literate composers use an iterative process that begins with expressing musical ideas on paper, followed by active parallel exploration on paper and in software, prior to final execution of their ideas as an original score. We conducted a participatory design study that focused on the creative exploration phase, to design tools that help composers better integrate their paper-based and electronic activities. We then developed InkSplorer as a technology probe that connects users' hand-written gestures on paper to Max/MSP and OpenMusic. Composers appropriated InkSplorer according to their preferred composition styles, emphasizing its ability to help them quickly explore musical ideas on paper as they interact with the computer. We conclude with recommendations for designing interactive paper tools that support the creative process, letting users explore musical ideas both on paper and electronically. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Composer, Creativity, Design Exploration, InkSplorer, Interac- tive Paper, OpenMusic, Technology Probes.  },
pages = {361--366},
}
@inproceedings{Gillian2011,
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
url = {http://www.nime.org/proceedings/2011/nime2011_337.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping},
year = {2011},
abstract = {This paper presents a novel algorithm that has been specifically designed for the recognition of multivariate temporal musical gestures. The algorithm is based on DynamicTime Warping and has been extended to classify any N dimensional signal, automatically compute a classificationthreshold to reject any data that is not a valid gesture andbe quickly trained with a low number of training examples.The algorithm is evaluated using a database of 10 temporalgestures performed by 10 participants achieving an averagecross-validation result of 99%.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Dynamic Time Warping, Gesture Recognition, Musician- Computer Interaction, Multivariate Temporal Gestures },
pages = {337--342},
}
@inproceedings{Gillian2011a,
author = {Gillian, Nicholas and Knapp, Benjamin and O'Modhrain, Sile},
url = {http://www.nime.org/proceedings/2011/nime2011_343.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Machine Learning Toolbox For Musician Computer Interaction},
year = {2011},
abstract = {This paper presents the SARC EyesWeb Catalog, (SEC),a machine learning toolbox that has been specifically developed for musician-computer interaction. The SEC features a large number of machine learning algorithms that can be used in real-time to recognise static postures, perform regression and classify multivariate temporal gestures. The algorithms within the toolbox have been designed to work with any N -dimensional signal and can be quickly trained with a small number of training examples. We also provide the motivation for the algorithms used for the recognition of musical gestures to achieve a low intra-personal generalisation error, as opposed to the inter-personal generalisation error that is more common in other areas of human-computer interaction.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Machine learning, gesture recognition, musician-computer interaction, SEC },
pages = {343--348},
}
@inproceedings{Gold2011,
author = {Gold, Nicolas E. and Dannenberg, Roger B.},
url = {http://www.nime.org/proceedings/2011/nime2011_036.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Reference Architecture and Score Representation for Popular Music Human-Computer Music Performance Systems},
year = {2011},
abstract = {Popular music (characterized by improvised instrumental parts, beat and measure-level organization, and steady tempo) poses challenges for human-computer music performance (HCMP). Pieces of music are typically rearrangeable on-the-fly and involve a high degree of variation from ensemble to ensemble, and even between rehearsal and performance. Computer systems aiming to participate in such ensembles must therefore cope with a dynamic high-level structure in addition to the more traditional problems of beat-tracking, score-following, and machine improvisation. There are many approaches to integrating the components required to implement dynamic human-computer music performance systems. This paper presents a reference architecture designed to allow the typical sub-components (e.g. beat-tracking, tempo prediction, improvisation) to be integrated in a consistent way, allowing them to be combined and/or compared systematically. In addition, the paper presents a dynamic score representation particularly suited to the demands of popular music performance by computer. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {live performance,popular music,software design},
pages = {36--39},
}
@inproceedings{Goncalves2011,
author = {Goncalves, André},
url = {http://www.nime.org/proceedings/2011/nime2011_092.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Towards a Voltage-Controlled Computer Control and Interaction Beyond an Embedded System},
year = {2011},
abstract = {The importance of embedded devices as new devices to thefield of Voltage-Controlled Synthesizers is realized. Emphasis is directed towards understanding the importance of suchdevices in Voltage-Controlled Synthesizers. Introducing theVoltage-Controlled Computer as a new paradigm. Specifications for hardware interfacing and programming techniquesare described based on real prototypes. Implementationsand successful results are reported.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Voltage-controlled synthesizer, embedded systems, voltage- controlled computer, computer driven control voltage gen- eration },
pages = {92--95},
}
@inproceedings{Hahnel2011,
author = {H\"{a}hnel, Tilo and Berndt, Axel},
url = {http://www.nime.org/proceedings/2011/nime2011_048.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Studying Interdependencies in Music Performance : An Interactive Tool},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {articula-,duration,dynamics,egales,loudness,notes in,synthetic performance,timing,tion},
pages = {48--51},
}
@inproceedings{Hansen2011,
author = {Hansen, Anne-Marie S. and Anderson, Hans J. and Raudaskoski, Pirkko},
url = {http://www.nime.org/proceedings/2011/nime2011_220.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Play Fluency in Music Improvisation Games for Novices},
year = {2011},
abstract = {In this paper a collaborative music game for two pen tablets is studied in order to see how two people with no professional music background negotiated musical improvisation. In an initial study of what it is that constitutes play fluency in improvisation, a music game has been designed and evaluated through video analysis: A qualitative view of mutual action describes the social context of music improvisation: how two people with speech, laughter, gestures, postures and pauses negotiate individual and joint action. The objective behind the design of the game application was to support players in some aspects of their mutual play. Results show that even though players activated additional sound feedback as a result of their mutual play, players also engaged in forms of mutual play that the game engine did not account for. These ways of mutual play are descibed further along with some suggestions for how to direct future designs of collaborative music improvisation games towards ways of mutual play.  },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Collaborative interfaces, improvisation, interactive music games, social interaction, play, novice.  },
pages = {220--223},
}
@inproceedings{Harriman2011,
author = {Harriman, Jiffer and Casey, Locky and Melvin, Linden},
url = {http://www.nime.org/proceedings/2011/nime2011_529.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Quadrofeelia – A New Instrument for Sliding into Notes},
year = {2011},
abstract = {This paper describes a new musical instrument inspired by the pedal-steel guitar, along with its motivations and other considerations. Creating a multi-dimensional, expressive instrument was the primary driving force. For these criteria the pedal steel guitar proved an apt model as it allows control over several instrument parameters simultaneously and continuously. The parameters we wanted control over were volume, timbre, release time and pitch.The Quadrofeelia is played with two hands on a horizontal surface. Single notes and melodies are easily played as well as chordal accompaniment with a variety of timbres and release times enabling a range of legato and staccato notes in an intuitive manner with a new yet familiar interface.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {NIME, pedal-steel, electronic, slide, demonstration, membrane, continuous, ribbon, instrument, polyphony, lead },
pages = {529--530},
}
@inproceedings{Hayes2011,
author = {Hayes, Lauren},
url = {http://www.nime.org/proceedings/2011/nime2011_072.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Vibrotactile Feedback-Assisted Performance},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Vibrotactile feedback, human-computer interfaces, digital composition, real-time performance, augmented instruments. },
pages = {72--75},
}
@inproceedings{Hochenbaum2011,
author = {Hochenbaum, Jordan and Kapur, Ajay},
url = {http://www.nime.org/proceedings/2011/nime2011_240.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Adding Z-Depth and Pressure Expressivity to Tangible Tabletop Surfaces},
year = {2011},
abstract = {This paper presents the SmartFiducial, a wireless tangible object that facilitates additional modes of expressivity for vision-based tabletop surfaces. Using infrared proximity sensing and resistive based force-sensors, the SmartFiducial affords users unique, and highly gestural inputs. Furthermore, the SmartFiducial incorporates additional customizable pushbutton switches. Using XBee radio frequency (RF) wireless transmission, the SmartFiducial establishes bipolar communication with a host computer. This paper describes the design and implementation of the SmartFiducial, as well as an exploratory use in a musical context. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Fiducial, Tangible Interface, Multi-touch, Sensors, Gesture, Hap- tics, Bricktable, Proximity Sensing  },
pages = {240--243},
}
@inproceedings{Hsu2011,
author = {Hsu, William},
url = {http://www.nime.org/proceedings/2011/nime2011_417.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {On Movement , Structure and Abstraction in Generative Audiovisual Improvisation},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {animation,audio-visual,generative,improvisation,interactive},
pages = {417--420},
}
@inproceedings{Janssen2011,
author = {Janssen, Berit},
url = {http://www.nime.org/proceedings/2011/nime2011_068.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Reverberation Instrument Based on Perceptual Mapping},
year = {2011},
abstract = {The present article describes a reverberation instrumentwhich is based on cognitive categorization of reverberating spaces. Different techniques for artificial reverberationwill be covered. A multidimensional scaling experimentwas conducted on impulse responses in order to determinehow humans acoustically perceive spatiality. This researchseems to indicate that the perceptual dimensions are related to early energy decay and timbral qualities. Theseresults are applied to a reverberation instrument based ondelay lines. It can be contended that such an instrumentcan be controlled more intuitively than other delay line reverberation tools which often provide a confusing range ofparameters which have a physical rather than perceptualmeaning.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Reverberation, perception, multidimensional scaling, map- ping },
pages = {68--71},
}
@inproceedings{Jessop2011,
author = {Jessop, Elena and Torpey, Peter A. and Bloomberg, Benjamin},
url = {http://www.nime.org/proceedings/2011/nime2011_349.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Music and Technology in Death and the Powers},
year = {2011},
abstract = {In composer Tod Machover's new opera Death and the Powers, the main character uploads his consciousness into anelaborate computer system to preserve his essence and agencyafter his corporeal death. Consequently, for much of theopera, the stage and the environment itself come alive asthe main character. This creative need brings with it a hostof technical challenges and opportunities. In order to satisfythe needs of this storyline, Machover's Opera of the Futuregroup at the MIT Media Lab has developed a suite of newperformance technologies, including robot characters, interactive performance capture systems, mapping systems for,
,
authoring interactive multimedia performances, new musical instruments, unique spatialized sound controls, anda unified control system for all these technological components. While developed for a particular theatrical production, many of the concepts and design procedures remain relevant to broader contexts including performance,robotics, and interaction design.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {opera, Death and the Powers, Tod Machover, gestural in- terfaces, Disembodied Performance, ambisonics },
pages = {349--354},
}
@inproceedings{Johnston2011,
author = {Johnston, Andrew},
url = {http://www.nime.org/proceedings/2011/nime2011_280.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Beyond Evaluation : Linking Practice and Theory in New Musical Interface Design},
year = {2011},
abstract = {This paper presents an approach to practice-based researchin new musical instrument design. At a high level, the process involves drawing on relevant theories and aesthetic approaches to design new instruments, attempting to identify relevant applied design criteria, and then examiningthe experiences of performers who use the instruments withparticular reference to these criteria. Outcomes of this process include new instruments, theories relating to musicianinstrument interaction and a set of design criteria informedby practice and research.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {practice-based research, evaluation, Human-Computer In- teraction, research methods, user studies },
pages = {280--283},
}
@inproceedings{Julia2011,
author = {Juli\`{a}, Carles F. and Gallardo, Daniel and Jord\`{a}, Sergi},
url = {http://www.nime.org/proceedings/2011/nime2011_457.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {MTCF : A Framework for Designing and Coding Musical Tabletop Applications Directly in Pure Data},
year = {2011},
abstract = {In the past decade we have seen a growing presence of tabletop systems applied to music, lately with even some products becoming commercially available and being used byprofessional musicians in concerts. The development of thistype of applications requires several demanding technicalexpertises such as input processing, graphical design, realtime sound generation or interaction design, and because ofthis complexity they are usually developed by a multidisciplinary group.In this paper we present the Musical Tabletop CodingFramework (MTCF) a framework for designing and codingmusical tabletop applications by using the graphical programming language for digital sound processing Pure Data(Pd). With this framework we try to simplify the creationprocess of such type of interfaces, by removing the need ofany programming skills other than those of Pd.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Pure Data, tabletop, tangible, framework },
pages = {457--460},
}
@inproceedings{Kapur2011,
author = {Kapur, Ajay and Darling, Michael and Murphy, Jim and Hochenbaum, Jordan and Diakopoulos, Dimitri and Trimpin, Trimpin},
url = {http://www.nime.org/proceedings/2011/nime2011_228.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The KarmetiK NotomotoN : A New Breed of Musical Robot for Teaching and Performance},
year = {2011},
abstract = {This paper describes the KarmetiK NotomotoN, a new musical robotic system for performance and education. A long time goal of the ,
,
authors has been to provide users with plug-andplay, highly expressive musical robot system with a high degree of portability. This paper describes the technical details of the NotomotoN, and discusses its use in performance and educational scenarios. Detailed tests performed to optimize technical aspects of the NotomotoN are described to highlight usability and performance specifications for electronic musicians and educators. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {music technology,musical robotics,robotic performance},
pages = {228--231},
}
@inproceedings{Keefe2011,
author = {Keefe, Patrick O. and Essl, Georg},
url = {http://www.nime.org/proceedings/2011/nime2011_191.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Visual in Mobile Music Performance},
year = {2011},
abstract = {Visual information integration in mobile music performanceis an area that has not been thoroughly explored and currentapplications are often individually designed. From camerainput to flexible output rendering, we discuss visual performance support in the context of urMus, a meta-environmentfor mobile interaction and performance development. Theuse of cameras, a set of image primitives, interactive visualcontent, projectors, and camera flashes can lead to visuallyintriguing performance possibilities.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Mobile performance, visual interaction, camera phone, mo- bile collaboration },
pages = {191--196},
}
@inproceedings{Kerllenevich2011,
author = {Kerlle\~{n}evich, Hern\'{a}n and Egu\'{\i}a, Manuel C. and Riera, Pablo E.},
url = {http://www.nime.org/proceedings/2011/nime2011_331.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {An Open Source Interface based on Biological Neural Networks for Interactive Music Performance},
year = {2011},
abstract = {We propose and discuss an open source real-time interface that focuses in the vast potential for interactive soundart creation emerging from biological neural networks, asparadigmatic complex systems for musical exploration. Inparticular, we focus on networks that are responsible for thegeneration of rhythmic patterns.The interface relies uponthe idea of relating metaphorically neural behaviors to electronic and acoustic instruments notes, by means of flexiblemapping strategies. The user can intuitively design network configurations by dynamically creating neurons andconfiguring their inter-connectivity. The core of the systemis based in events emerging from his network design, whichfunctions in a similar way to what happens in real smallneural networks. Having multiple signal and data inputsand outputs, as well as standard communications protocolssuch as MIDI, OSC and TCP/IP, it becomes and uniquetool for composers and performers, suitable for different performance scenarios, like live electronics, sound installationsand telematic concerts.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {rhythm generation, biological neural networks, complex pat- terns, musical interface, network performance },
pages = {331--336},
}
@inproceedings{Kim2011,
author = {Kim, Seunghun and Kim, Luke K. and Jeong, Songhee and Yeo, Woon Seung},
url = {http://www.nime.org/proceedings/2011/nime2011_060.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Clothesline as a Metaphor for a Musical Interface},
year = {2011},
abstract = {In this paper, we discuss the use of the clothesline as ametaphor for designing a musical interface called Airer Choir. This interactive installation is based on the function ofan ordinary object that is not a traditional instrument, andhanging articles of clothing is literally the gesture to use theinterface. Based on this metaphor, a musical interface withhigh transparency was designed. Using the metaphor, weexplored the possibilities for recognizing of input gesturesand creating sonic events by mapping data to sound. Thus,four different types of Airer Choir were developed. By classifying the interfaces, we concluded that various musicalexpressions are possible by using the same metaphor.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {musical interface, metaphor, clothesline installation },
pages = {60--63},
}
@inproceedings{Kim2011a,
author = {Kim, Seunghun and Yeo, Woon Seung},
url = {http://www.nime.org/proceedings/2011/nime2011_217.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Musical Control of a Pipe Based on Acoustic Resonance},
year = {2011},
abstract = {In this paper, we introduce a pipe interface that recognizestouch on tone holes by the resonances in the pipe instead ofa touch sensor. This work was based on the acoustic principles of woodwind instruments without complex sensors andelectronic circuits to develop a simple and durable interface.The measured signals were analyzed to show that differentfingerings generate various sounds. The audible resonancesignal in the pipe interface can be used as a sonic event formusical expression by itself and also as an input parameterfor mapping different sounds.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {resonance, mapping, pipe },
pages = {217--219},
}
@inproceedings{Kim2011b,
author = {Kim, Tae Hun and Fukayama, Satoru and Nishimoto, Takuya and Sagayama, Shigeki},
url = {http://www.nime.org/proceedings/2011/nime2011_096.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Polyhymnia : An Automatic Piano Performance System with Statistical Modeling of Polyphonic Expression and Musical Symbol Interpretation},
year = {2011},
abstract = {We developed an automatic piano performance system calledPolyhymnia that is able to generate expressive polyphonicpiano performances with music scores so that it can be usedas a computer-based tool for an expressive performance.The system automatically renders expressive piano musicby means of automatic musical symbol interpretation andstatistical models of structure-expression relations regarding polyphonic features of piano performance. Experimental results indicate that the generated performances of various piano pieces with diverse trained models had polyphonicexpression and sounded expressively. In addition, the models trained with different performance styles reflected thestyles observed in the training performances, and they werewell distinguishable by human listeners. Polyhymnia wonthe first prize in the autonomous section of the PerformanceRendering Contest for Computer Systems (Rencon) 2010.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {performance rendering, polyphonic expression, statistical modeling, conditional random fields },
pages = {96--99},
}
@inproceedings{Klugel2011,
author = {Kl\"{u}gel, Niklas and Frie\ss, Marc R. and Groh, Georg and Echtler, Florian},
url = {http://www.nime.org/proceedings/2011/nime2011_032.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {An Approach to Collaborative Music Composition},
year = {2011},
abstract = {This paper provides a discussion of how the electronic, solely ITbased composition and performance of electronic music can besupported in realtime with a collaborative application on a tabletopinterface, mediating between single-user style music compositiontools and co-located collaborative music improvisation. After having elaborated on the theoretical backgrounds of prerequisites ofco-located collaborative tabletop applications as well as the common paradigms in music composition/notation, we will review related work on novel IT approaches to music composition and improvisation. Subsequently, we will present our prototypical implementation and the results.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Tabletop Interface, Collaborative Music Composition, Creativity Support },
pages = {32--35},
}
@inproceedings{Knapp2011,
author = {Knapp, Benjamin and Bortz, Brennon},
url = {http://www.nime.org/proceedings/2011/nime2011_203.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {MobileMuse: Integral Music Control Goes Mobile},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {affective computing,bile music performance,mo-,physiological signal measurement},
pages = {203--206},
}
@inproceedings{Kondapalli2011,
author = {Kondapalli, Ravi and Sung, Ben-Zhen},
url = {http://www.nime.org/proceedings/2011/nime2011_140.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Daft Datum – An Interface for Producing Music Through Foot-based Interaction},
year = {2011},
abstract = {Daft Datum is an autonomous new media artefact that takes input from movement of the feet (i.e. tapping/stomping/stamping) on a wooden surface, underneath which is a sensor sheet. The sensors in the sheet are mapped to various sound samples and synthesized sounds. Attributes of the synthesized sound, such as pitch and octave, can be controlled using the Nintendo Wii Remote. It also facilitates switching between modes of sound and recording/playing back a segment of audio. The result is music generated by dancing on the device that is further modulated by a hand-held controller. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Daft Datum, Wii, Dance Pad, Feet, Controller, Bluetooth, Musical Interface, Dance, Sensor Sheet  },
pages = {140--141},
}
@inproceedings{Kruge2011,
author = {Kruge, Nick and Wang, Ge},
url = {http://www.nime.org/proceedings/2011/nime2011_185.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {MadPad : A Crowdsourcing System for Audiovisual Sampling},
year = {2011},
abstract = {MadPad is a networked audiovisual sample station for mobile devices. Twelve short video clips are loaded onto thescreen in a grid and playback is triggered by tapping anywhere on the clip. This is similar to tapping the pads of anaudio sample station, but extends that interaction to addvisual sampling. Clips can be shot on-the-fly with a cameraenabled mobile device and loaded into the player instantly,giving the performer an ability to quickly transform his orher surroundings into a sample-based, audiovisual instrument. Samples can also be sourced from an online community in which users can post or download content. The recent ubiquity of multitouch mobile devices and advances inpervasive computing have made this system possible, providing for a vast amount of content only limited by theimagination of the performer and the community. This paper presents the core features of MadPad and the designexplorations that inspired them.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {mobile music, networked music, social music, audiovisual, sampling, user-generated content, crowdsourcing, sample station, iPad, iPhone },
pages = {185--190},
}
@inproceedings{Kuhara2011,
author = {Kuhara, Yasuo and Kobayashi, Daiki},
url = {http://www.nime.org/proceedings/2011/nime2011_136.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Kinetic Particles Synthesizer Using Multi-Touch Screen Interface of Mobile Devices},
year = {2011},
abstract = {We developed a kinetic particles synthesizer for mobile devices having a multi-touch screen such as a tablet PC and a smart phone. This synthesizer generates music based on the kinetics of particles under a two-dimensional physics engine. The particles move in the screen to synthesize sounds according to their own physical properties, which are shape, size, mass, linear and angular velocity, friction, restitution, etc. If a particle collides with others, a percussive sound is generated. A player can play music by the simple operation of touching or dragging on the screen of the device. Using a three-axis acceleration sensor, a player can perform music by shuffling or tilting the device. Each particle sounds just a simple tone. However, a large amount of various particles play attractive music by aggregating their sounds. This concept has been inspired by natural sounds made from an assembly of simple components, for example, rustling leaves or falling rain. For a novice who has no experience of playing a musical instrument, it is easy to learn how to play instantly and enjoy performing music with intuitive operation. Our system is used for musical instruments for interactive music entertainment. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Particle, Tablet PC, iPhone, iPod touch, iPad, Smart phone, Kinetics, Touch screen, Physics engine.  },
pages = {136--137},
}
@inproceedings{Lamb2011,
author = {Lamb, Roland and Robertson, Andrew},
url = {http://www.nime.org/proceedings/2011/nime2011_503.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Seaboard : a New Piano Keyboard-related Interface Combining Discrete and Continuous Control},
year = {2011},
abstract = {This paper introduces the Seaboard, a new tangible musicalinstrument which aims to provide musicians with significantcapability to manipulate sound in real-time in a musicallyintuitive way. It introduces the core design features whichmake the Seaboard unique, and describes the motivationand rationale behind the design. The fundamental approachto dealing with problems associated with discrete and continuous inputs is summarized.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Piano keyboard-related interface, continuous and discrete control, haptic feedback, Human-Computer Interaction (HCI) },
pages = {503--506},
}
@inproceedings{Lee2011,
author = {Lee, Jeong-seob and Yeo, Woon Seung},
url = {http://www.nime.org/proceedings/2011/nime2011_024.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sonicstrument : A Musical Interface with Stereotypical Acoustic Transducers},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Stereotypical transducers, audible sound, Doppler effect, hand- free interface, musical instrument, interactive performance  },
pages = {24--27},
}
@inproceedings{Leslie2011,
author = {Leslie, Grace and Mullen, Tim},
url = {http://www.nime.org/proceedings/2011/nime2011_296.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {MoodMixer : EEG-based Collaborative Sonification},
year = {2011},
abstract = {MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music spaceby manipulating their cognitive state and conveying thisstate via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulateor passively convey their cognitive state depending on theirdesired approach and experience level. A four-channel electronic music mixture continuously conveys the participants'expressed cognitive states while a colored visualization oftheir locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.MoodMixer is a collaborative experience that incorporatesaspects of both passive and active EEG sonification andperformance art. We discuss the technical design of the installation and place its collaborative sonification aestheticdesign within the context of existing EEG-based music andart.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {EEG, BCMI, collaboration, sonification, visualization },
pages = {296--299},
}
@inproceedings{Liang2011,
author = {Liang, Dawen and Xia, Guangyu and Dannenberg, Roger B.},
url = {http://www.nime.org/proceedings/2011/nime2011_167.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Framework for Coordination and Synchronization of Media},
year = {2011},
abstract = {Computer music systems that coordinate or interact with human musicians exist in many forms. Often, coordination is at the level of gestures and phrases without synchronization at the beat level (or perhaps the notion of "beat" does not even exist). In music with beats, fine-grain synchronization can be achieved by having humans adapt to the computer (e.g. following a click track), or by computer accompaniment in which the computer follows a predetermined score. We consider an alternative scenario in which improvisation prevents traditional score following, but where synchronization is achieved at the level of beats, measures, and cues. To explore this new type of human-computer interaction, we have created new software abstractions for synchronization and coordination of music and interfaces in different modalities. We describe these new software structures, present examples, and introduce the idea of music notation as an interactive musical interface rather than a static document. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {automatic accompaniment,interactive,music display,popular music,real-time,synchronization},
pages = {167--172},
}
@inproceedings{Lopez2011,
author = {Lopez, Pedro and Ferreira, Alfredo and Pereira, J. A. Madeiras},
url = {http://www.nime.org/proceedings/2011/nime2011_367.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Battle of the DJs: an HCI Perspective of Traditional, Virtual, Hybrid and Multitouch DJing},
year = {2011},
abstract = {The DJ culture uses a gesture lexicon strongly rooted in thetraditional setup of turntables and a mixer. As novel toolsare introduced in the DJ community, this lexicon is adaptedto the features they provide. In particular, multitouch technologies can offer a new syntax while still supporting the oldlexicon, which is desired by DJs.We present a classification of DJ tools, from an interaction point of view, that divides the previous work into Traditional, Virtual and Hybrid setups. Moreover, we presenta multitouch tabletop application, developed with a groupof DJ consultants to ensure an adequate implementation ofthe traditional gesture lexicon.To conclude, we conduct an expert evaluation, with tenDJ users in which we compare the three DJ setups with ourprototype. The study revealed that our proposal suits expectations of Club/Radio-DJs, but fails against the mentalmodel of Scratch-DJs, due to the lack of haptic feedback torepresent the record's physical rotation. Furthermore, testsshow that our multitouch DJ setup, reduces task durationwhen compared with Virtual setups.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {DJing, Multitouch Interaction, Expert User evaluation, HCI },
pages = {367--372},
}
@inproceedings{Luhtala2011,
author = {Luhtala, Matti and Kym\"{a}l\"{a}inen, Tiina and Plomp, Johan},
url = {http://www.nime.org/proceedings/2011/nime2011_429.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Designing a Music Performance Space for Persons with Intellectual Learning Disabilities},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Music interfaces, music therapy, modifiable interfaces, design tools, Human-Technology Interaction (HTI), User-Centred Design (UCD), design for all (DfA), prototyping, performance.   },
pages = {429--432},
}
@inproceedings{Marchini2011,
author = {Marchini, Marco and Papiotis, Panos and P\'{e}rez, Alfonso and Maestre, Esteban},
url = {http://www.nime.org/proceedings/2011/nime2011_481.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Hair Ribbon Deflection Model for Low-intrusiveness Measurement of Bow Force in Violin Performance},
year = {2011},
abstract = {This paper introduces and evaluates a novel methodologyfor the estimation of bow pressing force in violin performance, aiming at a reduced intrusiveness while maintaininghigh accuracy. The technique is based on using a simplifiedphysical model of the hair ribbon deflection, and feeding thismodel solely with position and orientation measurements ofthe bow and violin spatial coordinates. The physical modelis both calibrated and evaluated using real force data acquired by means of a load cell.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {bow pressing force, bow force, pressing force, force, violin playing, bow simplified physical model, 6DOF, hair ribbon ends, string ends },
pages = {481--486},
}
@inproceedings{Marquez-Borbon2011,
author = {Marquez-Borbon, Adnan and Gurevich, Michael and Fyans, A. Cavan and Stapleton, Paul},
url = {http://www.nime.org/proceedings/2011/nime2011_373.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Designing Digital Musical Interactions in Experimental Contexts},
year = {2011},
abstract = {As NIME's focus has expanded beyond the design reportswhich were pervasive in the early days to include studies andexperiments involving music control devices, we report on aparticular area of activity that has been overlooked: designsof music devices in experimental contexts. We demonstratethis is distinct from designing for artistic performances, witha unique set of novel challenges. A survey of methodologicalapproaches to experiments in NIME reveals a tendency torely on existing instruments or evaluations of new devicesdesigned for broader creative application. We present twoexamples from our own studies that reveal the merits ofdesigning purpose-built devices for experimental contexts.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Experiment, Methodology, Instrument Design, DMIs },
pages = {373--376},
}
@inproceedings{Marshall2011,
author = {Marshall, Mark T. and Wanderley, Marcelo M.},
url = {http://www.nime.org/proceedings/2011/nime2011_399.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument},
year = {2011},
abstract = {This paper deals with the effects of integrated vibrotactile feedback on the "feel" of a digital musical instrument(DMI). Building on previous work developing a DMI withintegrated vibrotactile feedback actuators, we discuss howto produce instrument-like vibrations, compare these simulated vibrations with those produced by an acoustic instrument and examine how the integration of this feedbackeffects performer ratings of the instrument. We found thatintegrated vibrotactile feedback resulted in an increase inperformer engagement with the instrument, but resulted ina reduction in the perceived control of the instrument. Wediscuss these results and their implications for the design ofnew digital musical instruments.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Vibrotactile Feedback, Digital Musical Instruments, Feel, Loudspeakers },
pages = {399--404},
}
@inproceedings{Martin2011,
author = {Martin, Charles and Lai, Chi-Hsia},
url = {http://www.nime.org/proceedings/2011/nime2011_142.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Strike on Stage : a Percussion and Media Performance},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {computer vision,media performance,percussion},
pages = {142--143},
}
@inproceedings{Mcgee2011,
author = {Mcgee, Ryan and Fan, Yuan-Yi and Ali, Reza},
url = {http://www.nime.org/proceedings/2011/nime2011_080.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {BioRhythm : a Biologically-inspired Audio-Visual Installation},
year = {2011},
abstract = {BioRhythm is an interactive bio-feedback installation controlled by the cardiovascular system. Data from a photoplethysmograph (PPG) sensor controls sonification and visualization parameters in real-time. Biological signals areobtained using the techniques of Resonance Theory in Hemodynamics and mapped to audiovisual cues via the Five Element Philosophy. The result is a new media interface utilizing sound synthesis and spatialization with advanced graphics rendering. BioRhythm serves as an artistic explorationof the harmonic spectra of pulse waves.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {bio-feedback,bio-sensing,fm synthesis,open sound control,parallel computing,sonification,spa-,spatial audio,tialization,tion,visualiza-},
pages = {80--83},
}
@inproceedings{Mealla2011,
author = {Mealla, Sebasti\'{a}n and V\"{a}aljam\"{a}ae, Aleksander and Bosi, Mathieu and Jord\`{a}, Sergi},
url = {http://www.nime.org/proceedings/2011/nime2011_149.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Listening to Your Brain : Implicit Interaction in Collaborative Music Performances},
year = {2011},
abstract = {The use of physiological signals in Human Computer Interaction (HCI) is becoming popular and widespread, mostlydue to sensors miniaturization and advances in real-timeprocessing. However, most of the studies that use physiologybased interaction focus on single-user paradigms, and itsusage in collaborative scenarios is still in its beginning. Inthis paper we explore how interactive sonification of brainand heart signals, and its representation through physicalobjects (physiopucks) in a tabletop interface may enhancemotivational and controlling aspects of music collaboration.A multimodal system is presented, based on an electrophysiology sensor system and the Reactable, a musical tabletop interface. Performance and motivation variables wereassessed in an experiment involving a test "Physio" group(N=22) and a control "Placebo" group (N=10). Pairs ofparticipants used two methods for sound creation: implicitinteraction through physiological signals, and explicit interaction by means of gestural manipulation. The resultsshowed that pairs in the Physio Group declared less difficulty, higher confidence and more symmetric control thanthe Placebo Group, where no real-time sonification was provided as subjects were using pre-recorded physiological signal being unaware of it. These results support the feasibilityof introducing physiology-based interaction in multimodalinterfaces for collaborative music generation.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {also at specs laboratory,bci,collaboration,cscw,hci,multimodal interfaces,music,physiological computing,physiopucks,tabletops,universitat pompeu fabra},
pages = {149--154},
}
@inproceedings{Milne2011,
author = {Milne, Andrew J. and Xamb\'{o}, Anna and Laney, Robin and Sharp, David B. and Prechtl, Anthony and Holland, Simon},
url = {http://www.nime.org/proceedings/2011/nime2011_244.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Hex Player — A Virtual Musical Controller},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {generalized keyboard, isomorphic layout, multi-touch sur- face, tablet, musical interface design, iPad, microtonality },
pages = {244--247},
}
@inproceedings{Mitchell2011,
author = {Mitchell, Thomas and Heap, Imogen},
url = {http://www.nime.org/proceedings/2011/nime2011_465.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {SoundGrasp : A Gestural Interface for the Performance of Live Music},
year = {2011},
abstract = {This paper documents the first developmental phase of aninterface that enables the performance of live music usinggestures and body movements. The work included focuseson the first step of this project: the composition and performance of live music using hand gestures captured using asingle data glove. The paper provides a background to thefield, the aim of the project and a technical description ofthe work completed so far. This includes the developmentof a robust posture vocabulary, an artificial neural networkbased posture identification process and a state-based system to map identified postures onto a set of performanceprocesses. The paper is closed with qualitative usage observations and a projection of future plans.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Music Controller, Gestural Music, Data Glove, Neural Net- work, Live Music Composition, Looping, Imogen Heap },
pages = {465--468},
}
@inproceedings{Molina2011,
author = {Molina, Pablo and Haro, Mart\'{\i}n and Jord\`{a}, Sergi},
url = {http://www.nime.org/proceedings/2011/nime2011_288.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {BeatJockey : A New Tool for Enhancing DJ Skills},
year = {2011},
abstract = {We present BeatJockey, a prototype interface which makesuse of Audio Mosaicing (AM), beat-tracking and machinelearning techniques, for supporting Diskjockeys (DJs) byproposing them new ways of interaction with the songs onthe DJ's playlist. This prototype introduces a new paradigmto DJing in which the user has the capability to mix songsinteracting with beat-units that accompany the DJ's mix.For this type of interaction, the system suggests song slicestaken from songs selected from a playlist, which could gowell with the beats of whatever master song is being played.In addition the system allows the synchronization of multiple songs, thus permitting flexible, coherent and rapid progressions in the DJ's mix. BeatJockey uses the Reactable,a musical tangible user interface (TUI), and it has beendesigned to be used by all DJs regardless of their level ofexpertise, as the system helps the novice while bringing newcreative opportunities to the expert.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {DJ, music information retrieval, audio mosaicing, percus- sion, turntable, beat-mash, interactive music interfaces, real- time, tabletop interaction, reactable. },
pages = {288--291},
}
@inproceedings{Montag2011,
author = {Montag, Matthew and Sullivan, Stefan and Dickey, Scott and Leider, Colby},
url = {http://www.nime.org/proceedings/2011/nime2011_008.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {multi-touch, haptics, frustrated total internal reflection, mu- sic performance, music composition, latency, DIY },
pages = {8--13},
}
@inproceedings{Mullen2011,
author = {Mullen, Tim and Warp, Richard and Jansch, Adam},
url = {http://www.nime.org/proceedings/2011/nime2011_469.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface},
year = {2011},
abstract = {The use of non-invasive electroencephalography (EEG) in the experimental arts is not a novel concept. Since 1965, EEG has been used in a large number of, sometimes highly sophisticated, systems for musical and artistic expression. However, since the advent of the synthesizer, most such systems have utilized digital and/or synthesized media in sonifying the EEG signals. There have been relatively few attempts to create interfaces for musical expression that allow one to mechanically manipulate acoustic instruments by modulating one's mental state. Secondly, few such systems afford a distributed performance medium, with data transfer and audience participation occurring over the Internet. The use of acoustic instruments and Internet-enabled communication expands the realm of possibilities for musical expression in Brain-Computer Music Interfaces (BCMI), while also introducing additional challenges. In this paper we report and examine a first demonstration (Music for Online Performer) of a novel system for Internet-enabled manipulation of robotic acoustic instruments, with feedback, using a non-invasive EEG-based BCI and low-cost, commercially available robotics hardware. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {EEG, Brain-Computer Music Interface, Internet, Arduino.  },
pages = {469--472},
}
@inproceedings{Murray-Browne2011,
author = {Murray-Browne, Tim and Mainstone, Di and Bryan-Kinns, Nick and Plumbley, Mark D.},
url = {http://www.nime.org/proceedings/2011/nime2011_056.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Medium is the Message: Composing Instruments and Performing Mappings},
year = {2011},
abstract = {Many performers of novel musical instruments find it difficult to engage audiences beyond those in the field. Previousresearch points to a failure to balance complexity with usability, and a loss of transparency due to the detachmentof the controller and sound generator. The issue is oftenexacerbated by an audience's lack of prior exposure to theinstrument and its workings.However, we argue that there is a conflict underlyingmany novel musical instruments in that they are intendedto be both a tool for creative expression and a creative workof art in themselves, resulting in incompatible requirements.By considering the instrument, the composition and theperformance together as a whole with careful considerationof the rate of learning demanded of the audience, we propose that a lack of transparency can become an asset ratherthan a hindrance. Our approach calls for not only controllerand sound generator to be designed in sympathy with eachother, but composition, performance and physical form too.Identifying three design principles, we illustrate this approach with the Serendiptichord, a wearable instrument fordancers created by the ,
,
authors.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Performance, composed instrument, transparency, constraint. },
pages = {56--59},
}
@inproceedings{Newton2011,
author = {Newton, Dan and Marshall, Mark T.},
url = {http://www.nime.org/proceedings/2011/nime2011_155.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Examining How Musicians Create Augmented Musical Instruments},
year = {2011},
abstract = {This paper examines the creation of augmented musicalinstruments by a number of musicians. Equipped with asystem called the Augmentalist, 10 musicians created newaugmented instruments based on their traditional acousticor electric instruments. This paper discusses the ways inwhich the musicians augmented their instruments, examines the similarities and differences between the resultinginstruments and presents a number of interesting findingsresulting from this process.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Augmented Instruments, Instrument Design, Digital Musi- cal Instruments, Performance },
pages = {155--160},
}
@inproceedings{Nishino2011,
author = {Nishino, Hiroki},
url = {http://www.nime.org/proceedings/2011/nime2011_499.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Cognitive Issues in Computer Music Programming},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Computer music, programming language, the psychology of programming, usability  },
pages = {499--502},
}
@inproceedings{Nymoen2011,
author = {Nymoen, Kristian and Skogstad, St\aa le A. and Jensenius, Alexander R.},
url = {http://www.nime.org/proceedings/2011/nime2011_312.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {SoundSaber - A Motion Capture Instrument},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
pages = {312--315},
}
@inproceedings{Overholt2011,
author = {Overholt, Dan},
url = {http://www.nime.org/proceedings/2011/nime2011_004.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Overtone Fiddle: an Actuated Acoustic Instrument},
year = {2011},
abstract = {The Overtone Fiddle is a new violin-family instrument that incorporates electronic sensors, integrated DSP, and physical actuation of the acoustic body. An embedded tactile sound transducer creates extra vibrations in the body of the Overtone Fiddle, allowing performer control and sensation via both traditional violin techniques, as well as extended playing techniques that incorporate shared man/machine control of the resulting sound. A magnetic pickup system is mounted to the end of the fiddle's fingerboard in order to detect the signals from the vibrating strings, deliberately not capturing vibrations from the full body of the instrument. This focused sensing approach allows less restrained use of DSP-generated feedback signals, as there is very little direct leakage from the actuator embedded in the body of the instrument back to the pickup. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Actuated Musical Instruments, Hybrid Instruments, Active Acoustics, Electronic Violin  },
pages = {30--33},
}
@inproceedings{Papetti2011,
author = {Papetti, Stefano and Civolani, Marco and Fontana, Federico},
url = {http://www.nime.org/proceedings/2011/nime2011_473.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Rhythm’n’Shoes : a Wearable Foot Tapping Interface with Audio-Tactile Feedback},
year = {2011},
abstract = {A shoe-based interface is presented, which enables users toplay percussive virtual instruments by tapping their feet.The wearable interface consists of a pair of sandals equippedwith four force sensors and four actuators affording audiotactile feedback. The sensors provide data via wireless transmission to a host computer, where they are processed andmapped to a physics-based sound synthesis engine. Sincethe system provides OSC and MIDI compatibility, alternative electronic instruments can be used as well. The audiosignals are then sent back wirelessly to audio-tactile excitersembedded in the sandals' sole, and optionally to headphonesand external loudspeakers. The round-trip wireless communication only introduces very small latency, thus guaranteeing coherence and unity in the multimodal percept andallowing tight timing while playing.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {interface, audio, tactile, foot tapping, embodiment, footwear, wireless, wearable, mobile },
pages = {473--476},
}
@inproceedings{Pardue2011,
author = {Pardue, Laurel S. and Boch, Andrew and Boch, Matt and Southworth, Christine and Rigopulos, Alex},
url = {http://www.nime.org/proceedings/2011/nime2011_018.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Gamelan Elektrika: An Electronic Balinese Gamelan},
year = {2011},
abstract = {This paper describes the motivation and construction ofGamelan Elektrika, a new electronic gamelan modeled aftera Balinese Gong Kebyar. The first of its kind, Elektrika consists of seven instruments acting as MIDI controllers accompanied by traditional percussion and played by 11 or moreperformers following Balinese performance practice. Threemain percussive instrument designs were executed using acombination of force sensitive resistors, piezos, and capacitive sensing. While the instrument interfaces are designedto play interchangeably with the original, the sound andtravel possiblilities they enable are tremendous. MIDI enables a massive new sound palette with new scales beyondthe quirky traditional tuning and non-traditional sounds.It also allows simplified transcription for an aurally taughttradition. Significantly, it reduces the transportation challenges of a previously large and heavy ensemble, creatingopportunities for wider audiences to experience Gong Kebyar's enchanting sound. True to the spirit of oneness inBalinese music, as one of the first large all-MIDI ensembles,Elek Trika challenges performers to trust silent instrumentsand develop an understanding of highly intricate and interlocking music not through the sound of the individual, butthrough the sound of the whole.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {bali, gamelan, musical instrument design, MIDI ensemble },
pages = {18--23},
}
@inproceedings{Pigott2011,
author = {Pigott, Jon},
url = {http://www.nime.org/proceedings/2011/nime2011_084.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Vibration , Volts and Sonic Art: A Practice and Theory of Electromechanical Sound},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Electromechanical sonic art, kinetic sound art, prepared speakers, Infinite Spring.  },
pages = {84--87},
}
@inproceedings{Pirro2011,
author = {Pirr\`{o}, David and Eckel, Gerhard},
url = {http://www.nime.org/proceedings/2011/nime2011_461.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Physical Modelling Enabling Enaction: an Example},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {embod-,enactive interfaces,has been ap-,iment,interaction,motion tracking,of sound and music,physical modelling,to movement and gesture},
pages = {461--464},
}
@inproceedings{Polotti2011,
author = {Polotti, Pietro and Goina, Maurizio},
url = {http://www.nime.org/proceedings/2011/nime2011_064.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {EGGS in Action},
year = {2011},
abstract = {In this paper, we discuss the results obtained by means of the EGGS (Elementary Gestalts for Gesture Sonification) system in terms of artistic realizations. EGGS was introduced in a previous edition of this conference. The works presented include interactive installations in the form of public art and interactive onstage performances. In all of the works, the EGGS principles of simplicity based on the correspondence between elementary sonic and movement units, and of organicity between sound and gesture are applied. Indeed, we study both sound as a means for gesture representation and gesture as embodiment of sound. These principles constitute our guidelines for the investigation of the bidirectional relationship between sound and body expression with various strategies involving both educated and non-educated executors. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Gesture sonification, Interactive performance, Public art.  },
pages = {64--67},
}
@inproceedings{Popp2011,
author = {Popp, Phillip and Wright, Matthew},
url = {http://www.nime.org/proceedings/2011/nime2011_284.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Intuitive Real-Time Control of Spectral Model Synthesis},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Spectral Model Synthesis, Gesture Recognition, Synthesis Control, Wacom Tablet, Machine Learning },
pages = {284--287},
}
@inproceedings{Ramkissoon2011,
author = {Ramkissoon, Izzi},
url = {http://www.nime.org/proceedings/2011/nime2011_224.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance},
year = {2011},
abstract = {The Bass Sleeve uses an Arduino board with a combination of buttons, switches, flex sensors, force sensing resistors, and an accelerometer to map the ancillary movements of a performer to sampling, real-time audio and video processing including pitch shifting, delay, low pass filtering, and onscreen video movement. The device was created to augment the existing functions of the electric bass and explore the use of ancillary gestures to control the laptop in a live performance. In this research it was found that incorporating ancillary gestures into a live performance could be useful when controlling the parameters of audio processing, sound synthesis and video manipulation. These ancillary motions can be a practical solution to gestural multitasking allowing independent control of computer music parameters while performing with the electric bass. The process of performing with the Bass Sleeve resulted in a greater amount of laptop control, an increase in the amount of expressiveness using the electric bass in combination with the laptop, and an improvement in the interactivity on both the electric bass and laptop during a live performance. The design uses various gesture-to-sound mapping strategies to accomplish a compositional task during an electro acoustic multimedia musical performance piece. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Interactive Music, Interactive Performance Systems, Gesture Controllers, Augmented Instruments, Electric Bass, Video Tracking  },
pages = {224--227},
}
@inproceedings{Reus2011,
author = {Reus, Jonathan},
url = {http://www.nime.org/proceedings/2011/nime2011_377.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Crackle: A Dynamic Mobile Multitouch Topology for Exploratory Sound Interaction},
year = {2011},
abstract = {This paper describes the design of Crackle, a interactivesound and touch experience inspired by the CrackleBox.We begin by describing a ruleset for Crackle's interactionderived from the salient interactive qualities of the CrackleBox. An implementation strategy is then described forrealizing the ruleset as an application for the iPhone. Thepaper goes on to consider the potential of using Crackleas an encapsulated interaction paradigm for exploring arbitrary sound spaces, and concludes with lessons learned ondesigning for multitouch surfaces as expressive input sensors.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {touchscreen, interface topology, mobile music, interaction paradigm, dynamic mapping, CrackleBox, iPhone },
pages = {377--380},
}
@inproceedings{Roh2011,
author = {Roh, Jung-Sim and Mann, Yotam and Freed, Adrian and Wessel, David},
url = {http://www.nime.org/proceedings/2011/nime2011_393.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers},
year = {2011},
abstract = {The design space of fabric multitouch surface interaction is explored with emphasis on novel materials and construction techniques aimed towards reliable, repairable pressure sensing surfaces for musical applications. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Multitouch, surface interaction, piezoresistive, fabric sensor, e- textiles, tangible computing, drum controller  },
pages = {393--398},
}
@inproceedings{Rosenbaum2011,
author = {Rosenbaum, Eric},
url = {http://www.nime.org/proceedings/2011/nime2011_445.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {MelodyMorph: A Reconfigurable Musical Instrument},
year = {2011},
abstract = {I present MelodyMorph, a reconfigurable musical instrument designed with a focus on melodic improvisation. It is designed for a touch-screen interface, and allows the user to create "bells" which can be tapped to play a note, and dragged around on a pannable and zoomable canvas. Colors, textures and shapes of the bells represent pitch and timbre properties. "Recorder bells" can store and play back performances. Users can construct instruments that are modifiable as they play, and build up complex melodies hierarchically from simple parts. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Melody, improvisation, representation, multi-touch, iPad  },
pages = {445--447},
}
@inproceedings{Schacher2011,
author = {Schacher, Jan C. and Stoecklin, Angela},
url = {http://www.nime.org/proceedings/2011/nime2011_292.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Traces – Body, Motion and Sound},
year = {2011},
abstract = {In this paper the relationship between body, motion and sound is addressed. The comparison with traditional instruments and dance is shown with regards to basic types of motion. The difference between gesture and movement is outlined and some of the models used in dance for structuring motion sequences are described. In order to identify expressive aspects of motion sequences a test scenario is devised. After the description of the methods and tools used in a series of measurements, two types of data-display are shown and the applied in the interpretation. One salient feature is recognized and put into perspective with regards to movement and gestalt perception. Finally the merits of the technical means that were applied are compared and a model-based approach to motion-sound mapping is proposed. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Interactive Dance, Motion and Gesture, Sonification, Motion Perception, Mapping   },
pages = {292--295},
}
@inproceedings{Schedel2011,
author = {Schedel, Margaret and Perry, Phoenix and Fiebrink, Rebecca},
url = {http://www.nime.org/proceedings/2011/nime2011_453.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Wekinating 000000Swan : Using Machine Learning to Create and Control Complex Artistic Systems},
year = {2011},
abstract = {In this paper we discuss how the band 000000Swan uses machine learning to parse complex sensor data and create intricate artistic systems for live performance. Using the Wekinator software for interactive machine learning, we have created discrete and continuous models for controlling audio and visual environments using human gestures sensed by a commercially-available sensor bow and the Microsoft Kinect. In particular, we have employed machine learning to quickly and easily prototype complex relationships between performer gesture and performative outcome. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Wekinator, K-Bow, Machine Learning, Interactive, Multimedia, Kinect, Motion-Tracking, Bow Articulation, Animation  },
pages = {453--456},
}
@inproceedings{Schnell2011,
author = {Schnell, Norbert and Bevilacqua, Fr\'{e}d\'{e}ric and Rasamimanana, Nicolas and Blois, Julien and Gu\'{e}dy, Fabrice and Fl\'{e}ty, Emmanuel},
url = {http://www.nime.org/proceedings/2011/nime2011_535.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Playing the "MO" – Gestural Control and Re-Embodiment of Recorded Sound and Music},
year = {2011},
abstract = {We are presenting a set of applications that have been realized with the MO modular wireless motion capture deviceand a set of software components integrated into Max/MSP.These applications, created in the context of artistic projects,music pedagogy, and research, allow for the gestural reembodiment of recorded sound and music. They demonstrate a large variety of different "playing techniques" inmusical performance using wireless motion sensor modulesin conjunction with gesture analysis and real-time audioprocessing components.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Music, Gesture, Interface, Wireless Sensors, Gesture Recog- nition, Audio Processing, Design, Interaction },
pages = {535--536},
}
@inproceedings{Schoonderwaldt2011,
author = {Schoonderwaldt, Erwin and Jensenius, Alexander R.},
url = {http://www.nime.org/proceedings/2011/nime2011_256.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Effective and Expressive Movements in a French-Canadian fiddler’s Performance},
year = {2011},
abstract = {We report on a performance study of a French-Canadian fiddler. The fiddling tradition forms an interesting contrast toclassical violin performance in several ways. Distinguishingfeatures include special elements in the bowing techniqueand the presence of an accompanying foot clogging pattern.These two characteristics are described, visualized and analyzed using video and motion capture recordings as sourcematerial.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {fiddler, violin, French-Canadian, bowing, feet, clogging, mo- tion capture, video, motiongram, kinematics, sonification },
pages = {256--259},
}
@inproceedings{Schroeder2011,
author = {Schroeder, Benjamin and Ainger, Marc and Parent, Richard},
url = {http://www.nime.org/proceedings/2011/nime2011_120.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {A Physically Based Sound Space for Procedural Agents},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {a human performer,agents,agents smoothly changing the,behavioral animation,figure 1,length of,physically based sound,pro-,strings being played by},
pages = {120--123},
}
@inproceedings{Seldess2011,
author = {Seldess, Zachary and Yamada, Toshiro},
url = {http://www.nime.org/proceedings/2011/nime2011_161.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Tahakum: A Multi-Purpose Audio Control Framework},
year = {2011},
abstract = {We present "Tahakum", an open source, extensible collection of software tools designed to enhance workflow on multichannel audio systems within complex multi-functional research and development environments. Tahakum aims to provide critical functionality required across a broad spectrum of audio systems usage scenarios, while at the same time remaining sufficiently open as to easily support modifications and extensions via 3rd party hardware and software. Features provided in the framework include software for custom mixing/routing and audio system preset automation, software for network message routing/redirection and protocol conversion, and software for dynamic audio asset management and control. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Audio Control Systems, Audio for VR, Max/MSP, Spatial Audio   },
pages = {161--166},
}
@inproceedings{Shear2011,
author = {Shear, Greg and Wright, Matthew},
url = {http://www.nime.org/proceedings/2011/nime2011_014.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Electromagnetically Sustained Rhodes Piano},
year = {2011},
abstract = {The Electromagnetically Sustained Rhodes Piano is an augmentation of the original instrument with additional control over the amplitude envelope of individual notes. Thisincludes slow attacks and infinite sustain while preservingthe familiar spectral qualities of this classic electromechanical piano. These additional parameters are controlled withaftertouch on the existing keyboard, extending standardpiano technique. Two sustain methods were investigated,driving the actuator first with a pure sine wave, and secondwith the output signal of the sensor. A special isolationmethod effectively decouples the sensors from the actuatorsand tames unruly feedback in the high-gain signal path.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Rhodes, keyboard, electromagnetic, sustain, augmented in- strument, feedback, aftertouch },
pages = {14--17},
}
@inproceedings{Sioros2011,
author = {Sioros, George and Guedes, Carlos},
url = {http://www.nime.org/proceedings/2011/nime2011_088.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Automatic Rhythmic Performance in Max/MSP: the kin.rhythmicator},
year = {2011},
abstract = {We introduce a novel algorithm for automatically generating rhythms in real time in a certain meter. The generated rhythms are "generic" in the sense that they are characteristic of each time signature without belonging to a specific musical style. The algorithm is based on a stochastic model in which various aspects and qualities of the generated rhythm can be controlled intuitively and in real time. Such qualities are the density of the generated events per bar, the amount of variation in generation, the amount of syncopation, the metrical strength, and of course the meter itself. The kin.rhythmicator software application was developed to implement this algorithm. During a performance with the kin.rhythmicator the user can control all aspects of the performance through descriptive and intuitive graphic controls. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {automatic music generation, generative, stochastic, metric  indispensability, syncopation, Max/MSP, Max4Live  },
pages = {88--91},
}
@inproceedings{Skogstad2011,
author = {Skogstad, St\aa le A. and de Quay, Yago and Jensenius, Alexander R.},
url = {http://www.nime.org/proceedings/2011/nime2011_300.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {OSC Implementation and Evaluation of the Xsens MVN Suit},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
pages = {300--303},
}
@inproceedings{Smallwood2011,
author = {Smallwood, Scott},
url = {http://www.nime.org/proceedings/2011/nime2011_028.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Solar Sound Arts: Creating Instruments and Devices Powered by Photovoltaic Technologies},
year = {2011},
abstract = {This paper describes recent developments in the creation of sound-making instruments and devices powered by photovoltaic (PV) technologies. With the rise of more efficient PV products in diverse packages, the possibilities for creating solar-powered musical instruments, sound installations, and loudspeakers are becoming increasingly realizable. This paper surveys past and recent developments in this area, including several projects by the ,
,
author, and demonstrates how the use of PV technologies can influence the creative process in unique ways. In addition, this paper discusses how solar sound arts can enhance the aesthetic direction taken by recent work in soundscape studies and acoustic ecology. Finally, this paper will point towards future directions and possibilities as PV technologies continue to evolve and improve in terms of performance, and become more affordable. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Solar Sound Arts, Circuit Bending, Hardware Hacking, Human-Computer Interface Design, Acoustic Ecology, Sound Art, Electroacoustics, Laptop Orchestra, PV Technology  },
pages = {28--31},
}
@inproceedings{Smith2011,
author = {Smith, Benjamin D. and Garnett, Guy E.},
url = {http://www.nime.org/proceedings/2011/nime2011_108.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Self-Supervising Machine},
year = {2011},
abstract = {Supervised machine learning enables complex many-to-manymappings and control schemes needed in interactive performance systems. One of the persistent problems in theseapplications is generating, identifying and choosing inputoutput pairings for training. This poses problems of scope(limiting the realm of potential control inputs), effort (requiring significant pre-performance training time), and cognitive load (forcing the performer to learn and remember thecontrol areas). We discuss the creation and implementationof an automatic "supervisor," using unsupervised machinelearning algorithms to train a supervised neural networkon the fly. This hierarchical arrangement enables networktraining in real time based on the musical or gestural control inputs employed in a performance, aiming at freeing theperformer to operate in a creative, intuitive realm, makingthe machine control transparent and automatic. Three implementations of this self supervised model driven by iPod,iPad, and acoustic violin are described.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {NIME, machine learning, interactive computer music, ma- chine listening, improvisation, adaptive resonance theory },
pages = {108--111},
}
@inproceedings{Snyder2011,
author = {Snyder, Jeff},
url = {http://www.nime.org/proceedings/2011/nime2011_413.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Snyderphonics Manta Controller, a Novel USB Touch-Controller},
year = {2011},
abstract = {The Snyderphonics Manta controller is a USB touch controller for music and video. It features 48 capacitive touch sensors, arranged in a hexagonal grid, with bi-color LEDs that are programmable from the computer. The sensors send continuous data proportional to surface area touched, and a velocitydetection algorithm has been implemented to estimate attack velocity based on this touch data. In addition to these hexagonal sensors, the Manta has two high-dimension touch sliders (giving 12-bit values), and four assignable function buttons. In this paper, I outline the features of the controller, the available methods for communicating between the device and a computer, and some current uses for the controller. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Snyderphonics, Manta, controller, USB, capacitive, touch, sensor, decoupled LED, hexagon, grid, touch slider, HID, portable, wood, live music, live video  },
pages = {413--416},
}
@inproceedings{Sosnick2011,
author = {Sosnick, Marc and Hsu, William},
url = {http://www.nime.org/proceedings/2011/nime2011_264.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Implementing a Finite Difference-Based Real-time Sound Synthesizer using GPUs},
year = {2011},
abstract = {In this paper, we describe an implementation of a real-time sound synthesizer using Finite Difference-based simulation of a two-dimensional membrane. Finite Difference (FD) methods can be the basis for physics-based music instrument models that generate realistic audio output. However, such methods are compute-intensive; large simulations cannot run in real time on current CPUs. Many current systems now include powerful Graphics Processing Units (GPUs), which are a good fit for FD methods. We demonstrate that it is possible to use this method to create a usable real-time audio synthesizer. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Finite Difference, GPU, CUDA, Synthesis  },
pages = {264--267},
}
@inproceedings{Tidemann2011,
author = {Tidemann, Axel},
url = {http://www.nime.org/proceedings/2011/nime2011_268.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {An Artificial Intelligence Architecture for Musical Expressiveness that Learns by Imitation},
year = {2011},
abstract = {Interacting with musical avatars have been increasingly popular over the years, with the introduction of games likeGuitar Hero and Rock Band. These games provide MIDIequipped controllers that look like their real-world counterparts (e.g. MIDI guitar, MIDI drumkit) that the users playto control their designated avatar in the game. The performance of the user is measured against a score that needs tobe followed. However, the avatar does not move in responseto how the user plays, it follows some predefined movementpattern. If the user plays badly, the game ends with theavatar ending the performance (i.e. throwing the guitar onthe floor). The gaming experience would increase if theavatar would move in accordance with user input. This paper presents an architecture that couples musical input withbody movement. Using imitation learning, a simulated human robot learns to play the drums like human drummersdo, both visually and auditory. Learning data is recordedusing MIDI and motion tracking. The system uses an artificial intelligence approach to implement imitation learning,employing artificial neural networks.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {artificial intelli-,drumming,modeling human behaviour},
pages = {268--271},
}
@inproceedings{Todoroff2011,
author = {Todoroff, Todor},
url = {http://www.nime.org/proceedings/2011/nime2011_515.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Wireless Digital/Analog Sensors for Music and Dance Performances},
year = {2011},
abstract = {We developed very small and light sensors, each equippedwith 3-axes accelerometers, magnetometers and gyroscopes.Those MARG (Magnetic, Angular Rate, and Gravity) sensors allow for a drift-free attitude computation which in turnleads to the possibility of recovering the skeleton of bodyparts that are of interest for the performance, improvingthe results of gesture recognition and allowing to get relative position between the extremities of the limbs and thetorso of the performer. This opens new possibilities in termsof mapping. We kept our previous approach developed atARTeM [2]: wireless from the body to the host computer,but wired through a 4-wire digital bus on the body. Byrelieving the need for a transmitter on each sensing node,we could built very light and flat sensor nodes that can bemade invisible under the clothes. Smaller sensors, coupledwith flexible wires on the body, give more freedom of movement to dancers despite the need for cables on the body.And as the weight of each sensor node, box included, isonly 5 grams (Figure 1), they can also be put on the upper and lower arm and hand of a violin or viola player, toretrieve the skeleton from the torso to the hand, withoutadding any weight that would disturb the performer. Weused those sensors in several performances with a dancingviola player and in one where she was simultaneously controlling gas flames interactively. We are currently applyingthem to other types of musical performances.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {wireless MARG sensors },
pages = {515--518},
}
@inproceedings{Tseng2011,
author = {Tseng, Yu-Chung and Liu, Che-Wei and Chi, Tzu-Heng and Wang, Hui-Yu},
url = {http://www.nime.org/proceedings/2011/nime2011_320.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Sound Low Fun},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
pages = {320--321},
}
@inproceedings{Ustarroz2011,
author = {Ustarroz, Paula},
url = {http://www.nime.org/proceedings/2011/nime2011_425.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {TresnaNet Musical Generation based on Network Protocols},
year = {2011},
abstract = {TresnaNet explores the potential of Telematics as a generator ofmusical expressions. I pretend to sound the silent flow ofinformation from the network.This is realized through the fabrication of a prototypefollowing the intention of giving substance to the intangibleparameters of our communication. The result may haveeducational, commercial and artistic applications because it is aphysical and perceptible representation of the transfer ofinformation over the network. This paper describes the design,implementation and conclusions about TresnaNet.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Interface, musical generation, telematics, network, musical instrument, network sniffer. },
pages = {425--428},
}
@inproceedings{Verplank2011,
author = {Verplank, Bill and Georg, Francesco},
url = {http://www.nime.org/proceedings/2011/nime2011_539.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Can Haptics Make New Music ? - Fader and Plank Demos},
year = {2011},
abstract = {Haptic interfaces using active force-feedback have mostly been used for emulating existing instruments and making conventional music. With the right speed, force, precision and software they can also be used to make new sounds and perhaps new music. The requirements are local microprocessors (for low-latency and high update rates), strategic sensors (for force as well as position), and non-linear dynamics (that make for rich overtones and chaotic music).},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {NIME, Haptics, Music Controllers, Microprocessors.  },
pages = {539--540},
}
@inproceedings{VonFalkenstein2011,
author = {von Falkenstein, Jan T.},
url = {http://www.nime.org/proceedings/2011/nime2011_527.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Gliss : An Intuitive Sequencer for the iPhone and iPad},
year = {2011},
abstract = {Gliss is an application for iOS that lets the user sequence five separate instruments and play them back in various ways. Sequences can be created by drawing onto the screen while the sequencer is running. The playhead of the sequencer can be set to randomly deviate from the drawings or can be controlled via the accelerometer of the device. This makes Gliss a hybrid of a sequencer, an instrument and a generative music system. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Gliss, iOS, iPhone, iPad, interface, UPIC, music, sequencer, accelerometer, drawing  },
pages = {527--528},
}
@inproceedings{Waadeland2011,
author = {Waadeland, Carl H.},
url = {http://www.nime.org/proceedings/2011/nime2011_248.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Rhythm Performance from a Spectral Point of View},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {gesture,movement,rhythm performance,spectral analysis},
pages = {248--251},
}
@inproceedings{Wang2011,
author = {Wang, Ge and Oh, Jieun and Lieber, Tom},
url = {http://www.nime.org/proceedings/2011/nime2011_197.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Designing for the iPad : Magic Fiddle},
year = {2011},
abstract = {This paper describes the origin, design, and implementation of Smule's Magic Fiddle, an expressive musical instrument for the iPad. Magic Fiddle takes advantage of the physical aspects of the device to integrate game-like and pedagogical elements. We describe the origin of Magic Fiddle, chronicle its design process, discuss its integrated music education system, and evaluate the overall experience. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Magic Fiddle, iPad, physical interaction design, experiential design, music education.  },
pages = {197--202},
}
@inproceedings{Wang2011a,
author = {Wang, Johnty and d'Alessandro, Nicolas and Fels, Sidney S. and Pritchard, Bob},
url = {http://www.nime.org/proceedings/2011/nime2011_531.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {SQUEEZY : Extending a Multi-touch Screen with Force Sensing Objects for Controlling Articulatory Synthesis},
year = {2011},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
pages = {531--532},
}
@inproceedings{Wyse2011,
author = {Wyse, Lonce and Mitani, Norikazu and Nanayakkara, Suranga},
url = {http://www.nime.org/proceedings/2011/nime2011_304.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {The Effect of Visualizing Audio Targets in a Musical Listening and Performance Task},
year = {2011},
abstract = {The goal of our research is to find ways of supporting and encouraging musical behavior by non-musicians in shared public performance environments. Previous studies indicated simultaneous music listening and performance is difficult for non-musicians, and that visual support for the task might be helpful. This paper presents results from a preliminary user study conducted to evaluate the effect of visual feedback on a musical tracking task. Participants generated a musical signal by manipulating a hand-held device with two dimensions of control over two parameters, pitch and density of note events, and were given the task of following a target pattern as closely as possible. The target pattern was a machine-generated musical signal comprising of variation over the same two parameters. Visual feedback provided participants with information about the control parameters of the musical signal generated by the machine. We measured the task performance under different visual feedback strategies. Results show that single parameter visualizations tend to improve the tracking performance with respect to the visualized parameter, but not the non-visualized parameter. Visualizing two independent parameters simultaneously decreases performance in both dimensions. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Mobile phone, Interactive music performance, Listening, Group music play, Visual support   },
pages = {304--307},
}
@inproceedings{Yoo2011,
author = {Yoo, Min-Joon and Beak, Jin-Wook and Lee, In-Kwon},
url = {http://www.nime.org/proceedings/2011/nime2011_324.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Creating Musical Expression using Kinect},
year = {2011},
abstract = {Recently, Microsoft introduced a game interface called Kinect for the Xbox 360 video game platform. This interface enables users to control and interact with the game console without the need to touch a controller. It largely increases the users' degree of freedom to express their emotion. In this paper, we first describe the system we developed to use this interface for sound generation and controlling musical expression. The skeleton data are extracted from users' motions and the data are translated to pre-defined MIDI data. We then use the MIDI data to control several applications. To allow the translation between the data, we implemented a simple Kinect-to-MIDI data convertor, which is introduced in this paper. We describe two applications to make music with Kinect: we first generate sound with Max/MSP, and then control the adlib with our own adlib generating system by the body movements of the users. },
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Kinect, gaming interface, sound generation, adlib generation   },
pages = {324--325},
}
@inproceedings{Zamborlin2011,
author = {Zamborlin, Bruno and Partesana, Giorgio and Liuni, Marco},
url = {http://www.nime.org/proceedings/2011/nime2011_537.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {(LAND)MOVES},
year = {2011},
abstract = {(land)moves is an interactive installation: the user's gestures control the multimedia processing with a total synergybetween audio and video synthesis and treatment.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {mapping gesture-audio-video, gesture recognition, landscape, soundscape },
pages = {537--538},
}
@inproceedings{Zappi2011,
author = {Zappi, Victor and Mazzanti, Dario and Brogni, Andrea and Caldwell, Darwin},
url = {http://www.nime.org/proceedings/2011/nime2011_355.pdf},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
title = {Design and Evaluation of a Hybrid Reality Performance},
year = {2011},
abstract = {In this paper we introduce a multimodal platform for Hybrid Reality live performances: by means of non-invasiveVirtual Reality technology, we developed a system to presentartists and interactive virtual objects in audio/visual choreographies on the same real stage. These choreographiescould include spectators too, providing them with the possibility to directly modify the scene and its audio/visual features. We also introduce the first interactive performancestaged with this technology, in which an electronic musician played live five tracks manipulating the 3D projectedvisuals. As questionnaires have been distributed after theshow, in the last part of this work we discuss the analysisof collected data, underlining positive and negative aspectsof the proposed experience.This paper belongs together with a performance proposalcalled Dissonance, in which two performers exploit the platform to create a progressive soundtrack along with the exploration of an interactive virtual environment.},
editor = {Jensenius, Alexander R. and Tveit, Anders and God\o y, Rolf I. and Overholt, Dan},
address = {Oslo, Norway},
keywords = {Interactive Performance, Hybrid Choreographies, Virtual Reality, Music Control },
pages = {355--360},
}
