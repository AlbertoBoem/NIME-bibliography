% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{Allen2005,
  Title                    = {boomBox},
  Author                   = {Allen, Jamie},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {168--171},

  Abstract                 = {This paper describes the development, function andperformance contexts of a digital musical instrument called"boomBox." The instrument is a wireless, orientation-awarelow-frequency, high-amplitude human motion controller forlive and sampled sound. The instrument has been used inperformance and sound installation contexts. I describe someof what I have learned from the project herein.},
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_168.pdf}
}

@InProceedings{Allison2005,
  Title                    = {Teabox: A Sensor Data Interface System},
  Author                   = {Allison, Jesse T. and Place, Timothy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {56--59},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_056.pdf}
}

@InProceedings{Baird2005,
  Title                    = {Real-Time Generation of Music Notation via Audience Interaction Using Python and {GNU} Lilypond},
  Author                   = {Baird, Kevin C.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {240--241},

  Abstract                 = {No Clergy is an interactive music performance/installation inwhich the audience is able to shape the ongoing music. In it,members of a small acoustic ensemble read music notation fromcomputer screens. As each page refreshes, the notation is alteredand shaped by both stochastic transformations of earlier musicwith the same performance and audience feedback, collected viastandard CGI forms. },
  Keywords                 = {notation, stochastic, interactive, audience, Python, Lilypond },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_240.pdf}
}

@InProceedings{Barbosa2005,
  Title                    = {Network Latency Adaptive Tempo in the Public Sound Objects System},
  Author                   = {Barbosa, Alvaro and Cardoso, Jorge and Geiger, G\"{u}nter},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {184--187},

  Abstract                 = {In recent years Computer Network-Music has increasingly captured the attention of the Computer Music Community. With the advent of Internet communication, geographical displacement amongst the participants of a computer mediated music performance achieved world wide extension. However, when established over long distance networks, this form of musical communication has a fundamental problem: network latency (or net-delay) is an impediment for real-time collaboration. From a recent study, carried out by the ,
,
authors, a relation between network latency tolerance and Music Tempo was established. This result emerged from an experiment, in which simulated network latency conditions were applied to the performance of different musicians playing jazz standard tunes. The Public Sound Objects (PSOs) project is web-based shared musical space, which has been an experimental framework to implement and test different approaches for on-line music communication. This paper describe features implemented in the latest version of the PSOs system, including the notion of a network-music instrument incorporating latency as a software function, by dynamically adapting its tempo to the communication delay measured in real-time. },
  Keywords                 = {Network Music Instruments; Latency in Real-Time Performance; Interface-Decoupled Electronic Musical Instruments; Behavioral Driven Interfaces; Collaborative Remote Music Performance; },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_184.pdf}
}

@InProceedings{Bardos2005,
  Title                    = {Bangarama: Creating Music With Headbanging},
  Author                   = {Bardos, Laszlo and Korinek, Stefan and Lee, Eric and Borchers, Jan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {180--183},

  Abstract                 = {Bangarama is a music controller using headbanging as the primaryinteraction metaphor. It consists of a head-mounted tilt sensor and aguitar-shaped controller that does not require complex finger positions. We discuss the specific challenges of designing and buildingthis controller to create a simple, yet responsive and playable instrument, and show how ordinary materials such as plywood, tinfoil, and copper wire can be turned into a device that enables a fun,collaborative music-making experience.},
  Keywords                 = {head movements, music controllers, interface design, input devices },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_180.pdf}
}

@InProceedings{Bencina2005,
  Title                    = {The Metasurface -- Applying Natural Neighbour Interpolation to Two-to-Many Mapping},
  Author                   = {Bencina, Ross},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {101--104},

  Keywords                 = {computational geometry,design,design support,high-level control,interpolation,mapping,of interpo-,this section reviews related,user interface,work in the field},
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_101.pdf}
}

@InProceedings{Bevilacqua2005,
  Title                    = {MnM: a Max/MSP mapping toolbox},
  Author                   = {Bevilacqua, Fr\'{e}d\'{e}ric and M\"{u}ller, R\'{e}my and Schnell, Norbert},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {85--88},

  Abstract                 = {In this report, we describe our development on the Max/MSPtoolbox MnM dedicated to mapping between gesture andsound, and more generally to statistical and machine learningmethods. This library is built on top of the FTM library, whichenables the efficient use of matrices and other data structuresin Max/MSP. Mapping examples are described based onvarious matrix manipulations such as Single ValueDecomposition. The FTM and MnM libraries are freelyavailable.},
  Keywords                 = {Mapping, interface design, matrix, Max/MSP. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_085.pdf}
}

@InProceedings{Birchfield2005,
  Title                    = {Sustainable: a dynamic, robotic, sound installation},
  Author                   = {Birchfield, David and Lorig, David and Phillips, Kelly},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {160--163},

  Abstract                 = {This paper details the motivations, design, and realization of Sustainable, a dynamic, robotic sound installation that employs a generative algorithm for music and sound creation. The piece is comprised of seven autonomous water gong nodes that are networked together by water tubes to distribute water throughout the system. A water resource allocation algorithm guides this distribution process and produces an ever-evolving sonic and visual texture. A simple set of behaviors govern the individual gongs, and the system as a whole exhibits emergent properties that yield local and large scale forms in sound and light. },
  Keywords                 = {computing,dynamic systems,evolutionary,generative arts,installation art,music,robotics,sculpture,sound},
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_160.pdf}
}

@InProceedings{Birnbaum2005,
  Title                    = {Towards a Dimension Space for Musical Devices},
  Author                   = {Birnbaum, David and Fiebrink, Rebecca and Malloch, Joseph and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {192--195},

  Keywords                 = {design space analysis,human-computer interaction,interfaces for musical expression,new},
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_192.pdf}
}

@InProceedings{Blaine2005,
  Title                    = {The Convergence of Alternate Controllers and Musical Interfaces in Interactive Entertainment},
  Author                   = {Blaine, Tina},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {27--33},

  Abstract                 = {This paper will investigate a variety of alternate controllers that are making an impact in interactive entertainment, particularly in the video game industry. Since the late 1990's, the surging popularity of rhythmic and musical performance games in Japanese arcades has led to the development of new interfaces and alternate controllers for the consumer market worldwide. Rhythm action games such as Dance Dance Revolution, Taiko No Tatsujin (Taiko: Drum Master), and Donkey Konga are stimulating collaborative gameplay and exposing consumers to custom controllers designed specifically for musical and physical interaction. We are witnessing the emergence and acceptance of these breakthrough controllers and models for gameplay as an international cultural phenomenon penetrating the video game and toy markets in record numbers. Therefore, it is worth considering the potential benefits to developers of musical interfaces, electronic devices and alternate controllers in light of these new and emerging opportunities, particularly in the realm of video gaming, toy development, arcades, and other interactive entertainment experiences. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_027.pdf}
}

@InProceedings{Bowen2005,
  Title                    = {Soundstone: A {3-D} Wireless Music Controller},
  Author                   = {Bowen, Adam},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {268--269},

  Abstract                 = {Soundstone is a small wireless music controller that tracks movement and gestures, and maps these signals to characteristics of various synthesized and sampled sounds. It is intended to become a general-purpose platform for exploring the sonification of movement, with an emphasis on tactile (haptic) feedback. },
  Keywords                 = {Gesture recognition, haptics, human factors, force, acceleration, tactile feedback, general purpose controller, wireless. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_268.pdf}
}

@InProceedings{Bowers2005,
  Title                    = {Not Hyper, Not Meta, Not Cyber but Infra-Instruments},
  Author                   = {Bowers, John and Archer, Phil},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {5--10},

  Keywords                 = {Infra-instruments, hyperinstruments, meta-instruments, virtual instruments, design concepts and principles. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_005.pdf}
}

@InProceedings{Buchla2005,
  Title                    = {A History of Buchla\'s Musical Instruments},
  Author                   = {Buchla, Don},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {1--1},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_001.pdf}
}

@InProceedings{Buxton2005,
  Title                    = {Causality and Striking the Right Note},
  Author                   = {Buxton, Bill},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {4--4},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_004.pdf}
}

@InProceedings{Caceres2005,
  Title                    = {{SC}UBA: The Self-Contained Unified Bass Augmenter},
  Author                   = {C\'{a}ceres, Juan Pablo and Mysore, Gautham J. and Trevi\~{n}o, Jeffrey},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {38--41},

  Abstract                 = {The Self-Contained Unified Bass Augmenter (SCUBA) is a new augmentative OSC (Open Sound Control) [5] controller for the tuba. SCUBA adds new expressive possibilities to the existing tuba interface through onboard sensors. These sensors provide continuous and discrete user-controlled parametric data to be mapped at will to signal processing parameters, virtual instrument control parameters, sound playback, and various other functions. In its current manifestation, control data is mapped to change the processing of the instrument's natural sound in Pd (Pure Data) [3]. SCUBA preserves the unity of the solo instrument interface by acoustically mixing direct and processed sound in the instrument's bell via mounted satellite speakers, which are driven by a subwoofer below the performer's chair. The end result augments the existing interface while preserving its original unity and functionality. },
  Keywords                 = {Interactive music, electro-acoustic musical instruments, musical instrument design, human computer interface, signal processing, Open Sound Control (OSC) },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_038.pdf}
}

@InProceedings{Carlile2005,
  Title                    = {{OR}OBORO: A Collaborative Controller with Interpersonal Haptic Feedback},
  Author                   = {Carlile, Jennifer and Hartmann, Bj{\"{o}}rn},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {250--251},

  Keywords                 = {Musical Controller, Collaborative Control, Haptic Interfaces },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_250.pdf}
}

@InProceedings{Carter2005,
  Title                    = {Location33: A Mobile Musical},
  Author                   = {Carter, William and Liu, Leslie S.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {176--179},

  Abstract                 = {In this paper, we describe a course of research investigating thepotential for new types of music made possible by locationtracking and wireless technologies. Listeners walk arounddowntown Culver City, California and explore a new type ofmusical album by mixing together songs and stories based ontheir movement. By using mobile devices as an interface, wecan create new types of musical experiences that allowlisteners to take a more interactive approach to an album.},
  Keywords                 = {Mobile Music, Digital Soundscape, Location-Based Entertainment, Mobility, Interactive Music, Augmented Reality },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_176.pdf}
}

@InProceedings{Chew2005,
  Title                    = {ESP: A Driving Interface for Expression Synthesis},
  Author                   = {Chew, Elaine and Francois, Alexander R. and Liu, Jie and Yang, Aaron},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {224--227},

  Abstract                 = {In the Expression Synthesis Project (ESP), we propose adriving interface for expression synthesis. ESP aims toprovide a compelling metaphor for expressive performance soas to make high-level expressive decisions accessible to nonexperts. In ESP, the user drives a car on a virtual road thatrepresents the music with its twists and turns; and makesdecisions on how to traverse each part of the road. The driver'sdecisions affect in real-time the rendering of the piece. Thepedals and wheel provide a tactile interface for controlling thecar dynamics and musical expression, while the displayportrays a first person view of the road and dashboard from thedriver's seat. This game-like interface allows non-experts tocreate expressive renderings of existing music without havingto master an instrument, and allows expert musicians toexperiment with expressive choice without having to firstmaster the notes of the piece. The prototype system has beentested and refined in numerous demonstrations. This paperpresents the concepts underlying the ESP system and thearchitectural design and implementation of a prototype.},
  Keywords                 = {Music expression synthesis system, driving interface. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_224.pdf}
}

@InProceedings{Cook2005,
  Title                    = {Real-Time Performance Controllers for Synthesized Singing},
  Author                   = {Cook, Perry R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {236--237},

  Abstract                 = {A wide variety of singing synthesis models and methods exist,but there are remarkably few real-time controllers for thesemodels. This paper describes a variety of devices developedover the last few years for controlling singing synthesismodels implemented in the Synthesis Toolkit in C++ (STK),Max/MSP, and ChucK. All of the controllers share somecommon features, such as air-pressure sensing for breathingand/or loudness control, means to control pitch, and methodsfor selecting and blending phonemes, diphones, and words.However, the form factors, sensors, mappings, and algorithmsvary greatly between the different controllers.},
  Keywords                 = {Singing synthesis, real-time singing synthesis control. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_236.pdf}
}

@InProceedings{Crevoisier2005,
  Title                    = {Tangible Acoustic Interfaces and their Applications for the Design of New Musical Instruments},
  Author                   = {Crevoisier, Alain and Polotti, Pietro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {97--100},

  Abstract                 = {Tangible Acoustic Interfaces (TAI) rely on various acousticsensing technologies, such as sound source location and acoustic imaging, to detect the position of contact of users interacting with the surface of solid materials. With their ability to transform almost any physical objects, flat or curved surfaces and walls into interactive interfaces, acoustic sensing technologies show a promising way to bring the sense of touch into the realm of computer interaction. Because music making has been closely related to this sense during centuries, an application of particular interest is the use of TAI's for the design of new musical instruments that matches the physicality and expressiveness of classical instruments. This paper gives an overview of the various acoustic-sensing technologies involved in the realisation of TAI's and develops on the motivation underlying their use for the design of new musical instruments. },
  Keywords                 = {Tangible interfaces, new musical instruments design. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_097.pdf}
}

@InProceedings{Dannenberg2005,
  Title                    = {McBlare: A Robotic Bagpipe Player},
  Author                   = {Dannenberg, Roger B. and Brown, Ben and Zeglin, Garth and Lupish, Ron},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {80--84},

  Abstract                 = {McBlare is a robotic bagpipe player developed by the Robotics Institute at Carnegie Mellon University. McBlare plays a standard set of bagpipes, using a custom air compressor to supply air and electromechanical ``fingers'' to control the chanter. McBlare is MIDI controlled, allowing for simple interfacing to a keyboard, computer, or hardware sequencer. The control mechanism exceeds the measured speed of expert human performers. On the other hand, human performers surpass McBlare in their ability to compensate for limitations and imperfections in reeds, and we discuss future enhancements to address these problems. McBlare has been used to perform traditional bagpipe music as well as experimental computer generated music. },
  Keywords                 = {bagpipes, robot, music, instrument, MIDI },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_080.pdf}
}

@InProceedings{Deutscher2005,
  Title                    = {Echology},
  Author                   = {Deutscher, Meghan and Fels, Sidney S. and Hoskinson, Reynald and Takahashi, Sachiyo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {274--274},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_274.pdf}
}

@InProceedings{Eaton2005,
  Title                    = {Multiple-Touch-Sensitive Keyboard},
  Author                   = {Eaton, John and Moog, Robert},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {258--259},

  Abstract                 = {In this presentation, we discuss and demonstrate a multiple touch sensitive (MTS) keyboard developed by Robert Moog for John Eaton. Each key of the keyboard is equipped with sensors that detect the three-dimensional position of the performer's finger. The presentation includes some of Eaton's performances for certain earlier prototypes as well as this keyboard. },
  Keywords                 = {Multiple touch sensitive, MTS, keyboard, key sensor design, upgrading to present-day computers },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_258.pdf}
}

@InProceedings{Essl2005,
  Title                    = {Scrubber: An Interface for Friction-induced Sounds},
  Author                   = {Essl, Georg and O'Modhrain, Sile},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {70--75},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_070.pdf}
}

@InProceedings{Flety2005,
  Title                    = {The WiSe Box: a Multi-performer Wireless Sensor Interface using {WiFi} and OSC},
  Author                   = {Fl\'{e}ty, Emmanuel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {266--267},

  Abstract                 = {The Wise Box is a new wireless digitizing interface for sensors and controllers. An increasing demand for this kind of hardware, especially in the field of dance and computer performance lead us to design a wireless digitizer that allows for multiple users, with high bandwidth and accuracy. The interface design was initiated in early 2004 and shortly described in reference [1]. Our recent effort was directed to make this device available for the community on the form of a manufactured product, similarly to our previous interfaces such as AtoMIC Pro, Eobody or Ethersense [1][2][3]. We describe here the principles we used for the design of the device as well as its technical specifications. The demo will show several devices running at once and used in real-time with a various set of sensors. },
  Keywords                 = {Gesture, Sensors, WiFi, 802.11, OpenSoundControl. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_266.pdf}
}

@InProceedings{Fox2005,
  Title                    = {SoniMime: Movement Sonification for Real-Time Timbre Shaping},
  Author                   = {Fox, Jesse and Carlile, Jennifer},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {242--243},

  Abstract                 = {This paper describes the design of SoniMime, a system forthe sonification of hand movement for real-time timbre shaping. We explore the application of the tristimulus timbremodel for the sonification of gestural data, working towardthe goals of musical expressivity and physical responsiveness. SoniMime uses two 3-D accelerometers connected toan Atmel microprocessor which outputs OSC control messages. Data filtering, parameter mapping, and sound synthesis take place in Pd running on a Linux computer.},
  Keywords                 = {Sonification, Musical Controller, Human Computer Interac- tion },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_242.pdf}
}

@InProceedings{Fraietta2005,
  Title                    = {Smart Controller / Bell Garden Demo},
  Author                   = {Fraietta, Angelo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {260--261},

  Abstract                 = {This paper will demonstrate the use of the Smart Controller workbench in the Interactive Bell Garden. },
  Keywords                 = {Control Voltage, Open Sound Control, Algorithmic Composition, MIDI, Sound Installations, Programmable Logic Control, Synthesizers. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_260.pdf}
}

@InProceedings{Fraietta2005a,
  Title                    = {The Smart Controller Workbench},
  Author                   = {Fraietta, Angelo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {46--49},

  Abstract                 = {The Smart Controller is a portable hardware device that responds to input control voltage, OSC, and MIDI messages; producing output control voltage, OSC, and MIDI messages (depending upon the loaded custom patch). The Smart Controller is a stand alone device; a powerful, reliable, and compact instrument capable of reducing the number of electronic modules required in a live performance or installation, particularly the requirement of a laptop computer. More powerful, however, is the Smart Controller Workbench, a complete interactive development environment. In addition to enabling the composer to create and debug their patches, the Smart Controller Workbench accurately simulates the behaviour of the hardware, and functions as an incircuit debugger that enables the performer to remotely monitor, modify, and tune patches running in an installation without the requirement of stopping or interrupting the live performance. },
  Keywords                 = {Control Voltage, Open Sound Control, Algorithmic Composition, MIDI, Sound Installations, programmable logic control, synthesizers, electronic music, Sensors, Actuators, Interaction. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_046.pdf}
}

@InProceedings{Franco2005,
  Title                    = {The Airstick: A Free-Gesture Controller Using Infrared Sensing},
  Author                   = {Franco, Ivan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {248--249},

  Abstract                 = {This paper describes the development of AirStick, an interface for musical expression. AirStick is played "in the air", in a Theremin style. It is composed of an array of infrared proximity sensors, which allow the mapping of the position of any interfering obstacle inside a bi-dimensional zone. This controller sends both x and y control data to various real-time synthesis algorithms. },
  Keywords                 = {Music Controller, Infrared Sensing, Computer Music. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_248.pdf}
}

@InProceedings{Funk2005,
  Title                    = {Sonification of Facial Actions for Musical Expression},
  Author                   = {Funk, Mathias and Kuwabara, Kazuhiro and Lyons, Michael J.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {127--131},

  Abstract                 = {The central role of the face in social interaction and non-verbal communication suggest we explore facial action as a means of musical expression. This paper presents the design, implementation, and preliminary studies of a novel system utilizing face detection and optic flow algorithms to associate facial movements with sound synthesis in a topographically specific fashion. We report on our experience with various gesture-to-sound mappings and applications, and describe our preliminary experiments at musical performance using the system. },
  Keywords                 = {Video-based musical interface; gesture-based interaction; facial expression; facial therapy interface. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_127.pdf}
}

@InProceedings{Guisan2005,
  Title                    = {Interactive Sound Installation: INTRIUM},
  Author                   = {Guisan, Alain C.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {270--270},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_270.pdf}
}

@InProceedings{Gutknecht2005,
  Title                    = {GoingPublik: Using Realtime Global Score Synthesis},
  Author                   = {Gutknecht, J{\"u}rg and Clay, Art and Frey, Thomas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {148--151},

  Abstract                 = {This paper takes the reader through various elements of the GoingPublik sound artwork for distributive ensemble and introduces the Realtime Score Synthesis tool (RSS) used as a controller in the work. The collaboration between artists and scientists, details concerning the experimental hardware and software, and new theories of sound art are briefly explained and illustrated. The scope of this project is too broad to be fully covered in this paper, therefore the selection of topics made attempts to draw attention to the work itself and balance theory with practice. },
  Keywords                 = {Mobile Multimedia, Wearable Computers, Score Synthesis, Sound Art, System Research, HCIs },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_148.pdf}
}

@InProceedings{Hindman2005,
  Title                    = {Sonictroller},
  Author                   = {Hindman, David and Kiser, Spencer},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {254--255},

  Abstract                 = {The Sonictroller was originally conceived as a means ofintroducing competition into an improvisatory musicalperformance. By reverse-engineering a popular video gameconsole, we were able to map sound information (volume,pitch, and pitch sequences) to any continuous or momentaryaction of a video game sprite.},
  Keywords                 = {video game, Nintendo, music, sound, controller, Mortal Kombat, trumpet, guitar, voice },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_254.pdf}
}

@InProceedings{Huott2005,
  Title                    = {Precise Control on Compound Curves},
  Author                   = {Huott, Robert},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {244--245},

  Keywords                 = {Musical controller, sensate surface, mapping system },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_244.pdf}
}

@InProceedings{Janer2005,
  Title                    = {Voice-controlled plucked bass guitar through two synthesis techniques},
  Author                   = {Janer, Jordi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {132--135},

  Abstract                 = {In this paper we present an example of the use of the singingvoice as a controller for digital music synthesis. The analysis of the voice with spectral processing techniques, derivedfrom the Short-Time Fourier Transform, provides ways ofdetermining a performer's vocal intentions. We demonstratea prototype, in which the extracted vocal features drive thesynthesis of a plucked bass guitar. The sound synthesis stageincludes two different synthesis techniques, Physical Modelsand Spectral Morph.},
  Keywords                 = {Singing voice, musical controller, sound synthesis, spectral processing. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_132.pdf}
}

@InProceedings{Jorda2005,
  Title                    = {Multi-user Instruments: Models, Examples and Promises},
  Author                   = {Jord\`{a}, Sergi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {23--26},

  Abstract                 = {In this paper we study the potential and the challenges posed by multi-user instruments, as tools that can facilitate interaction and responsiveness not only between performers and their instrument but also between performers as well. Several previous studies and taxonomies are mentioned, after what different paradigms exposed with examples based on traditional mechanical acoustic instruments. In the final part, several existing systems and implementations, now in the digital domain, are described and identified according to the models and paradigms previously introduced. },
  Keywords                 = {Multi-user instruments, collaborative music, new instruments design guidelines. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_023.pdf}
}

@InProceedings{Kim-Boyle2005,
  Title                    = {Musical Score Generation in Valses and Etudes},
  Author                   = {Kim-Boyle, David},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {238--239},

  Abstract                 = {The ,
,
author describes a recent composition for piano andcomputer in which the score performed by the pianist, readfrom a computer monitor, is generated in real-time from avocabulary of predetermined scanned score excerpts. The,
,
author outlines the algorithm used to choose and display aparticular excerpt and describes some of the musicaldifficulties faced by the pianist in a performance of the work. },
  Keywords                 = {Score generation, Jitter. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_238.pdf}
}

@InProceedings{Kobayashi2005,
  Title                    = {Spinner: A Simple Approach to Reconfigurable User Interfaces},
  Author                   = {Kobayashi, Shigeru and Masayuki, Akamasu},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {208--211},

  Keywords                 = {Reconfigurable, Sensors, Computer Music },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_208.pdf}
}

@InProceedings{Lee2005,
  Title                    = {The Role of Time in Engineering Computer Music Systems},
  Author                   = {Lee, Eric and Borchers, Jan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {204--207},

  Abstract                 = {Discussion of time in interactive computer music systems engineering has been largely limited to data acquisition rates and latency.Since music is an inherently time-based medium, we believe thattime plays a more important role in both the usability and implementation of these systems. In this paper, we present a time designspace, which we use to expose some of the challenges of developing computer music systems with time-based interaction. Wedescribe and analyze the time-related issues we encountered whilstdesigning and building a series of interactive music exhibits thatfall into this design space. These issues often occur because ofthe varying and sometimes conflicting conceptual models of timein the three domains of user, application (music), and engineering.We present some of our latest work in conducting gesture interpretation and frameworks for digital audio, which attempt to analyzeand address these conflicts in temporal conceptual models.},
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_204.pdf}
}

@InProceedings{Lehrman2005,
  Title                    = {Bridging the Gap Between Art and Science Education Through Teaching Electronic Musical Instrument Design},
  Author                   = {Lehrman, Paul D. and Ryan, Todd M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {136--139},

  Abstract                 = {Electronic Musical Instrument Design is an excellent vehiclefor bringing students from multiple disciplines together towork on projects, and help bridge the perennial gap betweenthe arts and the sciences. This paper describes how at TuftsUniversity, a school with no music technology program,students from the engineering (electrical, mechanical, andcomputer), music, performing arts, and visual arts areas usetheir complementary skills, and teach each other, to developnew devices and systems for music performance and control.},
  Keywords                 = {Science education, music education, engineering, electronic music, gesture controllers, MIDI. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_136.pdf}
}

@InProceedings{Levin2005,
  Title                    = {A Personal Chronology of Audiovisual Systems Research},
  Author                   = {Levin, Golan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {2--3},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_002.pdf}
}

@InProceedings{Levin2005a,
  Title                    = {Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours in The Manual Input Sessions},
  Author                   = {Levin, Golan and Lieberman, Zachary},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {115--120},

  Abstract                 = {We report on The Manual Input Sessions, a series of audiovisual vignettes which probe the expressive possibilities of free-form hand gestures. Performed on a hybrid projection system which combines a traditional analog overhead projector and a digital PC video projector, our vision-based software instruments generate dynamic sounds and graphics solely in response to the forms and movements of the silhouette contours of the user's hands. Interactions and audiovisual mappings which make use of both positive (exterior) and negative (interior) contours are discussed. },
  Keywords                 = {Audiovisual performance, hand silhouettes, computer vision, contour analysis, sound-image relationships, augmented reality. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_115.pdf}
}

@InProceedings{Livingstone2005,
  Title                    = {Orb3 -- Adaptive Interface Design for Real time Sound Synthesis \& Diffusion within Socially Mediated Spaces},
  Author                   = {Livingstone, Dan and Miranda, Eduardo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {65--69},

  Keywords                 = {Adaptive System, Sound Installation, Smart Interfaces, Music Robots, Spatial Music, Conscious Subconscious Interaction. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_065.pdf}
}

@InProceedings{Loscos2005,
  Title                    = {The wahwactor: a voice controlled wah-wah pedal},
  Author                   = {Loscos, Alex and Aussenac, Thomas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {172--175},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_172.pdf}
}

@InProceedings{Lugo2005,
  Title                    = {Beat Boxing : Expressive Control for Electronic Music Performance and Musical Applications},
  Author                   = {Lugo, Robert and Damondrick, Jack},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {246--247},

  Abstract                 = {This paper describes the design and implementation of BeatBoxing, a percussive gestural interface for the liveperformance of electronic music and control of computerbased games and musical activities.},
  Keywords                 = {Performance, Gestural Mapping, Music Controller, Human- Computer Interaction, PureData (Pd), OSC },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_246.pdf}
}

@InProceedings{Maki-patola2005,
  Title                    = {Experiments with Virtual Reality Instruments},
  Author                   = {M\"{a}ki-patola, Teemu and Laitinen, Juha and Kanerva, Aki and Takala, Tapio},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {11--16},

  Abstract                 = {In this paper, we introduce and analyze four gesture-controlled musical instruments. We briefly discuss the test platform designed to allow for rapid experimentation of new interfaces and control mappings. We describe our design experiences and discuss the effects of system features such as latency, resolution and lack of tactile feedback. The instruments use virtual reality hardware and computer vision for user input, and three-dimensional stereo vision as well as simple desktop displays for providing visual feedback. The instrument sounds are synthesized in real-time using physical sound modeling. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_011.pdf}
}

@InProceedings{Magnusson2005,
  Title                    = {ixi software: The Interface as Instrument},
  Author                   = {Magnusson, Thor},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {212--215},

  Abstract                 = {This paper describes the audio human computer interface experiments of ixi in the past and outlines the current platform for future research. ixi software [5] was founded by Thor Magnusson and Enrike Hurtado Mendieta in year 2000 and since then we've been working on building prototypes in the form of screen-based graphical user interfaces for musical performance, researching human computer interaction in the field of music and creating environments which other people can use to do similar work and for us to use in our workshops. Our initial starting point was that computer music software and the way their interfaces are built need not necessarily be limited to copying the acoustic musical instruments and studio technology that we already have, but additionally we can create unique languages and work processes for the virtual world. The computer is a vast creative space with specific qualities that can and should be explored. },
  Keywords                 = {Graphical user interfaces, abstract graphical interfaces, hyper- control, intelligent instruments, live performance, machine learning, catalyst software, OSC, interfacing code, open source, Pure Data, SuperCollider. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_212.pdf}
}

@InProceedings{Maki-patola2005b,
  Title                    = {User Interface Comparison for Virtual Drums},
  Author                   = {Maki-patola, Teemu},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {144--147},

  Abstract                 = {An experimental study comparing different user interfaces for a virtual drum is reported. Virtual here means that the drum is not a physical object. 16 subjects played the drum on five different interfaces and two metronome patterns trying to match their hits to the metronome clicks. Temporal accuracy of the playing was evaluated. The subjects also rated the interfaces subjectively. The results show that hitting the drum alternately from both sides with motion going through the drum plate was less accurate than the traditional one sided hitting. A physical stick was more accurate than a virtual computer graphic stick. Visual feedback of the drum slightly increased accuracy compared to receiving only auditory feedback. Most subjects evaluated the physical stick to offer a better feeling and to be more pleasant than the virtual stick. },
  Keywords                 = {Virtual drum, user interface, feedback, musical instrument design, virtual reality, sound control, percussion instrument. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_144.pdf}
}

@InProceedings{Marinelli2005,
  Title                    = {Mocean},
  Author                   = {Marinelli, Maia and Lamenzo, Jared and Borissov, Liubo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {272--272},

  Abstract                 = {Mocean is an immersive environment that creates sensoryrelationships between natural media, particularly exploringthe potential of water as an emotive interface.},
  Keywords                 = {New interface, water, pipe organ, natural media, PIC microcontroller, wind instrument, human computer interface. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_272.pdf}
}

@InProceedings{Matsumura2005,
  Title                    = {Hop Step Junk: Sonic Visualization using Footsteps},
  Author                   = {Matsumura, Seiichiro and Arakawa, Chuichi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {273--273},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_273.pdf}
}

@InProceedings{Melo2005,
  Title                    = {Swayway - Midi Chimes},
  Author                   = {Melo, Mauricio and Fan, Doria},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {262--263},

  Abstract                 = {The Swayway is an audio/MIDI device inspired by the simpleconcept of the wind chime.This interactive sculpture translates its swaying motion,triggered by the user, into sound and light. Additionally, themotion of the reeds contributes to the visual aspect of thepiece, converting the whole into a sensory and engagingexperience.},
  Keywords                 = {Interactive sound sculpture, flex sensors, midi chimes, LEDs, sound installation. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_262.pdf}
}

@InProceedings{Miranda2005,
  Title                    = {Toward Direct Brain-Computer Musical Interfaces},
  Author                   = {Miranda, Eduardo and Brouse, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {216--219},

  Abstract                 = { Musicians and composers have been using brainwaves as generative sources in music for at least 40 years and the possibility of a brain-computer interface for direct communication and control was first seriously investigated in the early 1970s. Work has been done by many artists and technologists in the intervening years to attempt to control music systems with brainwaves and - indeed - many other biological signals. Despite the richness of EEG, fMRI and other data which can be read from the human brain, there has up to now been only limited success in translating the complex encephalographic data into satisfactory musical results. We are currently pursuing research which we believe will lead to the possibility of direct brain-computer interfaces for rich and expressive musical control. This report will outline the directions of our current research and results. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_216.pdf}
}

@InProceedings{Oore2005,
  Title                    = {Learning Advanced Skills on New Instruments},
  Author                   = {Oore, Sageev},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {60--64},

  Abstract                 = {When learning a classical instrument, people often eithertake lessons in which an existing body of "technique" is delivered, evolved over generations of performers, or in somecases people will "teach themselves" by watching people playand listening to existing recordings. What does one do witha complex new digital instrument?In this paper I address this question drawing on my experience in learning several very different types of sophisticatedinstruments: the Glove Talk II real-time gesture-to-speechinterface, the Digital Marionette controller for virtual 3Dpuppets, and pianos and keyboards. As the primary userof the first two systems, I have spent hundreds of hourswith Digital Marionette and Glove-Talk II, and thousandsof hours with pianos and keyboards (I continue to work asa professional musician). I will identify some of the underlying principles and approaches that I have observed duringmy learning and playing experience common to these instruments. While typical accounts of users learning new interfaces generally focus on reporting beginner's experiences, forvarious practical reasons, this is fundamentally different byfocusing on the expert's learning experience.},
  Keywords                 = {performance, learning new instruments },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_060.pdf}
}

@InProceedings{Overholt2005,
  Title                    = {The Overtone Violin},
  Author                   = {Overholt, Dan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {34--37},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_034.pdf}
}

@InProceedings{Pellarin2005,
  Title                    = {Connecting Strangers at a Train Station},
  Author                   = {Pellarin, Lars and B\"{o}ttcher, Niels and Olsen, Jakob M. and Gregersen, Ole and Serafin, Stefania and Guglielmi, Michel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {152--155},

  Keywords                 = {Motion tracking, mapping strategies, public installation, multiple participants music interfaces. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_152.pdf}
}

@InProceedings{Pelletier2005,
  Title                    = {A Graphical Interface for Real-Time Signal Routing},
  Author                   = {Pelletier, Jean-Marc},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {89--92},

  Abstract                 = {This paper describes DspMap, a graphical user interface (GUI)designed to assist the dynamic routing of signal generators andmodifiers currently being developed at the International Academyof Media Arts & Sciences. Instead of relying on traditional boxand-line approaches, DspMap proposes a design paradigm whereconnections are determined by the relative positions of the variouselements in a single virtual space.},
  Keywords                 = {Graphical user interface, real-time performance, map, dynamic routing },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_089.pdf}
}

@InProceedings{Poepel2005,
  Title                    = {On Interface Expressivity: A Player-Based Study},
  Author                   = {Poepel, Cornelius},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {228--231},

  Abstract                 = {While many new interfaces for musical expression have been presented in the past, methods to evaluate these interfaces are rare.This paper presents a method and a study comparing the potentialfor musical expression of different string-instrument based musicalinterfaces. Cues for musical expression are defined based on results of research in musical expression and on methods for musicaleducation in instrumental pedagogy. Interfaces are evaluated according to how well they are estimated to allow players making useof their existing technique for the creation of expressive music.},
  Keywords                 = {Musical Expression, electronic bowed string instrument, evaluation of musical input devices, audio signal driven sound synthesis },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_228.pdf}
}

@InProceedings{Rodet2005,
  Title                    = {Study of haptic and visual interaction for sound and music control in the Phase project},
  Author                   = {Rodet, Xavier and Lambert, Jean-Philippe and Cahen, Roland and Gaudy, Thomas and Guedy, Fabrice and Gosselin, Florian and Mobuchon, Pascal},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {109--114},

  Keywords                 = {Haptic, interaction, sound, music, control, installation. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_109.pdf}
}

@InProceedings{Rodriguez2005,
  Title                    = {VIFE \_ alpha v.01 Real-time Visual Sound Installation performed by Glove-Gesture},
  Author                   = {Rodr\'{\i}guez, David and Rodr\'{\i}guez, Iv\'{a}n},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {252--253},

  Abstract                 = {We present a Virtual Interface to Feel Emotions called VIFE _alpha v.01 (Virtual Interface to Feel Emotions). The work investigates the idea of Synaesthesia and her enormous possibilities creating new realities, sensations and zones where the user can find new points of interaction. This interface allows the user to create sonorous and visual compositions in real time. 6 three-dimensional sonorous forms are modified according to the movements of the user. These forms represent sonorous objects that respond to this by means of sensorial stimuli. Multiple combinations of colors and sound effects superpose to an a the others to give rise to a unique experience. },
  Keywords                 = {Synaesthesia, 3D render, new reality, virtual interface, creative interaction, sensors. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_252.pdf}
}

@InProceedings{Rodrigues2005,
  Title                    = {CyberSong},
  Author                   = {Rodrigues, Paulo Maria and Gir\~{a}o, Luis Miguel and Gehlhaar, Rolf},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {164--167},

  Abstract                 = {We present our work in the development of an interface for an actor/singer and its use in performing. Our work combines aspects of theatrical music with technology. Our interface has allowed the development of a new vocabulary for musical and theatrical expression and the possibility for merging classical and experimental music. It gave rise to a strong, strange, unpredictable, yet coherent, "character" and opens up the possibility for a full performance that will explore aspects of voice, theatrical music and, in the future, image projection. },
  Keywords                 = {Theatrical music, computer interaction, voice, gestural control. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_164.pdf}
}

@InProceedings{Scavone2005,
  Title                    = {Frequency Content of Breath Pressure and Implications for Use in Control},
  Author                   = {Scavone, Gary and Silva, Andrey R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {93--96},

  Abstract                 = {The breath pressure signal applied to wind music instruments is generally considered to be a slowly varying function of time. In a context of music control, this assumptionimplies that a relatively low digital sample rate (100-200Hz) is sufficient to capture and/or reproduce this signal.We tested this assumption by evaluating the frequency content in breath pressure, particularly during the use of extended performance techniques such as growling, humming,and flutter tonguing. Our results indicate frequency contentin a breath pressure signal up to about 10 kHz, with especially significant energy within the first 1000 Hz. We furtherinvestigated the frequency response of several commerciallyavailable pressure sensors to assess their responsiveness tohigher frequency breath signals. Though results were mixed,some devices were found capable of sensing frequencies upto at least 1.5 kHz. Finally, similar measurements were conducted with Yamaha WX11 and WX5 wind controllers andresults suggest that their breath pressure outputs are sampled at about 320 Hz and 280 Hz, respectively.},
  Keywords                 = {Breath Control, Wind Controller, Breath Sensors },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_093.pdf}
}

@InProceedings{Schiemer2005,
  Title                    = {Pocket Gamelan: a Pure Data interface for mobile phones},
  Author                   = {Schiemer, Greg and Havryliv, Mark},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {156--159},

  Abstract                 = {This paper describes software tools used to create java applications for performing music using mobile phones. The tools provide a means for composers working in the Pure Data composition environment to design and audition performances using ensembles of mobile phones. These tools were developed as part of a larger project motivated by the desire to allow large groups of non-expert players to perform music based on just intonation using ubiquitous technology. The paper discusses the process that replicates a Pure Data patch so that it will operate within the hardware and software constraints of the Java 2 Micro Edition. It also describes development of objects that will enable mobile phone performances to be simulated accurately in PD and to audition microtonal tuning implemented using MIDI in the j2me environment. These tools eliminate the need for composers to compose for mobile phones by writing java code. In a single desktop application, they offer the composer the flexibility to write music for multiple phones. },
  Keywords                 = {Java 2 Micro Edition; j2me; Pure Data; PD; Real-Time Media Performance; Just Intonation. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_156.pdf}
}

@InProceedings{Silva2005,
  Title                    = {On the Use of Flute Air Jet as A Musical Control Variable},
  Author                   = {Silva, Andrey R. and Wanderley, Marcelo M. and Scavone, Gary},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {105--108},

  Abstract                 = {This paper aims to present some perspectives on mappingembouchure gestures of flute players and their use as controlvariables. For this purpose, we have analyzed several typesof sensors, in terms of sensitivity, dimension, accuracy andprice, which can be used to implement a system capable ofmapping embouchure parameters such as air jet velocity andair jet direction. Finally, we describe the implementationof a sensor system used to map embouchure gestures of aclassical Boehm flute.},
  Keywords                 = {Embouchure, air pressure sensors, hot wires, mapping, aug- mented flute. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_105.pdf}
}

@InProceedings{Singer2005,
  Title                    = {A Large-Scale Networked Robotic Musical Instrument Installation},
  Author                   = {Singer, Eric and Feddersen, Jeff and Bowen, Bil},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {50--55},

  Abstract                 = {This paper describes an installation created by LEMUR(League of Electronic Musical Urban Robots) in January, 2005.The installation included over 30 robotic musical instrumentsand a multi-projector real-time video projection and wascontrollable and programmable over a MIDI network. Theinstallation was also controllable remotely via the Internet andcould be heard and viewed via room mics and a robotic webcam connected to a streaming server.},
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_050.pdf}
}

@InProceedings{Sinyor2005,
  Title                    = {Gyrotyre : A dynamic hand-held computer-music controller based on a spinning wheel},
  Author                   = {Sinyor, Elliot and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {42--45},

  Abstract                 = {This paper presents a novel controller built to exploit thephysical behaviour of a simple dynamical system, namely aspinning wheel. The phenomenon of gyroscopic precessioncauses the instrument to slowly oscillate when it is spunquickly, providing the performer with proprioceptive feedback. Also, due to the mass of the wheel and tire and theresulting rotational inertia, it maintains a relatively constant angular velocity once it is set in motion. Various sensors were used to measure continuous and discrete quantitiessuch as the the angular frequency of the wheel, its spatialorientation, and the performer's finger pressure. In addition, optical and hall-effect sensors detect the passing of aspoke-mounted photodiode and two magnets. A base software layer was developed in Max/MSP and various patcheswere written with the goal of mapping the dynamic behaviorof the wheel to varied musical processes.},
  Keywords                 = {HCI, Digital Musical Instruments, Gyroscopic Precession, Rotational Inertia, Open Sound Control },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_042.pdf}
}

@InProceedings{Socolofsky2005,
  Title                    = {Contemplace},
  Author                   = {Socolofsky, Eric},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {271--271},

  Abstract                 = {Contemplace is a spatial personality that redesigns itselfdynamically according to its conversations with its visitors.Sometimes welcoming, sometimes shy, and sometimeshostile, Contemplace's mood is apparent through a display ofprojected graphics, spatial sound, and physical motion.Contemplace is an environment in which inhabitationbecomes a two-way dialogue.},
  Keywords                 = {Interactive space, spatial installation, graphic and aural display, motion tracking, Processing, Flosc },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_271.pdf}
}

@InProceedings{Steiner2005,
  Title                    = {[hid] toolkit: a Unified Framework for Instrument Design},
  Author                   = {Steiner, Hans-christoph},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {140--143},

  Abstract                 = {The [hid] toolkit is a set of software objects for designingcomputer-based gestural instruments. All too frequently,computer-based performers are tied to the keyboard-mousemonitor model, narrowly constraining the range of possiblegestures. A multitude of gestural input devices are readilyavailable, making it easy to utilize a broader range of gestures. Human Interface Devices (HIDs) such as joysticks,tablets, and gamepads are cheap and can be good musicalcontrollers. Some even provide haptic feedback. The [hid]toolkit provides a unified, consistent framework for gettinggestural data from these devices, controlling the feedback,and mapping this data to the desired output. The [hid]toolkit is built in Pd, which provides an ideal platform forthis work, combining the ability to synthesize and controlaudio and video. The addition of easy access to gesturaldata allows for rapid prototypes. A usable environmentalso makes computer music instrument design accessible tonovices.},
  Keywords                 = {Instrument design, haptic feedback, gestural control, HID },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_140.pdf}
}

@InProceedings{Taylor2005,
  Title                    = {Using Music to Interact with a Virtual Character},
  Author                   = {Taylor, Robyn and Torres, Daniel and Boulanger, Pierre},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {220--223},

  Abstract                 = {We present a real-time system which allows musicians tointeract with synthetic virtual characters as they perform.Using Max/MSP to parameterize keyboard and vocal input, meaningful features (pitch, amplitude, chord information, and vocal timbre) are extracted from live performancein real-time. These extracted musical features are thenmapped to character behaviour in such a way that the musician's performance elicits a response from the virtual character. The system uses the ANIMUS framework to generatebelievable character expressions. Experimental results arepresented for simple characters.},
  Keywords                 = {Music, synthetic characters, advanced man-machine inter- faces, virtual reality, behavioural systems, interaction tech- niques, visualization, immersive entertainment, artistic in- stallations },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_220.pdf}
}

@InProceedings{Tindale2005,
  Title                    = {A Comparison of Sensor Strategies for Capturing Percussive Gestures},
  Author                   = {Tindale, Adam R. and Kapur, Ajay and Tzanetakis, George and Driessen, Peter and Schloss, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {200--203},

  Abstract                 = {Drum controllers designed by researchers and commercialcompanies use a variety of techniques for capturing percussive gestures. It is challenging to obtain both quick responsetimes and low-level data (such as position) that contain expressive information. This research is a comprehensive studyof current methods to evaluate the available strategies andtechnologies. This study aims to demonstrate the benefitsand detriments of the current state of percussion controllersas well as yield tools for those who would wish to conductthis type of study in the future.},
  Keywords                 = {Percussion Controllers, Timbre-recognition based instruments, Electronic Percussion, Sensors for Interface Design },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_200.pdf}
}

@InProceedings{Topper2005,
  Title                    = {Wireless Dance Control : PAIR and WISEAR},
  Author                   = {Topper, David and Swendsen, Peter V.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {76--79},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_076.pdf}
}

@InProceedings{Verplank2005,
  Title                    = {Haptic Music Exercises},
  Author                   = {Verplank, William},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {256--257},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_256.pdf}
}

@InProceedings{Villar2005,
  Title                    = {Pin \& Play \& Perform: A rearrangeable interface for musical composition and performance},
  Author                   = {Villar, Nicolas and Lindsay, Adam T. and Gellersen, Hans},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {188--191},

  Abstract                 = {We present the Pin&Play&Perform system: an interface inthe form of a tablet on which a number of physical controlscan be added, removed and arranged on the fly. These controls can easily be mapped to existing music sofware usingthe MIDI protocol. The interface provides a mechanism fordirect manipulation of application parameters and eventsthrough a set of familiar controls, while also encouraging ahigh degree of customisation through the ability to arrange,rearrange and annotate the spatial layout of the interfacecomponents on the surface of the tablet.The paper describes how we have realized this concept using the Pin&Play technology. As an application example, wedescribe our experiences in using our interface in conjunction with Propellerheads' Reason, a popular piece of musicsynthesis software.},
  Keywords                 = {tangible interface, rearrangeable interface, midi controllers },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_188.pdf}
}

@InProceedings{Wang2005,
  Title                    = {Bubbaboard and Mommaspeaker: Creating Digital Tonal Sounds from an Acoustic Percussive Instrument},
  Author                   = {Wang, Derek},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {264--265},

  Abstract                 = {This paper describes the transformation of an everyday object into a digital musical instrument. By tracking hand movements and tilt on one of two axes, the Bubbaboard, a transformed handheld washboard, allows a user to play scales at different octaves while simultaneously offering the ability to use its inherent acoustic percussive qualities. Processed sound is fed to the Mommaspeaker, which creates physically generated vibrato at a speed determined by tilting the Bubbaboard on its second axis. },
  Keywords                 = {Gesture based controllers, Musical Performance, MIDI, Accelerometer, Microcontroller, Contact Microphone },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_264.pdf}
}

@InProceedings{Wang2005a,
  Title                    = {Yeah, ChucK It! = > Dynamic , Controllable Interface Mapping},
  Author                   = {Wang, Ge and Cook, Perry R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {196--199},

  Abstract                 = {ChucK is a programming language for real-time sound synthesis. It provides generalized audio abstractions and precise control over timing and concurrency - combining the rapid-prototyping advantages of high-level programming tools, such as Pure Data, with the flexibility and controllability of lower-level, text-based languages like C/C++. In this paper, we present a new time-based paradigm for programming controllers with ChucK. In addition to real-time control over sound synthesis, we show how features such as dynamic patching, on-the-fly controller mapping, multiple control rates, and precisely-timed recording and playback of sensors can be employed under the ChucK programming model. Using this framework, composers, programmers, and performers can quickly write (and read/debug) complex controller/synthesis programs, and experiment with controller mapping on-the-fly. },
  Keywords                 = {Controller mapping, programming language, on-the-fly programming, real-time interaction, concurrency. },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_196.pdf}
}

@InProceedings{Weinberg2005,
  Title                    = {iltur -- Connecting Novices and Experts Through Collaborative Improvisation},
  Author                   = {Weinberg, Gil and Driscoll, Scott},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {17--22},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_017.pdf}
}

@InProceedings{Wingstedt2005,
  Title                    = {REMUPP -- An Interactive Tool for Investigating Musical Properties and Relations},
  Author                   = {Wingstedt, Johnny and Liljedahl, Mats and Lindberg, Stefan and Berg, Jan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {232--235},

  Url                      = {http://www.nime.org/proceedings/2005/nime2005_232.pdf}
}

@InProceedings{Yonezawa2005,
  Title                    = {HandySinger : Expressive Singing Voice Morphing using Personified Hand-puppet Interface},
  Author                   = {Yonezawa, Tomoko and Suzuki, Takahiko and Mase, Kenji and Kogure, Kiyoshi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2005},

  Address                  = {Vancouver, BC, Canada},
  Pages                    = {121--126},

  Abstract                 = {The HandySinger system is a personified tool developedto naturally express a singing voice controlled by the gestures of a hand puppet. Assuming that a singing voice is akind of musical expression, natural expressions of the singingvoice are important for personification. We adopt a singingvoice morphing algorithm that effectively smoothes out thestrength of expressions delivered with a singing voice. Thesystem's hand puppet consists of a glove with seven bendsensors and two pressure sensors. It sensitively capturesthe user's motion as a personified puppet's gesture. Tosynthesize the different expressional strengths of a singingvoice, the ``normal'' (without expression) voice of a particular singer is used as the base of morphing, and three different expressions, ``dark,'' ``whisper'' and ``wet,'' are used asthe target. This configuration provides musically expressedcontrols that are intuitive to users. In the experiment, weevaluate whether 1) the morphing algorithm interpolatesexpressional strength in a perceptual sense, 2) the handpuppet interface provides gesture data at sufficient resolution, and 3) the gestural mapping of the current systemworks as planned.},
  Keywords                 = {Personified Expression, Singing Voice Morphing, Voice Ex- pressivity, Hand-puppet Interface },
  Url                      = {http://www.nime.org/proceedings/2005/nime2005_121.pdf}
}

