@InProceedings{dAlessandro2007,
  Title                    = {HandSketch Bi-Manual Controller Investigation on Expressive Control Issues of an Augmented Tablet},
  Author                   = {d'Alessandro, Nicolas and Dutoit, Thierry},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {78--81},

  Abstract                 = {In this paper, we present a new bi-manual gestural controller, called HandSketch, composed of purchasable devices :pen tablet and pressure-sensing surfaces. It aims at achieving real-time manipulation of several continuous and articulated aspects of pitched sounds synthesis, with a focus onexpressive voice. Both prefered and non-prefered hand issues are discussed. Concrete playing diagrams and mappingstrategies are described. These results are integrated and acompact controller is proposed.},
  Keywords                 = {Pen tablet, FSR, bi-manual gestural control. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_078.pdf}
}

@InProceedings{Stanza2007,
  Title                    = {Sensity},
  Author                   = {, Stanza},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {427--427},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_427.pdf}
}

@InProceedings{Historia2007,
  Title                    = {Soundanism},
  Author                   = {\'{A}lvarez-Fern\'{a}ndez, Miguel and Kersten, Stefan and Piascik, Asia},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {432--432},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_432.pdf}
}

@InProceedings{Aimi2007,
  Title                    = {Percussion Instruments Using Realtime Convolution : Physical Controllers},
  Author                   = {Aimi, Roberto},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {154--159},

  Abstract                 = {This paper describes several example hybrid acoustic / electronic percussion instruments using realtime convolution toaugment and modify the apparent acoustics of damped physical objects. Examples of cymbal, frame drum, practice pad,brush, and bass drum controllers are described.},
  Keywords                 = {Musical controllers, extended acoustic instruments },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_154.pdf}
}

@InProceedings{Baalman2007,
  Title                    = {Schwelle : Sensor Augmented, Adaptive Sound Design for Live Theatrical Performance},
  Author                   = {Baalman, Marije A. and Moody-Grigsby, Daniel and Salter, Christopher L.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {178--184},

  Abstract                 = {This paper describes work on a newly created large-scaleinteractive theater performance entitled Schwelle (Thresholds). The ,
,
authors discuss an innovative approach towardsthe conception, development and implementation of dynamicand responsive audio scenography : a constantly evolving,multi-layered sound design generated by continuous inputfrom a series of distributed wireless sensors deployed bothon the body of a performer and placed within the physicalstage environment. The paper is divided into conceptualand technological parts. We first describe the project's dramaturgical and conceptual context in order to situate theartistic framework that has guided the technological systemdesign. Specifically, this framework discusses the team's approach in combining techniques from situated computing,theatrical sound design practice and dynamical systems inorder to create a new kind of adaptive audio scenographicenvironment augmented by wireless, distributed sensing foruse in live theatrical performance. The goal of this adaptive sound design is to move beyond both existing playbackmodels used in theatre sound as well as the purely humancentered, controller-instrument approach used in much current interactive performance practice.},
  Keywords                 = {Interactive performance, dynamical systems, wireless sens- ing, adaptive audio scenography, audio dramaturgy, situated computing, sound design },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_178.pdf}
}

@InProceedings{Baba2007,
  Title                    = {Freqtric Drums : A Musical Instrument that Uses Skin Contact as an Interface},
  Author                   = {Baba, Tetsuaki and Ushiama, Taketoshi and Tomimatsu, Kiyoshi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {386--387},

  Abstract                 = {Freqtric Drums is a new musical, corporal electronic instrument that allows us not only to recover face-to-face communication, but also makes possible body-to-body communication so that a self image based on the sense of being a separate body can be signicant altered through an openness toand even a sense of becoming part of another body. FreqtricDrums is a device that turns audiences surrounding a performer into drums so that the performer, as a drummer, cancommunicate with audience members as if they were a setof drums. We describe our concept and the implementationand process of evolution of Freqtric Drums.},
  Keywords                 = {interpersonal communication, musical instrument, interac- tion design, skin contact, touch },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_386.pdf}
}

@InProceedings{Bell2007,
  Title                    = {The Multimodal Music Stand},
  Author                   = {Bell, Bo and Kleban, Jim and Overholt, Dan and Putnam, Lance and Thompson, John and Morin-Kuchera, JoAnn},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {62--65},

  Abstract                 = {We present the Multimodal Music Stand (MMMS) for the untethered sensing of performance gestures and the interactive control of music. Using e-field sensing, audio analysis, and computer vision, the MMMS captures a performer's continuous expressive gestures and robustly identifies discrete cues in a musical performance. Continuous and discrete gestures are sent to an interactive music system featuring custom designed software that performs real-time spectral transformation of audio. },
  Keywords                 = {Multimodal, interactivity, computer vision, e-field sensing, untethered control. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_062.pdf}
}

@InProceedings{Bennett2007,
  Title                    = {DAMPER : A Platform for Effortful Interface Development},
  Author                   = {Bennett, Peter and Ward, Nicholas and O'Modhrain, Sile and Rebelo, Pedro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {273--276},

  Abstract                 = {This paper proposes that the physicality of an instrument be considered an important aspect in the design of new interfaces for musical expression. The use of Laban's theory of effort in the design of new effortful interfaces, in particular looking at effortspace modulation, is investigated, and a platform for effortful interface development (named the DAMPER) is described. Finally, future work is described and further areas of research are highlighted. },
  Keywords                 = {Effortful Interaction. Haptics. Laban Analysis. Physicality. HCI. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_273.pdf}
}

@InProceedings{Benning2007,
  Title                    = {Improved Position Tracking of a {3-D} Gesture-Based Musical Controller Using a {Kalman} Filter},
  Author                   = {Benning, Manjinder S. and McGuire, Michael and Driessen, Peter},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {334--337},

  Abstract                 = {This paper describes the design and experimentation of a Kalman Filter used to improve position tracking of a 3-D gesture-based musical controller known as the Radiodrum. The Singer dynamic model for target tracking is used to describe the evolution of a Radiodrum's stick position in time. The autocorrelation time constant of a gesture's acceleration and the variance of the gesture acceleration are used to tune the model to various performance modes. Multiple Kalman Filters tuned to each gesture type are run in parallel and an Interacting Multiple Model (IMM) is implemented to decide on the best combination of filter outputs to track the current gesture. Our goal is to accurately track Radiodrum gestures through noisy measurement signals. },
  Keywords                 = {Kalman Filtering, Radiodrum, Gesture Tracking, Interacting Multiple Model INTRODUCTION Intention is a key aspect of traditional music performance. The ability for an artist to reliably reproduce sound, pitch, rhythms, and emotion is paramount to the design of any instrument. With the },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_334.pdf}
}

@InProceedings{Bevilacqua2007,
  Title                    = {Wireless Sensor Interface and Gesture-Follower for Music Pedagogy},
  Author                   = {Bevilacqua, Fr\'{e}d\'{e}ric and Gu\'{e}dy, Fabrice and Schnell, Norbert and Fl\'{e}ty, Emmanuel and Leroy, Nicolas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {124--129},

  Abstract                 = {We present in this paper a complete gestural interface built to support music pedagogy. The development of this prototype concerned both hardware and software components: a small wireless sensor interface including accelerometers and gyroscopes, and an analysis system enabling gesture following and recognition. A first set of experiments was conducted with teenagers in a music theory class. The preliminary results were encouraging concerning the suitability of these developments in music education. },
  Keywords                 = {Technology-enhanced learning, music pedagogy, wireless interface, gesture-follower, gesture recognition },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_124.pdf}
}

@InProceedings{Biggs2007,
  Title                    = {The Tipping Point},
  Author                   = {Biggs, Betsey},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {424--424},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_424.pdf}
}

@InProceedings{B2007,
  Title                    = {{PHY}SMISM : A Control Interface for Creative Exploration of Physical Models},
  Author                   = {Bottcher, Niels and Gelineck, Steven and Serafin, Stefania},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {31--36},

  Abstract                 = {In this paper we describe the design and implementation ofthe PHYSMISM: an interface for exploring the possibilitiesfor improving the creative use of physical modelling soundsynthesis.The PHYSMISM is implemented in a software and hardware version. Moreover, four different physical modellingtechniques are implemented, to explore the implications ofusing and combining different techniques.In order to evaluate the creative use of physical models,a test was performed using 11 experienced musicians as testsubjects. Results show that the capability of combining thephysical models and the use of a physical interface engagedthe musicians in creative exploration of physical models.},
  Keywords                 = {Physical models, hybrid instruments, excitation, resonator. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_031.pdf}
}

@InProceedings{Bottoni2007,
  Title                    = {Use of a Dual-Core {DSP} in a Low-Cost, Touch-Screen Based Musical Instrument},
  Author                   = {Bottoni, Paolo and Caporali, Riccardo and Capuano, Daniele and Faralli, Stefano and Labella, Anna and Pierro, Mario},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {394--395},

  Abstract                 = {This paper reports our experiments on using a dual-coreDSP processor in the construction of a user-programmablemusical instrument and controller called the TouchBox.},
  Keywords                 = {dual-core, DSP, touch-screen, synthesizer, controller },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_394.pdf}
}

@InProceedings{Bouillot2007,
  Title                    = {nJam User Experiments : Enabling Remote Musical Interaction from Milliseconds to Seconds},
  Author                   = {Bouillot, Nicolas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {142--147},

  Abstract                 = {Remote real-time musical interaction is a domain where endto-end latency is a well known problem. Today, the mainexplored approach aims to keep it below the musicians perception threshold. In this paper, we explore another approach, where end-to-end delays rise to several seconds, butcomputed in a controlled (and synchronized) way dependingon the structure of the musical pieces. Thanks to our fullydistributed prototype called nJam, we perform user experiments to show how this new kind of interactivity breaks theactual end-to-end latency bounds.},
  Keywords                 = {Remote real-time musical interaction, end-to-end delays, syn- chronization, user experiments, distributed metronome, NMP. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_142.pdf}
}

@InProceedings{Bull2007,
  Title                    = {Cellphonia : WET},
  Author                   = {Bull, Steve and Gresham-Lancaster, Scot and Mintchev, Kalin and Svoboda, Terese},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {420--420},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_420.pdf}
}

@InProceedings{Camurri2007,
  Title                    = {Active Listening to a Virtual Orchestra Through an Expressive Gestural Interface : The Orchestra Explorer},
  Author                   = {Camurri, Antonio and Canepa, Corrado and Volpe, Gualtiero},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {56--61},

  Abstract                 = {In this paper, we present a new system, the Orchestra Explorer, enabling a novel paradigm for active fruition of sound and music content. The Orchestra Explorer allows users to physically navigate inside a virtual orchestra, to actively explore the music piece the orchestra is playing, to modify and mold the sound and music content in real-time through their expressive full-body movement and gesture. An implementation of the Orchestra Explorer was developed and presented in the framework of the science exhibition "Cimenti di Invenzione e Armonia", held at Casa Paganini, Genova, from October 2006 to January 2007. },
  Keywords                 = {Active listening of music, expressive interfaces, full-body motion analysis and expressive gesture processing, multimodal interactive systems for music and performing arts applications. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_056.pdf}
}

@InProceedings{Camurri2007a,
  Title                    = {Developing Multimodal Interactive Systems with EyesWeb XMI},
  Author                   = {Camurri, Antonio and Coletta, Paolo and Varni, Giovanna and Ghisio, Simone},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {305--308},

  Abstract                 = {EyesWeb XMI (for eXtended Multimodal Interaction) is the new version of the well-known EyesWeb platform. It has a main focus on multimodality and the main design target of this new release has been to improve the ability to process and correlate several streams of data. It has been used extensively to build a set of interactive systems for performing arts applications for Festival della Scienza 2006, Genoa, Italy. The purpose of this paper is to describe the developed installations as well as the new EyesWeb features that helped in their development.},
  Keywords                 = {EyesWeb, multimodal interactive systems, performing arts. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_305.pdf}
}

@InProceedings{Cartwright2007,
  Title                    = {Rage in Conjunction with the Machine},
  Author                   = {Cartwright, Mark and Jones, Matt and Terasawa, Hiroko},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {224--227},

  Abstract                 = {This report presents the design and construct ion of Rage in Conjunction with the Machine, a simple but novel pairing of musical interface and sound sculpture. The ,
,
authors discuss the design and creation of this instrument , focusing on the unique aspects of it, including the use of physical systems, large gestural input, scale, and the electronic coupling of a physical input to a physical output.},
  Keywords                 = {audience participation,inflatable,instrume nt design,instrume nt size,mapping,musical,new musical instrument,nime07,physical systems,sound scultpure},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_224.pdf}
}

@InProceedings{Castellano2007,
  Title                    = {Expressive Control of Music and Visual Media by Full-Body Movement},
  Author                   = {Castellano, Ginevra and Bresin, Roberto and Camurri, Antonio and Volpe, Gualtiero},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {390--391},

  Abstract                 = {In this paper we describe a system which allows users to use their full-body for controlling in real-time the generation of an expressive audio-visual feedback. The system extracts expressive motion features from the user's full-body movements and gestures. The values of these motion features are mapped both onto acoustic parameters for the real-time expressive rendering of a piece of music, and onto real-time generated visual feedback projected on a screen in front of the user. },
  Keywords                 = {Expressive interaction; multimodal environments; interactive music systems },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_390.pdf}
}

@InProceedings{Chang2007,
  Title                    = {Zstretch : A Stretchy Fabric Music Controller},
  Author                   = {Chang, Angela and Ishii, Hiroshi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {46--49},

  Abstract                 = {FigureWe present Zstretch, a textile music controller that supports expressive haptic interactions. The musical controller takes advantage of the fabric's topological constraints to enable proportional control of musical parameters. This novel interface explores ways in which one might treat music as a sheet of cloth. This paper proposes an approach to engage simple technologies for supporting ordinary hand interactions. We show that this combination of basic technology with general tactile movements can result in an expressive musical interface. a},
  Keywords                 = {Tangible interfaces, textiles, tactile design, musical expressivity },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_046.pdf}
}

@InProceedings{Chuchacz2007,
  Title                    = {Physical Models and Musical Controllers -- Designing a Novel Electronic Percussion Instrument},
  Author                   = {Chuchacz, Katarzyna and O'Modhrain, Sile and Woods, Roger},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {37--40},

  Abstract                 = {A novel electronic percussion synthesizer prototype is presented. Our ambition is to design an instrument that will produce a high quality, realistic sound based on a physical modelling sound synthesis algorithm. This is achieved using a real-time Field Programmable Gate Array (FPGA) implementation of the model coupled to an interface that aims to make efficient use of all the subtle nuanced gestures of the instrumentalist. It is based on a complex physical model of the vibrating plate - the source of sound in the majority of percussion instruments. A Xilinx Virtex II pro FPGA core handles the sound synthesis computations with an 8 billion operations per second performance and has been designed in such a way to allow a high level of control and flexibility. Strategies are also presented to that allow the parametric space of the model to be mapped to the playing gestures of the percussionist. },
  Keywords                 = {Physical Model, Electronic Percussion Instrument, FPGA. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_037.pdf}
}

@InProceedings{Clay2007,
  Title                    = {The Wrist-Conductor},
  Author                   = {Clay, Arthur and Majoe, Dennis},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {242--245},

  Abstract                 = {The starting point for this project is the want to produce amusic controller that could be employed in such a manner thateven lay public could enjoy the possibilities of mobile art. Allof the works that are discussed here are in relation to a newGPS-based controller, the Wrist-Conductor. The works aretechnically based around the synchronizing possibilitiesusing the GPS Time Mark and are aesthetically rooted in worksthat function in an open public space such as a city or a forest.One of the works intended for the controller, China Gates, i sdiscussed here in detail in order to describe how the GPSWrist-Controller is actually used in a public art context. Theother works, CitySonics, The Enchanted Forest and Get a Pot& a Spoon are described briefly in order to demonstrate thateven a simple controller can be used to create a body of works.This paper also addresses the breaking of the media bubblevia the concept of the "open audience", or how mobile art canengage pedestrians as viewers or listeners within public spaceand not remain an isolated experience for performers only.},
  Keywords                 = {Mobile Music, GPS, Controller, Collaborative Performance },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_242.pdf}
}

@InProceedings{Collins2007,
  Title                    = {Matching Parts : Inner Voice Led Control for Symbolic and Audio Accompaniment},
  Author                   = {Collins, Nick},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {220--223},

  Keywords                 = {accompaniment,concatenative sound syn-,feature matching,inner parts,interactive mu-,melodic similarity,nime07,thesis},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_220.pdf}
}

@InProceedings{Cook2007,
  Title                    = {Tactophonics : Your Favourite Thing Wants to Sing},
  Author                   = {Cook, Andrew A. and Pullin, Graham},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {285--288},

  Keywords                 = {1,affordance,background and problem space,cultural probes,design research,improvisation,interaction design,nime07,performance},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_285.pdf}
}

@InProceedings{Dahl2007,
  Title                    = {The WaveSaw : A Flexible Instrument for Direct Timbral Manipulation},
  Author                   = {Dahl, Luke and Whetsell, Nathan and Van Stoecker, John},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {270--272},

  Keywords                 = {Musical controller, Puredata, scanned synthesis, flex sensors. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_270.pdf}
}

@InProceedings{Dannenberg2007,
  Title                    = {New Interfaces for Popular Music Performance},
  Author                   = {Dannenberg, Roger B.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {130--135},

  Abstract                 = {Augmenting performances of live popular music with computer systems poses many new challenges. Here, "popular music" is taken to mean music with a mostly steady tempo, some improvisational elements, and largely predetermined melodies, harmonies, and other parts. The overall problem is studied by developing a framework consisting of constraints and subproblems that any solution should address. These problems include beat acquisition, beat phase, score location, sound synthesis, data preparation, and adaptation. A prototype system is described that offers a set of solutions to the problems posed by the framework, and future work is suggested. },
  Keywords                 = {accompaniment,beat,conducting,intelligent,music synchronization,nime07,synthetic performer,tracking,virtual orchestra},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_130.pdf}
}

@InProceedings{Court2007,
  Title                    = {{Miller}},
  Author                   = {Collective Dearraindrop},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {421--421},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_421.pdf}
}

@InProceedings{Donaldson2007,
  Title                    = {Chroma Palette : Chromatic Maps of Sound As Granular Synthesis Interface},
  Author                   = {Donaldson, Justin and Knopke, Ian and Raphael, Chris},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {213--219},

  Abstract                 = {Chroma based representations of acoustic phenomenon are representations of sound as pitched acoustic energy. A framewise chroma distribution over an entire musical piece is a useful and straightforward representation of its musical pitch over time. This paper examines a method of condensing the block-wise chroma information of a musical piece into a two dimensional embedding. Such an embedding is a representation or map of the different pitched energies in a song, and how these energies relate to each other in the context of the song. The paper presents an interactive version of this representation as an exploratory analytical tool or instrument for granular synthesis. Pointing and clicking on the interactive map recreates the acoustical energy present in the chroma blocks at that location, providing an effective way of both exploring the relationships between sounds in the original piece, and recreating a synthesized approximation of these sounds in an instrumental fashion. },
  Keywords                 = {Chroma, granular synthesis, dimensionality reduction },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_213.pdf}
}

@InProceedings{Fiebrink2007,
  Title                    = {Don't Forget the Laptop : Using Native Input Capabilities for Expressive Musical Control},
  Author                   = {Fiebrink, Rebecca and Wang, Ge and Cook, Perry R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {164--167},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_164.pdf}
}

@InProceedings{Fornari2007,
  Title                    = {Interactive Spatialization and Sound Design using an Evolutionary System},
  Author                   = {Fornari, Jose and Maia, Adolfo Jr. and Manzolli, Jonatas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {293--298},

  Abstract                 = {We present an interactive sound spatialization and synthesis system based on Interaural Time Difference (ITD) model and Evolutionary Computation. We define a Sonic Localization Field using sound attenuation and ITD azimuth angle parameters and, in order to control an adaptive algorithm, we used pairs of these parameters as Spatial Sound Genotypes (SSG). They are extracted from waveforms which are considered individuals of a Population Set. A user-interface receives input from a generic gesture interface (such as a NIME device) and interprets them as ITD cues. Trajectories provided by these signals are used as Target Sets of an evolutionary algorithm. A Fitness procedure optimizes locally the distance between the Target Set and the SSG pairs. Through a parametric score the user controls dynamic changes in the sound output. },
  Keywords                 = {interactive, sound, spatialization, evolutionary, adaptation. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_293.pdf}
}

@InProceedings{Francois2007,
  Title                    = {Visual Feedback in Performer-Machine Interaction for Musical Improvisation},
  Author                   = {Fran\c{c}ois, Alexandre R. and Chew, Elaine and Thurmond, Dennis},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {277--280},

  Abstract                 = {This paper describes the design of Mimi, a multi-modal interactive musical improvisation system that explores the potential and powerful impact of visual feedback in performermachine interaction. Mimi is a performer-centric tool designed for use in performance and teaching. Its key andnovel component is its visual interface, designed to providethe performer with instantaneous and continuous information on the state of the system. For human improvisation,in which context and planning are paramount, the relevantstate of the system extends to the near future and recentpast. Mimi's visual interface allows for a peculiar blendof raw reflex typically associated with improvisation, andpreparation and timing more closely affiliated with scorebased reading. Mimi is not only an effective improvisationpartner, it has also proven itself to be an invaluable platformthrough which to interrogate the mental models necessaryfor successful improvisation.},
  Keywords                 = {Performer-machine interaction, visualization design, machine improvisation },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_277.pdf}
}

@InProceedings{Franinovic2007,
  Title                    = {New Musical Interfaces in Context : Sonic Interaction Design in the Urban Setting},
  Author                   = {Franinovic, Karmen and Visell, Yon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {191--196},

  Abstract                 = {The distinctive features of interactive sound installations inpublic space are considered, with special attention to therich, if undoubtedly difficult, environments in which theyexist. It is argued that such environments, and the socialcontexts that they imply, are among the most valuable features of these works for the approach that we have adoptedto creation as research practice. The discussion is articulated through case studies drawn from two of our installations, Recycled Soundscapes (2004) and Skyhooks (2006).Implications for the broader design of new musical instruments are presented.},
  Keywords                 = {architecture,interaction,music,nime07,sound in-,urban design},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_191.pdf}
}

@InProceedings{Freeman2007,
  Title                    = {Graph Theory : Interfacing Audiences Into the Compositional Process},
  Author                   = {Freeman, Jason},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {260--263},

  Abstract                 = {Graph Theory links the creative music-making activities of web site visitors to the dynamic generation of an instrumental score for solo violin. Participants use a web-based interface to navigate among short, looping musical fragments to create their own unique path through the open-form composition. Before each concert performance, the violinist prints out a new copy of the score that orders the fragments based on the decisions made by web visitors. },
  Keywords                 = {Music, Composition, Residency, Audience Interaction, Collaboration, Violin, Graph, Flash, Internet, Traveling Salesman. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_260.pdf}
}

@InProceedings{Gomez2007,
  Title                    = {A Look at the Design and Creation of a Graphically Controlled Digital Musical Instrument},
  Author                   = {G\'{o}mez, Daniel and Donner, Tjebbe and Posada, Andr\'{e}s},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {327--329},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_327.pdf}
}

@InProceedings{Gimenes2007,
  Title                    = {Musicianship for Robots with Style},
  Author                   = {Gimenes, Marcelo and Miranda, Eduardo and Johnson, Chris},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {197--202},

  Abstract                 = {In this paper we introduce a System conceived to serve as the "musical brain" of autonomous musical robots or agent-based software simulations of robotic systems. Our research goal is to provide robots with the ability to integrate with the musical culture of their surroundings. In a multi-agent configuration, the System can simulate an environment in which autonomous agents interact with each other as well as with external agents (e.g., robots, human beings or other systems). The main outcome of these interactions is the transformation and development of their musical styles as well as the musical style of the environment in which they live. },
  Keywords                 = {artificial life,musical style,musicianship,nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_197.pdf}
}

@InProceedings{Groux2007,
  Title                    = {VR-RoBoser : Real-Time Adaptive Sonification of Virtual Environments Based on Avatar Behavior},
  Author                   = {le Groux, Sylvain and Manzolli, Jonatas and Verschure, Paul F.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {371--374},

  Abstract                 = {Until recently, the sonification of Virtual Environments had often been reduced to its simplest expression. Too often soundscapes and background music are predetermined, repetitive and somewhat predictable. Yet, there is room for more complex and interesting sonification schemes that can improve the sensation of presence in a Virtual Environment. In this paper we propose a system that automatically generates original background music in real-time called VR-RoBoser. As a test case we present the application of VR-RoBoser to a dynamic avatar that explores its environment. We show that the musical events are directly and continuously generated and influenced by the behavior of the avatar in three-dimensional virtual space, generating a context dependent sonification. },
  Keywords                 = {Real-time Composition, Interactive Sonification, Real-time Neural Processing, Multimedia, Virtual Environment, Avatar. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_371.pdf}
}

@InProceedings{Gruenbaum2007,
  Title                    = {The Samchillian Tip Tip Tip Cheeepeeeee : A Relativistic Keyboard Instrument},
  Author                   = {Gruenbaum, Leon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {256--259},

  Abstract                 = {Almost all traditional musical instruments have a one-to-one correspondence between a given fingering and the pitch that sounds for that fingering. The Samchillian Tip Tip Tip Cheeepeeeee does not - it is a keyboard MIDI controller that is based on intervals rather than fixed pitches. That is, a given keypress will sound a pitch a number of steps away from the last note sounded (within the key signature and scale selected) according to the 'delta' value assigned to that key. The advantages of such a system are convenience, speed, and the ability to play difficult, unusual and/or unintended passages extemporaneously. },
  Keywords                 = {samchillian, keyboard, MIDI controller, relative, interval, microtonal, computer keyboard, pitch, musical instrument },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_256.pdf}
}

@InProceedings{Guedes2007,
  Title                    = {Establishing a Musical Channel of Communication between Dancers and Musicians in Computer-Mediated Collaborations in Dance Performance},
  Author                   = {Guedes, Carlos},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {417--419},

  Abstract                 = {In this demonstration, I exemplify how a musical channel ofcommunication can be established in computer-mediatedinteraction between musicians and dancers in real time. Thischannel of communication uses a software libraryimplemented as a library of external objects for Max/MSP[1],that processes data from an object or library that performsframe-differencing analysis of a video stream in real time inthis programming environment.},
  Keywords                 = {dance,in dance,interaction between music and,interactive,interactive dance,interactive performance,musical rhythm and rhythm,nime07,performance systems},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_417.pdf}
}

@InProceedings{Gurevich2007,
  Title                    = {Expression and Its Discontents : Toward an Ecology of Musical Creation},
  Author                   = {Gurevich, Michael and Trevi\~{n}o, Jeffrey},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {106--111},

  Abstract                 = {We describe the prevailing model of musical expression, which assumes a binary formulation of "the text" and "the act," along with its implied roles of composer and performer. We argue that this model not only excludes some contemporary aesthetic values but also limits the communicative ability of new music interfaces. As an alternative, an ecology of musical creation accounts for both a diversity of aesthetic goals and the complex interrelation of human and non-human agents. An ecological perspective on several approaches to musical creation with interactive technologies reveals an expanded, more inclusive view of artistic interaction that facilitates novel, compelling ways to use technology for music. This paper is fundamentally a call to consider the role of aesthetic values in the analysis of artistic processes and technologies. },
  Keywords                 = {Expression, expressivity, non-expressive, emotion, discipline, model, construct, discourse, aesthetic goal, experience, transparency, evaluation, communication },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_106.pdf}
}

@InProceedings{Han2007,
  Title                    = {Project Scriabin v.3},
  Author                   = {Han, Chang Min},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {388--389},

  Keywords                 = {1,1872-1915,alexander scriabin,historic background,nime07,sonification,synaesthesia,touch screen,was a russian composer,whose},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_388.pdf}
}

@InProceedings{Hashida2007,
  Title                    = {jPop-E : An Assistant System for Performance Rendering of Ensemble Music},
  Author                   = {Hashida, Mitsuyo and Nagata, Noriko and Katayose, Haruhiro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {313--316},

  Abstract                 = {This paper introduces jPop-E (java-based PolyPhrase Ensemble), an assistant system for the Pop-E performancerendering system. Using this assistant system, MIDI dataincluding expressive tempo changes or velocity control canbe created based on the user's musical intention. Pop-E(PolyPhrase Ensemble) is one of the few machine systemsdevoted to creating expressive musical performances thatcan deal with the structure of polyphonic music and theuser's interpretation of the music. A well-designed graphical user interface is required to make full use of the potential ability of Pop-E. In this paper, we discuss the necessaryelements of the user interface for Pop-E, and describe theimplemented system, jPop-E.},
  Keywords                 = {Performance Rendering, User Interface, Ensemble Music Ex- pression },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_313.pdf}
}

@InProceedings{Hashida2007a,
  Title                    = {A System for Improvisational Musical Expression Based on Player ' s Sense of Tempo},
  Author                   = {Hashida, Tomoko and Naemura, Takeshi and Sato, Takao},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {407--408},

  Abstract                 = {This paper introduces a system for improvisational musicalexpression that enables all users, novice and experienced, toperform intuitively and expressively. Users can generate musically consistent results through intuitive action, inputtingrhythm in a decent tempo. We demonstrate novel mappingways that reect user's input information more interactivelyand eectively in generating the music. We also present various input devices that allow users more creative liberty.},
  Keywords                 = {Improvisation, interactive music, a sense of tempo },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_407.pdf}
}

@InProceedings{Hauert2007,
  Title                    = {Instant City, a Music Building Game Table},
  Author                   = {Hauert, Sibylle and Reichmuth, Daniel and B\"{o}hm, Volker},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {422--422},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_422.pdf}
}

@InProceedings{Hoffman2007,
  Title                    = {Real-Time Feature-Based Synthesis for Live Musical Performance},
  Author                   = {Hoffman, Matt and Cook, Perry R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {309--312},

  Abstract                 = {A crucial set of decisions in digital musical instrument design deals with choosing mappings between parameters controlled by the performer and the synthesis algorithms that actually generate sound. Feature-based synthesis offers a way to parameterize audio synthesis in terms of the quantifiable perceptual characteristics, or features, the performer wishes the sound to take on. Techniques for accomplishing such mappings and enabling feature-based synthesis to be performed in real time are discussed. An example is given of how a real-time performance system might be designed to take advantage of feature-based synthesis's ability to provide perceptually meaningful control over a large number of synthesis parameters. },
  Keywords                 = {Feature, Synthesis, Analysis, Mapping, Real-time. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_309.pdf}
}

@InProceedings{Hollinger2007,
  Title                    = {fMRI-Compatible Electronic Controllers},
  Author                   = {Hollinger, Avrum and Steele, Christopher and Penhune, Virginia and Zatorre, Robert and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {246--249},

  Abstract                 = {This paper presents an electronic piano keyboard and computer mouse designed for use in a magnetic resonance imaging scanner. The interface allows neuroscientists studying motor learning of musical tasks to perform functional scans of a subject's brain while synchronizing the scanner, auditory and visual stimuli, and auditory feedback with the onset, offset, and velocity of the piano keys. The design of the initial prototype and environment-specific issues are described, as well as prior work in the field. Preliminary results are positive and were unable to show the existence of image artifacts caused by the interface. Recommendations to improve the optical assembly are provided in order to increase the robustness of the design. },
  Keywords                 = {Input device, MRI-compatible, fMRI, motor learning, optical sensing. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_246.pdf}
}

@InProceedings{Hornof2007,
  Title                    = {EyeMusic : Performing Live Music and Multimedia Compositions with Eye Movements},
  Author                   = {Hornof, Anthony J. and Rogers, Troy and Halverson, Tim},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {299--300},

  Abstract                 = {In this project, eye tracking researchers and computer music composers collaborate to create musical compositions that are played with the eyes. A commercial eye tracker (LC Technologies Eyegaze) is connected to a music and multimedia ,
,
authoring environment (Max/MSP/Jitter). The project addresses issues of both noise and control: How will the performance benefit from the noise inherent in eye trackers and eye movements, and to what extent should the composition encourage the performer to try to control a specific musical outcome? Providing one set of answers to these two questions, the ,
,
authors create an eye-controlled composition, EyeMusic v1.0, which was selected by juries for live performance at computer music conferences.,
,
author KeywordsComputer music, eye tracking, new media art, performance.ACM Classification },
  Keywords                 = {H.5.2 [Information Interfaces and Presentation] User Interfaces - input devices and strategies, interaction styles. J.5 [Arts and Humanities] Fine arts, performing arts. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_299.pdf}
}

@InProceedings{Hsu2007,
  Title                    = {Design Issues in Interaction Modeling for Free Improvisation},
  Author                   = {Hsu, William},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {367--370},

  Abstract                 = {In previous publications (see for example [2] and [3]), we described an interactive music system, designed to improvise with saxophonist John Butcher; our system analyzes timbral and gestural features in real-time, and uses this information to guide response generation. This paper overviews our recent work with the system's interaction management component (IMC). We explore several options for characterizing improvisation at a higher level, and managing decisions for interactive performance in a rich timbral environment. We developed a simple, efficient framework using a small number of features suggested by recent work in mood modeling in music. We describe and evaluate the first version of the IMC, which was used in performance at the Live Algorithms for Music (LAM) conference in December 2006. We touch on developments on the system since LAM, and discuss future plans to address perceived shortcomings in responsiveness, and the ability of the system to make long-term adaptations. },
  Keywords                 = {Interactive music systems, timbral analysis, free improvisation. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_367.pdf}
}

@InProceedings{Jakovich2007,
  Title                    = {ParticleTecture : Interactive Granular Soundspaces for Architectural Design},
  Author                   = {Jakovich, Joanne and Beilharz, Kirsty},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {185--190},

  Keywords                 = {Architecture, installation, interaction, granular synthesis, adaptation, engagement. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_185.pdf}
}

@InProceedings{Jones2007,
  Title                    = {Controlling a Physical Model with a {2D} Force Matrix},
  Author                   = {Jones, Randy and Schloss, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {27--30},

  Abstract                 = {Physical modeling has proven to be a successful method ofsynthesizing highly expressive sounds. However, providingdeep methods of real time musical control remains a majorchallenge. In this paper we describe our work towards aninstrument for percussion synthesis, in which a waveguidemesh is both excited and damped by a 2D matrix of forcesfrom a sensor. By emulating a drum skin both as controllerand sound generator, our instrument has reproduced someof the expressive qualities of hand drumming. Details of ourimplementation are discussed, as well as qualitative resultsand experience gleaned from live performances.},
  Keywords                 = {Physical modeling, instrument design, expressive control, multi-touch, performance },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_027.pdf}
}

@InProceedings{Kanebako2007,
  Title                    = {Mountain Guitar : a Musical Instrument for Everyone},
  Author                   = {Kanebako, Junichi and Gibson, James and Mignonneau, Laurent},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {396--398},

  Keywords                 = {Turntable, D.J. ,Phenakistoscope, multimedia performance },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_396.pdf}
}

@InProceedings{Kapur2007,
  Title                    = {Integrating HyperInstruments , Musical Robots \& Machine Musicianship for North {India}n Classical Music},
  Author                   = {Kapur, Ajay and Singer, Eric and Benning, Manjinder S. and Tzanetakis, George and Trimpin, Trimpin},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {238--241},

  Abstract                 = {This paper describes a system enabling a human to perform music with a robot in real-time, in the context of North Indian classical music. We modify a traditional acoustic sitar into a hyperinstrument in order to capture performance gestures for musical analysis. A custom built four-armed robotic Indian drummer was built using a microchip, solenoids, aluminum and folk frame drums. Algorithms written towards "intelligent" machine musicianship are described. The final goal of this research is to have a robotic drummer accompany a professional human sitar player live in performance. },
  Keywords                 = {Musical Robotics, Electronic Sitar, Hyperinstruments, Music Information Retrieval (MIR). },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_238.pdf}
}

@InProceedings{Keating2007,
  Title                    = {The Lambent Reactive : An Audiovisual Environment for Kinesthetic Playforms},
  Author                   = {Keating, Noah H.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {338--343},

  Abstract                 = {In this paper, design scenarios made possible by the use of an interactive illuminated floor as the basis of an audiovisual environment are presented. By interfacing a network of pressure sensitive, light-emitting tiles with a 7.1 channel speaker system and requisite audio software, many avenues for collaborative expression emerge, as do heretofore unexplored modes of multiplayer music and dance gaming. By giving users light and sound cues that both guide and respond to their movement, a rich environment is created that playfully integrates the auditory, the visual, and the kinesthetic into a unified interactive experience.},
  Keywords                 = {Responsive Environments, Audiovisual Play, Kinetic Games, Movement Rich Game Play, Immersive Dance, Smart Floor },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_338.pdf}
}

@InProceedings{Kim2007,
  Title                    = {Oculog : Playing with Eye Movements},
  Author                   = {Kim, Juno and Schiemer, Greg and Narushima, Terumi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {50--55},

  Keywords                 = {1,algorithmic composition,expressive control interfaces,eye movement recording,microtonal tuning,midi,nime07,pure data,video},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_050.pdf}
}

@InProceedings{Kirk2007,
  Title                    = {The FrankenPipe : A Novel Bagpipe Controller},
  Author                   = {Kirk, Turner and Leider, Colby},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {301--304},

  Abstract                 = {The FrankenPipe project is an attempt to convert a traditionalHighland Bagpipe into a controller capable of driving both realtime synthesis on a laptop as well as a radio-controlled (RC) car.Doing so engages musical creativity while enabling novel, oftenhumorous, performance art. The chanter is outfitted withphotoresistors (CdS photoconductive cells) underneath each hole,allowing a full range of MIDI values to be produced with eachfinger and giving the player a natural feel. An air-pressure sensoris also deployed in the bag to provide another element of controlwhile capturing a fundamental element of bagpipe performance.The final product navigates the realm of both musical instrumentand toy, allowing the performer to create a novel yet richperformance experience for the audience.},
  Keywords                 = {FrankenPipe, alternate controller, MIDI, bagpipe, photoresistor, chanter. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_301.pdf}
}

@InProceedings{Knorig2007,
  Title                    = {Articulated Paint : Musical Expression for Non-Musicians},
  Author                   = {Kn\"{o}rig, Andr\'{e} and M\"{u}ller, Boris and Wettach, Reto},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {384--385},

  Abstract                 = {In this paper we present the concept and prototype of a new musical interface that utilizes the close relationship between gestural expression in the act of painting and that of playing a musical instrument in order to provide non-musicians the opportunity to create musical expression. A physical brush on a canvas acts as the instrument. The characteristics of its stroke are intuitively mapped to a conductor program, defining expressive parameters of the tone in real-time. Two different interaction modes highlight the importance of bodily expression in making music as well as the value of a metaphorical visual representation.},
  Keywords                 = {musical interface, musical expression, expressive gesture, musical education, natural interface },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_384.pdf}
}

@InProceedings{Lee2007,
  Title                    = {Towards Rhythmic Analysis of Human Motion Using Acceleration-Onset Times},
  Author                   = {Lee, Eric and Enke, Urs and Borchers, Jan and de Jong, Leo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {136--141},

  Abstract                 = {We present a system for rhythmic analysis of human motion inreal-time. Using a combination of both spectral (Fourier) andspatial analysis of onsets, we are able to extract repeating rhythmic patterns from data collected using accelerometers. These extracted rhythmic patterns show the relative magnitudes of accentuated movements and their spacing in time. Inspired by previouswork in automatic beat detection of audio recordings, we designedour algorithms to be robust to changes in timing using multipleanalysis techniques and methods for sensor fusion, filtering andclustering. We tested our system using a limited set of movements,as well as dance movements collected from a professional, bothwith promising results.},
  Keywords                 = {rhythm analysis, dance movement analysis, onset analysis },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_136.pdf}
}

@InProceedings{Lee2007a,
  Title                    = {REXband : A Multi-User Interactive Exhibit for Exploring Medieval Music},
  Author                   = {Lee, Eric and Wolf, Marius and Jansen, Yvonne and Borchers, Jan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {172--177},

  Abstract                 = {We present REXband, an interactive music exhibit for collaborative improvisation to medieval music. This audio-only system consists of three digitally augmented medieval instrument replicas: thehurdy gurdy, harp, and frame drum. The instruments communicatewith software that provides users with both musical support andfeedback on their performance using a "virtual audience" set in amedieval tavern. REXband builds upon previous work in interactivemusic exhibits by incorporating aspects of e-learning to educate, inaddition to interaction design patterns to entertain; care was alsotaken to ensure historic authenticity. Feedback from user testingin both controlled (laboratory) and public (museum) environmentshas been extremely positive. REXband is part of the RegensburgExperience, an exhibition scheduled to open in July 2007 to showcase the rich history of Regensburg, Germany.},
  Keywords                 = {interactive music exhibits, medieval music, augmented instru- ments, e-learning, education },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_172.pdf}
}

@InProceedings{Loewenstein2007,
  Title                    = {"Acoustic Map" -- An Interactive Cityportrait},
  Author                   = {Loewenstein, Stefan},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {405--406},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_405.pdf}
}

@InProceedings{Magnusson2007,
  Title                    = {The Acoustic, the Digital and the Body : A Survey on Musical Instruments},
  Author                   = {Magnusson, Thor and Mendieta, Enrike H.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {94--99},

  Abstract                 = {This paper reports on a survey conducted in the autumn of 2006 with the objective to understand people's relationship to their musical tools. The survey focused on the question of embodiment and its different modalities in the fields of acoustic and digital instruments. The questions of control, instrumental entropy, limitations and creativity were addressed in relation to people's activities of playing, creating or modifying their instruments. The approach used in the survey was phenomenological, i.e. we were concerned with the experience of playing, composing for and designing digital or acoustic instruments. At the time of analysis, we had 209 replies from musicians, composers, engineers, designers, artists and others interested in this topic. The survey was mainly aimed at instrumentalists and people who create their own instruments or compositions in flexible audio programming environments such as SuperCollider, Pure Data, ChucK, Max/MSP, CSound, etc. },
  Keywords                 = {Survey, musical instruments, usability, ergonomics, embodiment, mapping, affordances, constraints, instrumental entropy, audio programming. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_094.pdf}
}

@InProceedings{Malloch2007,
  Title                    = {The T-Stick : From Musical Interface to Musical Instrument},
  Author                   = {Malloch, Joseph and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {66--69},

  Abstract                 = {This paper describes the T-Stick, a new family of digitalmusical instruments. It presents the motivation behind theproject, hardware and software design, and presents insightsgained through collaboration with performers who have collectively practised and performed with the T-Stick for hundreds of hours, and with composers who have written piecesfor the instrument in the context of McGill University's Digital Orchestra project. Each of the T-Sticks is based on thesame general structure and sensing platform, but each alsodiffers from its siblings in size, weight, timbre and range.},
  Keywords                 = {gestural controller, digital musical instrument, families of instruments },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_066.pdf}
}

@InProceedings{Mann2007,
  Title                    = {Natural Interfaces for Musical Expression : Physiphones and a Physics-Based Organology},
  Author                   = {Mann, Steve},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {118--123},

  Keywords                 = {all or part of,ethnomusicology,hydraulophone,is granted without fee,nime07,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,tangible user interface,this work for},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_118.pdf}
}

@InProceedings{Milmoe2007,
  Title                    = {NIME Performance \& Installation : Sonic Pong V3.0},
  Author                   = {Milmoe, Andrew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {423--423},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_423.pdf}
}

@InProceedings{Moody2007,
  Title                    = {Ashitaka : An Audiovisual Instrument},
  Author                   = {Moody, Niall and Fells, Nick and Bailey, Nicholas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {148--153},

  Abstract                 = {This paper describes the Ashitaka audiovisual instrumentand the process used to develop it. The main idea guidingthe design of the instrument is that motion can be used toconnect audio and visuals, and the first part of the paperconsists of an exploration of this idea. The issue of mappings is raised, discussing both audio-visual mappings andthe mappings between the interface and synthesis methods.The paper concludes with a detailed look at the instrumentitself, including the interface, synthesis methods, and mappings used.},
  Keywords                 = {audiovisual,instrument,mappings,nime07,synchresis,x3d},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_148.pdf}
}

@InProceedings{Moriwaki2007,
  Title                    = {MIDI Scrapyard Challenge Workshops},
  Author                   = {Moriwaki, Katherine and Brucken-Cohen, Jonah},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {168--172},

  Abstract                 = {In this paper the ,
,
authors present the MIDI Scrapyard Challenge (MSC) workshop, a one-day hands-on experience which asks participants to create musical controllers out of cast-off electronics, found materials and junk. The workshop experience, principles, and considerations are detailed, along with sample projects which have been created in various MSC workshops. Observations and implications as well as future developments for the workshop are discussed. },
  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_168.pdf}
}

@InProceedings{Morris2007,
  Title                    = {Musique Concrete : Transforming Space , Sound and the City Through Skateboarding},
  Author                   = {Morris, Simon},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {425--425},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_425.pdf}
}

@InProceedings{Nagashima2007,
  Title                    = {GHI project and "Cyber Kendang"},
  Author                   = {Nagashima, Yoichi},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {250--253},

  Keywords                 = {kendang,media arts,new instruments,nime07,sound and light},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_250.pdf}
}

@InProceedings{Nakamoto2007,
  Title                    = {Circle Canon Chorus System Used To Enjoy A Musical Ensemble Singing "Frog Round"},
  Author                   = {Nakamoto, Misako and Kuhara, Yasuo},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {409--410},

  Abstract                 = {We proposed a circle canon system for enjoying a musical ensemble supported by a computer and network. Using the song "Frog round", which is a popular circle canon chorus originated from a German folk song, we produced a singing ensemble opportunity where everyone plays the music together at the same time. The aim of our system is that anyone can experience the joyful feeling of actually playing the music as well as sharing it with others. },
  Keywords                 = {Circle canon, Chorus, Song, Frog round, Ensemble, Internet, Max/MSP, MySQL database. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_409.pdf}
}

@InProceedings{Nilson2007,
  Title                    = {Live Coding Practice},
  Author                   = {Nilson, Click},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {112--117},

  Abstract                 = {Live coding is almost the antithesis of immediate physicalmusicianship, and yet, has attracted the attentions of a numberof computer-literate musicians, as well as the music-savvyprogrammers that might be more expected. It is within thecontext of live coding that I seek to explore the question ofpractising a contemporary digital musical instrument, which i soften raised as an aside but more rarely carried out in research(though see [12]). At what stage of expertise are the membersof the live coding movement, and what practice regimes mighthelp them to find their true potential? },
  Keywords                 = {Practice, practising, live coding },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_112.pdf}
}

@InProceedings{Ojanen2007,
  Title                    = {Design Principles and User Interfaces of Erkki Kurenniemi's Electronic Musical Instruments of the 1960's and 1970's},
  Author                   = {Ojanen, Mikko and Suominen, Jari and Kallio, Titti and Lassfolk, Kai},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {88--93},

  Abstract                 = {This paper presents a line of historic electronic musical instruments designed by Erkki Kurenniemi in the 1960's and1970's. Kurenniemi's instruments were influenced by digitallogic and an experimental attitude towards user interfacedesign. The paper presents an overview of Kurenniemi'sinstruments and a detailed description of selected devices.Emphasis is put on user interface issues such as unconventional interactive real-time control and programming methods.},
  Keywords                 = {Erkki Kurenniemi, Dimi, Synthesizer, Digital electronics, User interface design },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_088.pdf}
}

@InProceedings{Oliver2007,
  Title                    = {Fijuu2 : A Game-Based Audio-Visual Performance and Composition Engine},
  Author                   = {Oliver, Julian and Pickles, Steven},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {430--430},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_430.pdf}
}

@InProceedings{Perez2007,
  Title                    = {D\'{\i}amair : Composing for Choir and Integral Music Controller},
  Author                   = {P\'{e}rez, Miguel A. and Knapp, Benjamin and Alcorn, Michael},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {289--292},

  Abstract                 = {In this paper, we describe the composition of a piece for choir and Integral Music Controller. We focus more on the aesthetic, conceptual, and practical aspects of the interface and less on the technological details. We especially stress the influence that the designed interface poses on the compositional process and how we approach the expressive organisation of musical materials during the composition of the piece, as well as the addition of nuances (personal real-time expression) by the musicians at performance time. },
  Keywords                 = {Composition, Integral Music Controller, Emotion measurement, Physiological Measurement, Spatialisation. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_289.pdf}
}

@InProceedings{Paine2007,
  Title                    = {The Thummer Mapping Project (ThuMP)},
  Author                   = {Paine, Garth and Stevenson, Ian and Pearce, Angela},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {70--77},

  Keywords                 = {Musical Instrument Design, Mapping, Musicianship, evaluation, testing. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_070.pdf}
}

@InProceedings{Estrada2007,
  Title                    = {Loop-R : Real-Time Video Interface},
  Author                   = {Pereira, Rui},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {411--414},

  Abstract                 = {Loop-R is a real-time video performance tool, based in the exploration of low-tech, used technology and human engineering research. With this tool its ,
,
author is giving a shout to industry, using existing and mistreated technology in innovative ways, combining concepts and interfaces: blending segregated interfaces (GUI and Physical) into one. After graspable interfaces and the "end" of WIMP interfaces, hardware and software blend themselves in a new genre providing free control of video-loops in an expressive hybrid tool. },
  Keywords                 = {Real-time; video; interface; live-visuals; loop; },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_411.pdf}
}

@InProceedings{Poepel2007,
  Title                    = {>hot\_strings SIG},
  Author                   = {Poepel, Cornelius and Marx, G\"{u}nter},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {281--284},

  Abstract                 = {Many fascinating new developments in the area bowed stringed instruments have been developed in recent years. However, the majority of these new applications are either not well known, used orconsidered in a broader context by their target users. The necessaryexchange between the world of developers and the players is ratherlimited. A group of performers, researchers, instrument developersand composers was founded in order to share expertise and experiences and to give each other feedback on the work done to developnew instruments. Instruments incorporating new interfaces, synthesis methods, sensor technology, new materials like carbon fiber andwood composites as well as composite materials and research outcome are presented and discussed in the group. This paper gives anintroduction to the group and reports about activities and outcomesin the last two years.},
  Keywords                 = {Interdisciplinary user group, electronic bowed string instrument, evaluation of computer based musical instruments },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_281.pdf}
}

@InProceedings{Porres2007,
  Title                    = {Adaptive Tuning Using Theremin as Gestural Controller},
  Author                   = {Porres, Alexandre T. and Manzolli, Jonatas},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {363--366},

  Abstract                 = {This work presents an interactive device to control an adaptive tuning and synthesis system. The gestural controller is based on the theremin concept in which only an antenna is used as a proximity sensor. This interactive process is guided by sensorial consonance curves and adaptive tuning related to psychoacoustical studies. We used an algorithm to calculate the dissonance values according to amplitudes and frequencies of a given sound spectrum. The theoretical background is presented followed by interactive composition strategies and sound results. },
  Keywords                 = {Interaction, adaptive tuning, theremin, sensorial dissonance, synthesis. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_363.pdf}
}

@InProceedings{Quessy2007,
  Title                    = {Human Sequencer},
  Author                   = {Quessy, Alexandre},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {433--433},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_433.pdf}
}

@InProceedings{Rigler2007,
  Title                    = {The Music Cre8tor : an Interactive System for Musical Exploration and Education},
  Author                   = {Rigler, Jane and Seldess, Zachary},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {415--416},

  Abstract                 = {The Music Cre8tor is an interactive music composition systemcontrolled by motion sensors specifically designed forchildren with disabilities although not exclusively for thispopulation. The player(s) of the Music Cre8tor can either holdor attach accelerometer sensors to trigger a variety ofcomputer-generated sounds, MIDI instruments and/or prerecorded sound files. The sensitivity of the sensors can bemodified for each unique individual so that even the smallestmovement can control a sound. The flexibility of the systemis such that either four people can play simultaneously and/orone or more players can use up to four sensors. The originalgoal of this program was to empower students with disabilitiesto create music and encourage them to perform with othermusicians, however this same goal has expanded to includeother populations.},
  Keywords                 = {Music Education, disabilities, special education, motion sensors, music composition, interactive performance. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_415.pdf}
}

@InProceedings{Robertson2007,
  Title                    = {B-Keeper : A Beat-Tracker for Live Performance},
  Author                   = {Robertson, Andrew and Plumbley, Mark D.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {234--237},

  Abstract                 = {This paper describes the development of B-Keeper, a reatime beat tracking system implemented in Java and Max/MSP,which is capable of maintaining synchronisation between anelectronic sequencer and a drummer. This enables musicians to interact with electronic parts which are triggeredautomatically by the computer from performance information. We describe an implementation which functions withthe sequencer Ableton Live.},
  Keywords                 = {Human-Computer Interaction, Automatic Accompaniment, Performance },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_234.pdf}
}

@InProceedings{Rohs2007,
  Title                    = {CaMus 2 -- Optical Flow and Collaboration in Camera Phone Music Performance},
  Author                   = {Rohs, Michael and Essl, Georg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {160--163},

  Abstract                 = {CaMus2 allows collaborative performance with mobile camera phones. The original CaMus project was extended tosupport multiple phones performing in the same space andgenerating MIDI signals to control sound generation andmanipulation software or hardware. Through an opticalflow technology the system can be used without a referencemarker grid. When using a marker grid, the use of dynamicdigital zoom extends the range of performance. Semanticinformation display helps guide the performer visually.},
  Keywords                 = {Camera phone, mobile phone, music performance, mobile sound generation, sensing-based interaction, collaboration },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_160.pdf}
}

@InProceedings{Sa2007,
  Title                    = {Thresholds},
  Author                   = {Sa, Adriana},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {428--428},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_428.pdf}
}

@InProceedings{Sarkar2007,
  Title                    = {Recognition and Prediction in a Network Music Performance System for {India}n Percussion},
  Author                   = {Sarkar, Mihir and Vercoe, Barry},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {317--320},

  Abstract                 = {Playing music over the Internet, whether for real-time jamming, network performance or distance education, is constrained by the speed of light which introduces, over long distances, time delays unsuitable for musical applications. Current musical collaboration systems generally transmit compressed audio streams over low-latency and high-bandwidthnetworks to optimize musician synchronization. This paperproposes an alternative approach based on pattern recognition and music prediction. Trained for a particular typeof music, here the Indian tabla drum, the system calledTablaNet identifies rhythmic patterns by recognizing individual strokes played by a musician and mapping them dynamically to known musical constructs. Symbols representing these musical structures are sent over the network toa corresponding computer system. The computer at thereceiving end anticipates incoming events by analyzing previous phrases and synthesizes an estimated audio output.Although such a system may introduce variants due to prediction approximations, resulting in a slightly different musical experience at both ends, we find that it demonstratesa high level of playability with an immediacy not present inother systems, and functions well as an educational tool.},
  Keywords                 = {network music performance, real-time online musical collab- oration, Indian percussions, tabla bols, strokes recognition, music prediction },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_317.pdf}
}

@InProceedings{Schacher2007,
  Title                    = {Gesture Control of Sounds in {3D} Space},
  Author                   = {Schacher, Jan C.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {358--362},

  Abstract                 = {This paper presents a methodology and a set of tools for gesture control of sources in 3D surround sound. The techniques for rendering acoustic events on multi-speaker or headphone-based surround systems have evolved considerably, making it possible to use them in real-time performances on light equipment. Controlling the placement of sound sources is usually done in idiosyncratic ways and has not yet been fully explored and formalized. This issue is addressed here with the proposition of a methodical approach. The mapping of gestures to source motion is implemented by giving the sources physical object properties and manipulating these characteristics with standard geometrical transforms through hierarchical or emergent relationships. },
  Keywords                 = {Gesture, Surround Sound, Mapping, Trajectory, Transform Matrix, Tree Hierarchy, Emergent Structures. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_358.pdf}
}

@InProceedings{Corness2007,
  Title                    = {nite\_aura : An Audio-Visual Interactive Immersive Installation},
  Author                   = {Seo, Jinsil and Corness, Greg},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {431--431},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_431.pdf}
}

@InProceedings{Sinclair2007,
  Title                    = {Defining a Control Standard for Easily Integrating Haptic Virtual Environments with Existing Audio / Visual Systems},
  Author                   = {Sinclair, Stephen and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {209--212},

  Abstract                 = {This paper presents an approach to audio-haptic integration that utilizes Open Sound Control, an increasingly wellsupported standard for audio communication, to initializeand communicate with dynamic virtual environments thatwork with off-the-shelf force-feedback devices.},
  Keywords                 = {Haptics, control, multi-modal, audio, force-feedback },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_209.pdf}
}

@InProceedings{Sirguy2007,
  Title                    = {Eobody2 : A Follow-up to Eobody's Technology},
  Author                   = {Sirguy, Marc and Gallin, Emmanuelle},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {401--402},

  Abstract                 = {Eowave and Ircam have been deeply involved into gestureanalysis and sensing for a few years by now, as severalartistic projects demonstrate (1). In 2004, Eowave has beenworking with Ircam on the development of the Eobodysensor system, and since that, Eowave's range of sensors hasbeen increased with new sensors sometimes developed innarrow collaboration with artists for custom sensor systemsfor installations and performances. This demo-paperdescribes the recent design of a new USB/MIDI-to-sensorinterface called Eobody2.},
  Keywords                 = {Gestural controller, Sensor, MIDI, USB, Computer music, Relays, Motors, Robots, Wireless. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_401.pdf}
}

@InProceedings{Stark2007,
  Title                    = {Real-Time Beat-Synchronous Audio Effects},
  Author                   = {Stark, Adam M. and Plumbley, Mark D. and Davies, Matthew E.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {344--345},

  Abstract                 = {We present a new group of audio effects that use beat tracking, the detection of beats in an audio signal, to relate effectparameters to the beats in an input signal. Conventional audio effects are augmented so that their operation is related tothe output of a beat tracking system. We present a temposynchronous delay effect and a set of beat synchronous lowfrequency oscillator effects including tremolo, vibrato andauto-wah. All effects are implemented in real-time as VSTplug-ins to allow for their use in live performance.},
  Keywords                 = {a beat-synchronous tremolo effect,audio effects,beat tracking,figure 1,im-,nime07,plemented as a vst,plug-in,real-time,the rate is controlled,vst plug-in},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_344.pdf}
}

@InProceedings{Steiner2007,
  Title                    = {A Unified Toolkit for Accessing Human Interface Devices in Pure Data and Max / MSP},
  Author                   = {Steiner, Hans-Christoph},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {375--378},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_375.pdf}
}

@InProceedings{Takahashi2007,
  Title                    = {bog : Instrumental Aliens},
  Author                   = {Takahashi, Masato and Tanaka, Hiroya},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {429--429},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_429.pdf}
}

@InProceedings{Takegawa2007,
  Title                    = {Mobile Clavier : New Music Keyboard for Flexible Key Transpose},
  Author                   = {Takegawa, Yoshinari and Terada, Tsutomu},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {82--87},

  Keywords                 = {Portable keyboard, Additional black keys, Diapason change },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_082.pdf}
}

@InProceedings{Till2007,
  Title                    = {Wireless Inertial Sensor Package (WISP)},
  Author                   = {Till, Bernie C. and Benning, Manjinder S. and Livingston, Nigel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {403--404},

  Abstract                 = {The WISP is a novel wireless sensor that uses 3 axis magnetometers, accelerometers, and rate gyroscopes to provide a real-time measurement of its own orientation in space. Orientation data are transmitted via the Open Sound Control protocol (OSC) to a synthesis engine for interactive live dance performance. },
  Keywords                 = {Music Controller, Human-Computer Interaction, Wireless Sensing, Inertial Sensing. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_403.pdf}
}

@InProceedings{Tindale2007,
  Title                    = {A Hybrid Method for Extended Percussive Gesture},
  Author                   = {Tindale, Adam R.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {392--393},

  Keywords                 = {electronic percussion,nime07,physical modeling,timbre recogni-},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_392.pdf}
}

@InProceedings{Topper2007,
  Title                    = {Extended Applications of the Wireless Sensor Array (WISEAR)},
  Author                   = {Topper, David},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {203--204},

  Abstract                 = { WISEAR (Wireless Sensor Array)8, provides a robust andscalable platform for virtually limitless types of data input tosoftware synthesis engines. It is essentially a Linux based SBC(Single Board Computer) with 802.11a/b/g wireless capability.The device, with batteries, only weighs a few pounds and can beworn by a dancer or other live performer. Past work has focusedon connecting "conventional" sensors (eg., bend sensors,accelerometers, FSRs, etc...) to the board and using it as a datarelay, sending the data as real time control messages to synthesisengines like Max/MSP and RTcmix1. Current research hasextended the abilities of the device to take real-time audio andvideo data from USB cameras and audio devices, as well asrunning synthesis engines on board the device itself. Given itsgeneric network ability (eg., being an 802.11a/b/g device) there istheoretically no limit to the number of WISEAR boxes that canbe used simultaneously in a performance, facilitating multiperformer compositions. This paper will present the basic design philosophy behindWISEAR, explain some of the basic concepts and methods, aswell as provide a live demonstration of the running device, wornby the ,
,
author.},
  Keywords                 = {Wireless, sensors, embedded devices, linux, real-time audio, real- time video },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_203.pdf}
}

@InProceedings{Fernstrom2007,
  Title                    = {Celeritas : Wearable Wireless System},
  Author                   = {Torre, Giuseppe and Fernstr\"{o}m, Mikael and O'Flynn, Brendan and Angove, Philip},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {205--208},

  Keywords                 = {Inertial Measurement Unit, IMU, Position Tracking, Interactive Dance Performance, Graphical Object, Mapping. },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_205.pdf}
}

@InProceedings{Toyoda2007,
  Title                    = {Sensillum : An Improvisational Approach to Composition},
  Author                   = {Toyoda, Shinichiro},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {254--255},

  Abstract                 = {This study proposes new possibilities for interaction design pertaining to music piece creation. Specifically, the study created an environment wherein a wide range of users are able to easily experience new musical expressions via a combination of newly developed software and the Nintendo Wii Remote controller. },
  Keywords                 = {Interactive systems, improvisation, gesture, composition INTRODUCTION Though music related research focusing on the interaction between people and computers is currently experiencing wide range development, the history of approaches wherein the creation of new musical expression is made possible via the active },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_254.pdf}
}

@InProceedings{Uozumi2007,
  Title                    = {Bd : A Sound Installation with Swarming Robots},
  Author                   = {Uozumi, Yuta and Takahashi, Masato and Kobayashi, Ryoho},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {426--426},

  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_426.pdf}
}

@InProceedings{Nort2007,
  Title                    = {Control Strategies for Navigation of Complex Sonic Spaces Transformation of Resonant Models},
  Author                   = {Van Nort, Doug and Wanderley, Marcelo M.},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {379--383},

  Abstract                 = {This paper describes musical experiments aimed at designing control structures for navigating complex and continuous sonic spaces. The focus is on sound processing techniques which contain a high number of control parameters,and which exhibit subtle and interesting micro-variationsand textural qualities when controlled properly. The examples all use a simple low-dimensional controller - a standardgraphics tablet - and the task of initimate and subtle textural manipulations is left to the design of proper mappings,created using a custom toolbox of mapping functions. Thiswork further acts to contextualize past theoretical results bythe given musical presentations, and arrives at some conclusions about the interplay between musical intention, controlstrategies and the process of their design.},
  Keywords                 = {Mapping, Control, Sound Texture, Musical Gestures },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_379.pdf}
}

@InProceedings{Vanegas2007,
  Title                    = {The MIDI Pick : Trigger Serial Data , Samples, and MIDI from a Guitar Pick},
  Author                   = {Vanegas, Roy},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {330--333},

  Abstract                 = {The guitar pick has traditionally been used to strike or rakethe strings of a guitar or bass, and in rarer instances, ashamisen, lute, or other stringed instrument. The pressure exerted on it, however, has until now been ignored.The MIDI Pick, an enhanced guitar pick, embraces this dimension, acting as a trigger for serial data, audio samples,MIDI messages 1, Max/MSP patches, and on/off messages.This added scope expands greatly the stringed instrumentplayer's musical dynamic in the studio or on stage.},
  Keywords                 = {guitar, MIDI, pick, plectrum, wireless, bluetooth, ZigBee, Arduino, NIME, ITP },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_330.pdf}
}

@InProceedings{Vigoda2007,
  Title                    = {JamiOki-PureJoy : A Game Engine and Instrument for Electronically-Mediated Musical Improvisation},
  Author                   = {Vigoda, Benjamin and Merrill, David},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {321--326},

  Keywords                 = {JamiOki, PureJoy, collaborative performance, structured im- provisation, electronically-mediated performance, found sound },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_321.pdf}
}

@InProceedings{Villar2007,
  Title                    = {The ColorDex DJ System : A New Interface for Live Music Mixing},
  Author                   = {Villar, Nicolas and Gellersen, Hans and Jervis, Matt and Lang, Alexander},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {264--269},

  Abstract                 = {This paper describes the design and implementation of a new interface prototype for live music mixing. The ColorDex system employs a completely new operational metaphor which allows the mix DJ to prepare up to six tracks at once, and perform mixes between up to three of those at a time. The basic premises of the design are: 1) Build a performance tool that multiplies the possible choices a DJ has in respect in how and when tracks are prepared and mixed; 2) Design the system in such a way that the tool does not overload the performer with unnecessary complexity, and 3) Make use of novel technology to make the performance of live music mixing more engaging for both the performer and the audience. The core components of the system are: A software program to load, visualize and playback digitally encoded tracks; the HDDJ device (built chiefly out of a repurposed hard disk drive), which provides tactile manipulation of the playback speed and position of tracks; and the Cubic Crossfader, a wireless sensor cube that controls of the volume of individual tracks, and allows the DJ to mix these in interesting ways. },
  Keywords                 = {Novel interfaces, live music-mixing, cube-based interfaces, crossfading, repurposing HDDs, accelerometer-based cubic control },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_264.pdf}
}

@InProceedings{Weinberg2007,
  Title                    = {The Design of a Robotic Marimba Player -- Introducing Pitch into Robotic Musicianship},
  Author                   = {Weinberg, Gil and Driscoll, Scott},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {228--233},

  Keywords                 = {human-machine interaction,improvisation,nime07,perceptual modeling,robotic musicianship},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_228.pdf}
}

@InProceedings{Wessel2007,
  Title                    = {A Force Sensitive Multi-Touch Array Supporting Multiple {2-D} Musical Control Structures},
  Author                   = {Wessel, David and Avizienis, Rimas and Freed, Adrian and Wright, Matthew},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {41--45},

  Keywords                 = {high-resolution gestural signals,nime07,pressure and force sensing},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_041.pdf}
}

@InProceedings{Wulfson2007,
  Title                    = {Automatic Notation Generators},
  Author                   = {Wulfson, Harris and Barrett, G. Douglas and Winter, Michael},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {346--351},

  Abstract                 = {This article presents various custom software tools called Automatic Notation Generators (ANG's) developed by the authors to aid in the creation of algorithmic instrumental compositions. The unique possibilities afforded by ANG software are described, along with relevant examples of their compositional output. These avenues of exploration include: mappings of spectral data directly into notated music, the creation of software transcribers that enable users to generate multiple realizations of algorithmic compositions, and new types of spontaneous performance with live generated screen-based music notation. The authors present their existing software tools along with suggestions for future research and artistic inquiry. },
  Keywords                 = {nime07},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_346.pdf}
}

@InProceedings{Young2007,
  Title                    = {Bowstroke Database : A Web-Accessible Archive of Violin Bowing Data},
  Author                   = {Young, Diana and Deshmane, Anagha},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {352--357},

  Abstract                 = {This paper presents a newly created database containingcalibrated gesture and audio data corresponding to variousviolin bowstrokes, as well as video and motion capture datain some cases. The database is web-accessible and searchable by keywords and subject. It also has several importantfeatures designed to improve accessibility to the data and tofoster collaboration between researchers in fields related tobowed string synthesis, acoustics, and gesture.},
  Keywords                 = {violin, bowed string, bowstroke, bowing, bowing parame- ters, technique, gesture, audio },
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_352.pdf}
}

@InProceedings{Zbyszynski2007,
  Title                    = {Ten Years of Tablet Musical Interfaces at CNMAT},
  Author                   = {Zbyszynski, Michael and Wright, Matthew and Momeni, Ali and Cullen, Daniel},
  Booktitle                = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year                     = {2007},

  Address                  = {New York City, NY, United States},
  Pages                    = {100--105},

  Abstract                 = {We summarize a decade of musical projects and research employing Wacom digitizing tablets as musical controllers, discussing general implementation schemes using Max/MSP and OpenSoundControl, and specific implementations in musical improvisation, interactive sound installation, interactive multimedia performance, and as a compositional assistant. We examine two-handed sensing strategies and schemes for gestural mapping. },
  Keywords                 = {1,algorithmic composition,digitizing tablet,expressivity,gesture,mapping,nime07,position sensing,wacom tablet,why the wacom tablet},
  Url                      = {http://www.nime.org/proceedings/2007/nime2007_100.pdf}
}

