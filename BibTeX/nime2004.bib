%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Alexander Refsum Jensenius at 2013-08-16 20:23:32 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Argo:2004,
	Abstract = {When working with sample-based media, a performer is managing timelines, loop points, sample parameters and effects parameters. The Slidepipe is a performance controller that gives the artist a visually simple way to work with their material. Its design is modular and lightweight, so it can be easily transported and quickly assembled. Also, its large stature magnifies the gestures associated with its play, providing a more convincing performance. In this paper, I will describe what the controller is, how this new controller interface has affected my live performance, and how it can be used in different performance scenarios. },
	Address = {Hamamatsu, Japan},
	Author = {Argo, Mark},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Controller, Sample Manipulation, Live Performance, Open Sound Control, Human Computer Interaction},
	Pages = {189--192},
	Title = {The Slidepipe: A Timeline-Based Controller for Real-Time Sample Manipulation},
	Url = {http://www.nime.org/proceedings/2004/nime2004_189.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_189.pdf}}

@inproceedings{Bryan-Kinns:2004,
	Address = {Hamamatsu, Japan},
	Author = {Bryan-Kinns, Nick and Healey, Patrick G.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {collaboration,composition,improvisation,music},
	Pages = {27--30},
	Title = {Daisyphone: Support for Remote Music Collaboration},
	Url = {http://www.nime.org/proceedings/2004/nime2004_027.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_027.pdf}}

@inproceedings{Burtner:2004,
	Abstract = {This paper describes a theory for modulated objects based onobservations of recent musical interface design trends. Thetheory implies extensions to an object-based approach tocontroller design. Combining NIME research withethnographic study of shamanic traditions. The ,
,
authordiscusses the creation of new controllers based on theshamanic use of ritual objects.},
	Address = {Hamamatsu, Japan},
	Author = {Burtner, Matthew},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Music and Video Controllers, New Interface Design, Music Composition, Multimedia, Mythology, Shamanism, Ecoacoustics},
	Pages = {193--196},
	Title = {A Theory of Modulated Objects for New Shamanic Controller Design},
	Url = {http://www.nime.org/proceedings/2004/nime2004_193.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_193.pdf}}

@inproceedings{Coduys:2004,
	Abstract = {High capacity of transmission lines (Ethernet in particular) is much higher than what imposed by MIDI today. So it is possible to use capturing interfaces with high-speed and high-resolution, thanks to the OSC protocol, for musical synthesis (either in realtime or non real-time). These new interfaces offer many advantages, not only in the area of musical composition with use of sensors but also in live and interactive performances. In this manner, the processes of calibration and signal processing are delocalized on a personal computer and augments possibilities of processing. In this demo, we present two hardware interfaces developed in La kitchen with corresponding processing to achieve a high-resolution, high-speed sensor processing for musical applications. },
	Address = {Hamamatsu, Japan},
	Author = {Coduys, Thierry and Henry, Cyrille and Cont, Arshia},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Interface, Sensors, Calibration, Precision, OSC, Pure Data, Max/MSP.},
	Pages = {205--206},
	Title = {TOASTER and KROONDE: High-Resolution and High- Speed Real-time Sensor Interfaces},
	Url = {http://www.nime.org/proceedings/2004/nime2004_205.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_205.pdf}}

@inproceedings{Cont:2004,
	Abstract = {In this paper, we describe an adaptive approach to gesture mapping for musical applications which serves as a mapping system for music instrument design. A neural network approach is chosen for this goal and all the required interfaces and abstractions are developed and demonstrated in the Pure Data environment. In this paper, we will focus on neural network representation and implementation in a real-time musical environment. This adaptive mapping is evaluated in different static and dynamic situations by a network of sensors sampled at a rate of 200Hz in real-time. Finally, some remarks are given on the network design and future works. },
	Address = {Hamamatsu, Japan},
	Author = {Cont, Arshia and Coduys, Thierry and Henry, Cyrille},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Real-time gesture control, adaptive interfaces, Sensor and actuator technologies for musical applications, Musical mapping algorithms and intelligent controllers, Pure Data.},
	Pages = {39--42},
	Title = {Real-time Gesture Mapping in Pd Environment using Neural Networks},
	Url = {http://www.nime.org/proceedings/2004/nime2004_039.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_039.pdf}}

@inproceedings{Dannenberg:2004,
	Address = {Hamamatsu, Japan},
	Author = {Dannenberg, Roger B.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Pages = {132--137},
	Title = {Aura II: Making Real-Time Systems Safe for Music},
	Url = {http://www.nime.org/proceedings/2004/nime2004_132.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_132.pdf}}

@inproceedings{DArcangelo:2004,
	Abstract = {This essay outlines a framework for understanding newmusical compositions and performances that utilizepre-existing sound recordings. In attempting toarticulate why musicians are increasingly using soundrecordings in their creative work, the ,
,
author calls fornew performance tools that enable the dynamic use ofpre-recorded music. },
	Address = {Hamamatsu, Japan},
	Author = {D'Arcangelo, Gideon},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Call and response, turntablism, DJ tools, oral culture},
	Pages = {55--58},
	Title = {Recycling Music, Answering Back: Toward an Oral Tradition of Electronic Music},
	Url = {http://www.nime.org/proceedings/2004/nime2004_055.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_055.pdf}}

@inproceedings{Silva:2004,
	Abstract = {We describe a simple, computationally light, real-time system for tracking the lower face and extracting informationabout the shape of the open mouth from a video sequence.The system allows unencumbered control of audio synthesismodules by action of the mouth. We report work in progressto use the mouth controller to interact with a physical modelof sound production by the avian syrinx.},
	Address = {Hamamatsu, Japan},
	Author = {de Silva, Gamhewage C. and Smyth, Tamara and Lyons, Michael J.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Mouth Controller, Face Tracking, Bioacoustics},
	Pages = {169--172},
	Title = {A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models},
	Url = {http://www.nime.org/proceedings/2004/nime2004_169.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_169.pdf}}

@inproceedings{Fels:2004,
	Abstract = {The Tooka was created as an exploration of two personinstruments. We have worked with two Tooka performers toenhance the original experimental device to make a musicalinstrument played and enjoyed by them. The main additions tothe device include: an additional button that behaves as amusic capture button, a bend sensor, an additional thumbactuated pressure sensor for vibrato, additional musicalmapping strategies, and new interfacing hardware. Thesedevelopments a rose through exper iences andrecommendations from the musicians playing it. In addition tothe changes to the Tooka, this paper describes the learningprocess and experiences of the musicians performing with theTooka.},
	Address = {Hamamatsu, Japan},
	Author = {Fels, Sidney S. and Kaastra, Linda and Takahashi, Sachiyo and Mccaig, Graeme},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Musician-centred design, two-person musical instrument.},
	Pages = {1--6},
	Title = {Evolving Tooka: from Experiment to Instrument},
	Url = {http://www.nime.org/proceedings/2004/nime2004_001.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_001.pdf}}

@inproceedings{Flety:2004,
	Abstract = {This paper reports our recent developments on sensor acquisition systems, taking advantage of computer network technology. We present a versatile hardware system which can be connected to wireless modules, Analog to Digital Converters, and enables Ethernet communication. We are planning to make freely available the design of this architecture. We describe also several approaches we tested for wireless communication. Such technology developments are currently used in our newly formed Performance Arts Technology Group. },
	Address = {Hamamatsu, Japan},
	Author = {Fl\'{e}ty, Emmanuel and Leroy, Nicolas and Ravarini, Jean-Christophe and Bevilacqua, Fr\'{e}d\'{e}ric},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Gesture, Sensors, Ethernet, 802.11, Computer Music.},
	Pages = {157--160},
	Title = {Versatile Sensor Acquisition System Utilizing Network Technology},
	Url = {http://www.nime.org/proceedings/2004/nime2004_157.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_157.pdf}}

@inproceedings{Franco:2004,
	Abstract = { This paper begins by evaluating various systems in terms of factors for building interactive audiovisual environments. The main issues for flexibility and expressiveness in the generation of dynamic sounds and images are then isolated. The design and development of an audiovisual system prototype is described at the end. },
	Address = {Hamamatsu, Japan},
	Author = {Franco, Enrique and Griffith, Niall J. and Fernstr\"{o}m, Mikael},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Audiovisual, composition, performance, gesture, image, representation, mapping, expressiveness.},
	Pages = {165--168},
	Title = {Issues for Designing a Flexible Expressive Audiovisual System for Real-time Performance \& Composition},
	Url = {http://www.nime.org/proceedings/2004/nime2004_165.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_165.pdf}}

@inproceedings{Gaye:2004,
	Abstract = {Sonic City is a wearable system enabling the use of the urban environment as an interface for real-time electronic music making, when walking through and interacting with a city. The device senses everyday interactions and surrounding contexts, and maps this information in real time to the sound processing of urban sounds. We conducted a short-term study with various participants using our prototype in everyday settings. This paper describes the course of the study and preliminary results in terms of how the participants used and experienced the system. These results showed that the city was perceived as the main performer but that the user improvised different tactics and ad hoc interventions to actively influence and participate in how the music was created. },
	Address = {Hamamatsu, Japan},
	Author = {Gaye, Lalya and Holmquist, Lars E.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {User study, new interface for musical expression, interactive music, wearable computing, mobility, context-awareness.},
	Pages = {161--164},
	Title = {In Duet with Everyday Urban Settings: A User Study of Sonic City},
	Url = {http://www.nime.org/proceedings/2004/nime2004_161.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_161.pdf}}

@inproceedings{Gerhard:2004,
	Abstract = {This paper describes an approach to match visual and acoustic parameters to produce an animated musical expression.Music may be generated to correspond to animation, asdescribed here; imagery may be created to correspond tomusic; or both may be developed simultaneously. This approach is intended to provide new tools to facilitate bothcollaboration between visual artists and musicians and examination of perceptual issues between visual and acousticmedia. As a proof-of-concept, a complete example is developed with linear fractals as a basis for the animation, andarranged rhythmic loops for the music. Since both visualand acoustic elements in the example are generated fromconcise specifications, the potential of this approach to create new works through parameter space exploration is accentuated, however, there are opportunities for applicationto a wide variety of source material. These additional applications are also discussed, along with issues encounteredin development of the example.},
	Address = {Hamamatsu, Japan},
	Author = {Gerhard, David and Hepting, Daryl and Mckague, Matthew},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Multimedia creation and interaction, parameter space, visu- alization, sonification.},
	Pages = {96--99},
	Title = {Exploration of the Correspondence between Visual and Acoustic Parameter Spaces},
	Url = {http://www.nime.org/proceedings/2004/nime2004_096.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_096.pdf}}

@inproceedings{Goto:2004,
	Abstract = {We will discuss the case study of application of the VirtualMusical Instrument and Sound Synthesis. Doing thisapplication, the main subject is advanced Mapping Interface inorder to connect these. For this experiment, our discussionalso refers to Neural Network, as well as a brief introduction ofthe Virtual Musical Instrument "Le SuperPolm" and GestureController "BodySuit".},
	Address = {Hamamatsu, Japan},
	Author = {Goto, Suguru and Suzuki, Takahiko},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Virtual Musical Instrument, Gesture Controller, Mapping Interface},
	Pages = {207--208},
	Title = {The Case Study of Application of Advanced Gesture Interface and Mapping Interface, Virtual Musical Instrument "Le SuperPolm" and Gesture Controller "BodySuit"},
	Url = {http://www.nime.org/proceedings/2004/nime2004_207.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_207.pdf}}

@inproceedings{Hashida:2004,
	Abstract = {This paper proposes an interface for improvisational ensemble plays which synthesizes musical sounds and graphical images on the floor from people's act of "walking." The aim of this paper is to develop such a system that enables nonprofessional people in our public spaces to play good contrapuntal music without any knowledge of music theory. The people are just walking. This system is based on the i-trace system [1] which can capture the people's behavior and give some visual feedback. },
	Address = {Hamamatsu, Japan},
	Author = {Hashida, Tomoko and Kakehi, Yasuaki and Naemura, Takeshi},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Improvisational Ensemble Play, Contrapuntal Music, Human Tracking, Traces, Spatially Augmented Reality},
	Pages = {215--216},
	Title = {Ensemble System with i-trace},
	Url = {http://www.nime.org/proceedings/2004/nime2004_215.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_215.pdf}}

@inproceedings{Havel:2004,
	Abstract = {This paper presents a project involving a percussionist playing on a virtual percussion. Both artistic and technical aspects of the project are developed. Especially, a method forstrike recognition using the Flock of Birds is presented, aswell as its use for artistic purpose.},
	Address = {Hamamatsu, Japan},
	Author = {Havel, Christophe and Desainte-Catherine, Myriam},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Gesture analysis, virtual percussion, strike recognition.},
	Pages = {31--34},
	Title = {Modeling an Air Percussion for Composition and Performance},
	Url = {http://www.nime.org/proceedings/2004/nime2004_031.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_031.pdf}}

@inproceedings{Hiraga:2004,
	Address = {Hamamatsu, Japan},
	Author = {Hiraga, Rumi and Bresin, Roberto and Hirata, Keiji and Katayose, Haruhiro},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Rencon, Turing Test, Musical Expression, Performance Ren- dering},
	Pages = {120--123},
	Title = {Rencon 2004: Turing Test for Musical Expression},
	Url = {http://www.nime.org/proceedings/2004/nime2004_120.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_120.pdf}}

@inproceedings{Hornof:2004,
	Abstract = {Though musical performers routinely use eye movements to communicate with each other during musical performances, very few performers or composers have used eye tracking devices to direct musical compositions and performances. EyeMusic is a system that uses eye movements as an input to electronic music compositions. The eye movements can directly control the music, or the music can respond to the eyes moving around a visual scene. EyeMusic is implemented so that any composer using established composition software can incorporate prerecorded eye movement data into their musical compositions.},
	Address = {Hamamatsu, Japan},
	Author = {Hornof, Anthony J. and Sato, Linda},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Electronic music composition, eye movements, eye tracking, human-computer interaction, Max/MSP.},
	Pages = {185--188},
	Title = {EyeMusic: Making Music with the Eyes},
	Url = {http://www.nime.org/proceedings/2004/nime2004_185.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_185.pdf}}

@inproceedings{Hughes:2004,
	Abstract = {The Epipe is a novel electronic woodwind controller with continuous tonehole coverage sensing, an initial design for which was introduced at NIME '03. Since then, we have successfully completed two fully operational prototypes. This short paper describes some of the issues encountered during the design and construction of this controller. It also details our own early experiences and impressions of the interface as well as its technical specifications. },
	Address = {Hamamatsu, Japan},
	Author = {Hughes, Stephen and Cannon, Cormac and O'Modhrain, Sile},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {woodwind controller, variable tonehole control, MIDI, capacitive sensing},
	Pages = {199--200},
	Title = {Epipe : A Novel Electronic Woodwind Controller},
	Url = {http://www.nime.org/proceedings/2004/nime2004_199.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_199.pdf}}

@inproceedings{Ishida:2004,
	Address = {Hamamatsu, Japan},
	Author = {Ishida, Katsuhisa and Kitahara, Tetsuro and Takeda, Masayuki},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Improvisation support, jam session, melody correction, N-gram model, melody modeling, musical instrument},
	Pages = {177--180},
	Title = {ism: Improvisation Supporting System based on Melody Correction},
	Url = {http://www.nime.org/proceedings/2004/nime2004_177.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_177.pdf}}

@inproceedings{Jorda:2004,
	Abstract = {When envisaging new digital instruments, designers do not have to limit themselves to their sonic capabilities (which can be absolutely any), not even to their algorithmic power; they must be also especially careful about the instruments' conceptual capabilities, to the ways instruments impose or suggest to their players new ways of thinking, new ways of establishing relations, new ways of interacting, new ways of organizing time and textures; new ways, in short, of playing new musics. This article explores the dynamic relation that builds between the player and the instrument, introducing concepts such as efficiency, apprenticeship and learning curve It aims at constructing a framework in which the possibilities and the diversity of music instruments as well as the possibilities and the expressive freedom of human music performers could start being evaluated. },
	Address = {Hamamatsu, Japan},
	Author = {Jord\`{a}, Sergi},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Musical instruments design, learning curve, apprenticeship, musical efficiency.},
	Pages = {59--63},
	Title = {Digital Instruments and Players: Part I -- Efficiency and Apprenticeship},
	Url = {http://www.nime.org/proceedings/2004/nime2004_059.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_059.pdf}}

@inproceedings{Kaltenbrunner:2004,
	Abstract = {This article reflects the current state of the reacTable* project,an electronic music instrument with a tangible table-basedinterface, which is currently under development at theAudiovisual Institute at the Universitat Pompeu Fabra. In thispaper we are focussing on the issue of Dynamic Patching,which is a particular and unique aspect of the sound synthesisand control paradigms of the reacTable*. Unlike commonvisual programming languages for sound synthesis, whichconceptually separate the patch building process from theactual musical performance, the reacTable* combines theconstruction and playing of the instrument in a unique way.The tangible interface allows direct manipulation control overany of the used building blocks, which physically representthe whole synthesizer function.},
	Address = {Hamamatsu, Japan},
	Author = {Kaltenbrunner, Martin and Geiger, G\"{u}nter and Jord\`{a}, Sergi},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {dynamic patching,musical instrument,sound synthesis,tangible interfaces,visual programming},
	Pages = {19--22},
	Title = {Dynamic Patches for Live Musical Performance},
	Url = {http://www.nime.org/proceedings/2004/nime2004_019.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_019.pdf}}

@inproceedings{Kapur:2004,
	Abstract = {This paper describes the design of an Electronic Sitar controller, adigitally modified version of Saraswati's (the Hindu Goddess ofMusic) 19-stringed, pumpkin shelled, traditional North Indianinstrument. The ESitar uses sensor technology to extract gesturalinformation from a performer, deducing music information suchas pitch, pluck timing, thumb pressure, and 3-axes of head tilt totrigger real-time sounds and graphics. It allows for a variety oftraditional sitar technique as well as new performance methods.Graphical feedback allows for artistic display and pedagogicalfeedback. The ESitar uses a programmable Atmel microprocessorwhich outputs control messages via a standard MIDI jack.},
	Address = {Hamamatsu, Japan},
	Author = {Kapur, Ajay and Lazier, Ariel J. and Davidson, Philip L. and Wilson, Scott and Cook, Perry R.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {atmel microcontroller,controller,electronic sitar,esitar,human computer interface,indian string controller,instrument graphical feedback,midi,veldt},
	Pages = {7--12},
	Title = {The Electronic Sitar Controller},
	Url = {http://www.nime.org/proceedings/2004/nime2004_007.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_007.pdf}}

@inproceedings{Katayose:2004,
	Abstract = {This paper describes an approach for playing expressivemusic, as it refers to a pianist's expressiveness, with atapping-style interface. MIDI-formatted expressiveperformances played by pianists were first analyzed andtransformed into performance templates, in which thedeviations from a canonical description was separatelydescribed for each event. Using one of the templates as askill complement, a player can play music expressivelyover and under the beat level. This paper presents ascheduler that allows a player to mix her/his own intensionand the expressiveness in the performance template. Theresults of a forty-subject user study suggest that using theexpression template contributes the subject's joy of playingmusic with the tapping-style performance interface. Thisresult is also supported by a brain activation study that wasdone using a near-infrared spectroscopy (NIRS).Categories and Subject DescriptorsH.5.5 [Information Interfaces and Presentation]: Sound andMusic Computing methodologies and techniques.},
	Address = {Hamamatsu, Japan},
	Author = {Katayose, Haruhiro and Okudaira, Keita},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Rencon, interfaces for musical expression, visualization},
	Pages = {124--129},
	Title = {Using an Expressive Performance Template in a Music Conducting Interface},
	Url = {http://www.nime.org/proceedings/2004/nime2004_124.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_124.pdf}}

@inproceedings{Kawahara:2004,
	Abstract = {A series of demonstrations of synthesized acappella songsbased on an auditory morphing using STRAIGHT [5] willbe presented. Singing voice data for morphing were extracted from the RWCmusic database of musical instrument sound. Discussions on a new extension of the morphing procedure to deal with vibrato will be introduced basedon the statistical analysis of the database and its effect onsynthesized acappella will also be demonstrated.},
	Address = {Hamamatsu, Japan},
	Author = {Kawahara, Hideki and Banno, Hideki and Morise, Masanori},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Rencon, Acappella, RWCdatabase, STRAIGHT, morph- ing},
	Pages = {130--131},
	Title = {Acappella Synthesis Demonstrations using RWC Music Database},
	Url = {http://www.nime.org/proceedings/2004/nime2004_130.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_130.pdf}}

@inproceedings{Lee:2004,
	Abstract = {This paper describes the first system designed to allow children to conduct an audio and video recording of an orchestra. No prior music experience is required to control theorchestra, and the system uses an advanced algorithm totime stretch the audio in real-time at high quality and without altering the pitch. We will discuss the requirements andchallenges of designing an interface to target our particularuser group (children), followed by some system implementation details. An overview of the algorithm used for audiotime stretching will also be presented. We are currently using this technology to study and compare professional andnon-professional conducting behavior, and its implicationswhen designing new interfaces for multimedia. You're theConductor is currently a successful exhibit at the Children'sMuseum in Boston, USA.},
	Address = {Hamamatsu, Japan},
	Author = {Lee, Eric and Nakra, Teresa M. and Borchers, Jan},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {conducting systems,design patterns,gesture recogni-,interactive exhibits,real-time audio stretching,tion},
	Pages = {68--73},
	Title = {You're The Conductor: A Realistic Interactive Conducting System for Children},
	Url = {http://www.nime.org/proceedings/2004/nime2004_068.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_068.pdf}}

@inproceedings{Lew:2004,
	Abstract = {This paper describes the design of an expressive tangible interface for cinema editing as a live performance. A short survey of live video practices is provided. The Live Cinema instrument is a cross between a musical instrument and a film editing tool, tailored for improvisational control as well as performance presence. Design specifications for the instrument evolved based on several types of observations including: our own performances in which we used a prototype based on available tools; an analysis of performative aspects of contemporary DJ equipment; and an evaluation of organizational aspects of several generations of film editing tools. Our instrument presents the performer with a large canvas where projected images can be grabbed and moved around with both hands simultaneously; the performer also has access to two video drums featuring haptic display to manipulate the shots and cut between streams. The paper ends with a discussion of issues related to the tensions between narrative structure and hands-on control, live and recorded arts and the scoring of improvised films. },
	Address = {Hamamatsu, Japan},
	Author = {Lew, Michael},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {live cinema, video controller, visual music, DJ, VJ, film editing, tactile interface, two-hand interaction, improvisation, performance, narrative structure.},
	Pages = {144--149},
	Title = {Live Cinema: Designing an Instrument for Cinema Editing as a Live Performance},
	Url = {http://www.nime.org/proceedings/2004/nime2004_144.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_144.pdf}}

@inproceedings{Lippit:2004,
	Abstract = {In this paper, I describe a realtime sampling system for theturntablist, and the hardware and software design of the secondprototype, 16padjoystickcontroller.},
	Address = {Hamamatsu, Japan},
	Author = {Lippit, Takuro M.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {DJ, Turntablism, Realtime Sampling, MAX/MSP, Microchip PIC microcontroller, MIDI},
	Pages = {211--212},
	Title = {Realtime Sampling System for the Turntablist, Version 2: 16padjoystickcontroller},
	Url = {http://www.nime.org/proceedings/2004/nime2004_211.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_211.pdf}}

@inproceedings{Mandelis:2004,
	Abstract = {This paper describes the use of evolutionary and artificial life techniques in sound design and the development of performance mapping to facilitate the real-time manipulation of such sounds through some input device controlled by the performer. A concrete example of such a system is described which allows musicians without detailed knowledge and experience of sound synthesis techniques to interactively develop new sounds and performance manipulation mappings according to their own aesthetic judgements. Experiences with the system are discussed. },
	Address = {Hamamatsu, Japan},
	Author = {Mandelis, James and Husbands, Phil},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {musical interaction,performance mapping,sound synthesis},
	Pages = {47--50},
	Title = {Don't Just Play it, Grow it! : Breeding Sound Synthesis and Performance Mappings},
	Url = {http://www.nime.org/proceedings/2004/nime2004_047.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_047.pdf}}

@inproceedings{Miyashita:2004,
	Abstract = {In this paper, we propose Thermoscore, a musical score form-that dynamically alters the temperature of the instrument/player interface. We developed the first version of theThermoscore display by lining Peltier devices on piano keys.The system is controlled by MIDI notes-on messages from anMIDI sequencer, so that a composer can design songs that aresequences of temperature for each piano key. We also discussmethodologies for composing with this system, and suggesttwo approaches. The first is to make desirable keys (or otherkeys) hot. The second one uses chroma-profile, that is, a radarchart representation of the frequency of pitch notations in the-piece. By making keys of the same chroma hot in reverse proportion to the value of the chroma-profile, it is possible to-constrain the performer's improvisation and to bring the tonality space close to a certain piece.},
	Address = {Hamamatsu, Japan},
	Author = {Miyashita, Homei and Nishimoto, Kazushi},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {musical score, improvisation, peltier device, chroma profile},
	Pages = {104--107},
	Title = {Thermoscore: A New-type Musical Score with Temperature Sensation},
	Url = {http://www.nime.org/proceedings/2004/nime2004_104.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_104.pdf}}

@inproceedings{Morris:2004,
	Abstract = {This paper describes the SillyTone Squish Factory, a haptically engaging musical interface. It contains the motivation behind the device's development, a description of the interface, various mappings of the interface to musical applications, details of its construction, and the requirements to demo the interface. },
	Address = {Hamamatsu, Japan},
	Author = {Morris, Geoffrey C. and Leitman, Sasha and Kassianidou, Marina},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Pages = {201--202},
	Title = {SillyTone Squish Factory},
	Url = {http://www.nime.org/proceedings/2004/nime2004_201.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_201.pdf}}

@inproceedings{Nagashima:2004,
	Address = {Hamamatsu, Japan},
	Author = {Nagashima, Yoichi},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Pages = {173--176},
	Title = {Measurement of Latency in Interactive Multimedia Art},
	Url = {http://www.nime.org/proceedings/2004/nime2004_173.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_173.pdf}}

@inproceedings{Nelson:2004,
	Address = {Hamamatsu, Japan},
	Author = {Nelson, Mark and Thom, Belinda},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Pages = {35--38},
	Title = {A Survey of Real-Time MIDI Performance},
	Url = {http://www.nime.org/proceedings/2004/nime2004_035.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_035.pdf}}

@inproceedings{OModhrain:2004,
	Abstract = {The PebbleBox and the CrumbleBag are examples of a granular interaction paradigm, in which the manipulation ofphysical grains of arbitrary material becomes the basis forinteracting with granular sound synthesis models. The soundsmade by the grains as they are manipulated are analysed,and parameters such as grain rate, grain amplitude andgrain density are extracted. These parameters are then usedto control the granulation of arbitrary sound samples in realtime. In this way, a direct link is made between the haptic sensation of interacting with grains and the control ofgranular sounds.},
	Address = {Hamamatsu, Japan},
	Author = {O'Modhrain, Sile and Essl, Georg},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Musical instrument, granular synthesis, haptic},
	Pages = {74--79},
	Title = {PebbleBox and CrumbleBag: Tactile Interfaces for Granular Synthesis},
	Url = {http://www.nime.org/proceedings/2004/nime2004_074.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_074.pdf}}

@inproceedings{Paine:2004,
	Address = {Hamamatsu, Japan},
	Author = {Paine, Garth},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {dynamic,dynamic morphology,gesture,interaction,mapping,mind,music,orchestration,spectral morphology},
	Pages = {80--86},
	Title = {Gesture and Musical Interaction : Interactive Engagement Through Dynamic Morphology},
	Url = {http://www.nime.org/proceedings/2004/nime2004_080.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_080.pdf}}

@inproceedings{Pashenkov:2004,
	Abstract = {This report presents a novel interface for musical performance which utilizes a record-player turntable augmented with a computation engine and a high-density optical sensing array. The turntable functions as a standalone step sequencer for MIDI events transmitted to a computer or another device and it is programmed in real-time using visual disks. The program instructions are represented on printed paper disks directly as characters of English alphabet that could be read by human as effectively as they are picked up by the machine's optical cartridge. The result is a tangible interface that allows the user to manipulate pre-arranged musical material by hand, by adding together instrumental tracks to form a dynamic mix. A functional implementation of this interface is discussed in view of historical background and other examples of electronic instruments for music creation and performance incorporating optical turntable as a central element.},
	Address = {Hamamatsu, Japan},
	Author = {Pashenkov, Nikita},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Interaction, visualization, tangible interface, controllers, optical turntable, performance.},
	Pages = {64--67},
	Title = {A New Mix of Forgotten Technology: Sound Generation, Sequencing and Performance Using an Optical Turntable},
	Url = {http://www.nime.org/proceedings/2004/nime2004_064.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_064.pdf}}

@inproceedings{Pelletier:2004,
	Address = {Hamamatsu, Japan},
	Author = {Pelletier, Jean-Marc},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {computer vision,image analysis,maxmsp,morphology,musical},
	Pages = {197--198},
	Title = {A Shape-Based Approach to Computer Vision Musical Performance Systems},
	Url = {http://www.nime.org/proceedings/2004/nime2004_197.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_197.pdf}}

@inproceedings{Poepel:2004,
	Abstract = {A system is introduced that allows a string player to control asynthesis engine with the gestural skills he is used to. Theimplemented system is based on an electric viola and asynthesis engine that is directly controlled by the unanalysedaudio signal of the instrument and indirectly by controlparameters mapped to the synthesis engine. This method offersa highly string-specific playability, as it is sensitive to thekinds of musical articulation produced by traditional playingtechniques. Nuances of sound variation applied by the playerwill be present in the output signal even if those nuances arebeyond traditionally measurable parameters like pitch,amplitude or brightness. The relatively minimal hardwarerequirements make the instrument accessible with littleexpenditure.},
	Address = {Hamamatsu, Japan},
	Author = {Poepel, Cornelius},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Electronic bowed string instrument, playability, musical instrument design, human computer interface, oscillation controlled sound synthesis},
	Pages = {150--153},
	Title = {Synthesized Strings for String Players},
	Url = {http://www.nime.org/proceedings/2004/nime2004_150.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_150.pdf}}

@inproceedings{Ramakrishnan:2004,
	Address = {Hamamatsu, Japan},
	Author = {Ramakrishnan, Chandrasekhar and Freeman, Jason and Varnik, Kristjan},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Interactive Music Systems, Networking and Control, Voice and Speech Analysis, Auracle, JSyn, TransJam, Linear Pre- diction, Neural Networks, Voice Interface, Open Sound Con- trol},
	Pages = {100--103},
	Title = {The Architecture of Auracle: a Real-Time, Distributed, Collaborative Instrument},
	Url = {http://www.nime.org/proceedings/2004/nime2004_100.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_100.pdf}}

@inproceedings{Serafin:2004,
	Address = {Hamamatsu, Japan},
	Author = {Serafin, Stefania and Young, Diana},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Pages = {108--111},
	Title = {Toward a Generalized Friction Controller: from the Bowed String to Unusual Musical Instruments},
	Url = {http://www.nime.org/proceedings/2004/nime2004_108.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_108.pdf}}

@inproceedings{Sharon:2004,
	Abstract = {This paper describes the design and on-going development ofan expressive gestural MIDI interface and how this couldenhance live performance of electronic music.},
	Address = {Hamamatsu, Japan},
	Author = {Sharon, Michael E.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {gestural control, mapping, Pure Data (pd), accelerometers, MIDI, microcontrollers, synthesis, musical instruments},
	Pages = {213--214},
	Title = {The Stranglophone: Enhancing Expressiveness In Live Electronic Music},
	Url = {http://www.nime.org/proceedings/2004/nime2004_213.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_213.pdf}}

@inproceedings{Shatin:2004,
	Abstract = {In this report, we discuss Tree Music, an interactive computermusic installation created using GAIA (Graphical Audio InterfaceApplication), a new open-source interface for controlling theRTcmix synthesis and effects processing engine. Tree Music,commissioned by the University of Virginia Art Museum, used awireless camera with a wide-angle lens to capture motion andocclusion data from exhibit visitors. We show how GAIA wasused to structure and navigate the compositional space, and howthis program supports both graphical and text-based programmingin the same application. GAIA provides a GUI which combinestwo open-source applications: RTcmix and Perl.},
	Address = {Hamamatsu, Japan},
	Author = {Shatin, Judith and Topper, David},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Composition, new interfaces, interactive systems, open source, Real time audio, GUI controllers, video tracking},
	Pages = {51--54},
	Title = {Tree Music: Composing with GAIA},
	Url = {http://www.nime.org/proceedings/2004/nime2004_051.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_051.pdf}}

@inproceedings{Sheehan:2004,
	Abstract = {This paper discusses some of the issues pertaining to thedesign of digital musical instruments that are to effectively fillthe role of traditional instruments (i.e. those based on physicalsound production mechanisms). The design andimplementation of a musical instrument that addresses some ofthese issues, using scanned synthesis coupled to a "smart"physical system, is described.},
	Address = {Hamamatsu, Japan},
	Author = {Sheehan, Brian},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Digital musical instruments, real-time performance, scanned synthesis, pd, tactile interfaces, sensors, Shapetape, mapping.},
	Pages = {92--95},
	Title = {The Squiggle: A Digital Musical Instrument},
	Url = {http://www.nime.org/proceedings/2004/nime2004_092.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_092.pdf}}

@inproceedings{Singer:2004,
	Abstract = {This paper describes new work and creations of LEMUR, agroup of artists and technologists creating robotic musicalinstruments.},
	Address = {Hamamatsu, Japan},
	Author = {Singer, Eric and Feddersen, Jeff and Redmon, Chad and Bowen, Bil},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {additional computer or special,commands allows,familiar tools with no,improvisations,the musician or composer,to control the instrument,use of standard midi,using},
	Pages = {181--184},
	Title = {LEMUR's Musical Robots},
	Url = {http://www.nime.org/proceedings/2004/nime2004_181.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_181.pdf}}

@inproceedings{Steiner:2004,
	Abstract = {StickMusic is an instrument comprised of two haptic devices, a joystick and a mouse, which control a phase vocoder in real time. The purpose is to experiment with ideas of how to apply haptic feedback when controlling synthesis algorithms that have no direct analogy to methods of generating sound in the physical world. },
	Address = {Hamamatsu, Japan},
	Author = {Steiner, Hans-Christoph},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {haptic feedback, gestural control, performance, joystick, mouse},
	Pages = {203--204},
	Title = {StickMusic: Using Haptic Feedback with a Phase Vocoder},
	Url = {http://www.nime.org/proceedings/2004/nime2004_203.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_203.pdf}}

@inproceedings{Takahata:2004,
	Address = {Hamamatsu, Japan},
	Author = {Takahata, Masami and Shiraki, Kensuke and Sakane, Yutaka and Takebayashi, Yoichi},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Sound feedback, Karate, Learning environment, Wearable device},
	Pages = {13--18},
	Title = {Sound Feedback for Powerful Karate Training},
	Url = {http://www.nime.org/proceedings/2004/nime2004_013.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_013.pdf}}

@inproceedings{Talmudi:2004,
	Address = {Hamamatsu, Japan},
	Author = {Talmudi, Assaf K.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {1,acting as a soul,and its pneumatic,decentralized processes,emergence,genetic algorithms,instrument dualism,score,with its paper rolls},
	Pages = {43--46},
	Title = {The Decentralized Pianola: Evolving Mechanical Music Instruments using a Genetic Algorithm},
	Url = {http://www.nime.org/proceedings/2004/nime2004_043.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_043.pdf}}

@inproceedings{Tanaka:2004,
	Abstract = {We present a system for collaborative musical creation onmobile wireless networks. The work extends on simple peerto-peer file sharing systems towards ad-hoc mobility andstreaming. It extends upon music listening from a passiveact to a proactive, participative activity. The system consistsof a network based interactive music engine and a portablerendering player. It serves as a platform for experiments onstudying the sense of agency in collaborative creativeprocess, and requirements for fostering musical satisfactionin remote collaboration. },
	Address = {Hamamatsu, Japan},
	Author = {Tanaka, Atau},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {mobile music,peer-to-peer,wireless ad-hoc networks},
	Pages = {154--156},
	Title = {Mobile Music Making},
	Url = {http://www.nime.org/proceedings/2004/nime2004_154.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_154.pdf}}

@inproceedings{Torchia:2004,
	Address = {Hamamatsu, Japan},
	Author = {Torchia, Ryan H. and Lippe, Cort},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Pages = {116--119},
	Title = {Techniques for Multi-Channel Real-Time Spatial Distribution Using Frequency-Domain Processing},
	Url = {http://www.nime.org/proceedings/2004/nime2004_116.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_116.pdf}}

@inproceedings{Van-Nort:2004,
	Abstract = {The choice of mapping strategies to effectively map controller variables to sound synthesis algorithms is examined.Specifically, we look at continuous mappings that have ageometric representation. Drawing from underlying mathematical theory, this paper presents a way to compare mapping strategies, with the goal of achieving an appropriatematch between mapping and musical performance context.This method of comparison is applied to existing techniques,while a suggestion is offered on how to integrate and extendthis work through a new implementation.},
	Address = {Hamamatsu, Japan},
	Author = {Van Nort, Doug and Wanderley, Marcelo M. and Depalle, Philippe},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Mapping, Interface Design, Interpolation, Computational Geometry},
	Pages = {87--91},
	Title = {On the Choice of Mappings Based on Geometric Properties},
	Url = {http://www.nime.org/proceedings/2004/nime2004_087.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_087.pdf}}

@inproceedings{Wang:2004,
	Abstract = {On-the-fly programming is a style of programming in which the programmer/performer/composer augments and modifies the program while it is running, without stopping or restarting, in order to assert expressive, programmable control at runtime. Because of the fundamental powers of programming languages, we believe the technical and aesthetic aspects of on-the-fly programming are worth exploring. In this paper, we present a formalized framework for on-the-fly programming, based on the ChucK synthesis language, which supports a truly concurrent audio programming model with sample-synchronous timing, and a highly on-the-fly style of programming. We first provide a well-defined notion of on-thefly programming. We then address four fundamental issues that confront the on-the-fly programmer: timing, modularity, conciseness, and flexibility. Using the features and properties of ChucK, we show how it solves many of these issues. In this new model, we show that (1) concurrency provides natural modularity for on-the-fly programming, (2) the timing mechanism in ChucK guarantees on-the-fly precision and consistency, (3) the Chuck syntax improves conciseness, and (4) the overall system is a useful framework for exploring on-the-fly programming. Finally, we discuss the aesthetics of on-the-fly performance. },
	Address = {Hamamatsu, Japan},
	Author = {Wang, Ge and Cook, Perry R.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {code as interface,compiler,concurrency,concurrent audio programming,on-the-fly programming,real-,synchronization,synthesis,time,timing,virtual machine},
	Pages = {138--143},
	Title = {On-the-fly Programming: Using Code as an Expressive Musical Instrument},
	Url = {http://www.nime.org/proceedings/2004/nime2004_138.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_138.pdf}}

@inproceedings{Won:2004,
	Abstract = {In this paper, we describe a new MIDI controller, the LightPipes. The Light Pipes are a series of pipes that respond toincident light. The paper will discuss the design of theinstrument, and the prototype we built. A piece was composedfor the instrument using algorithms designed in Pure Data.},
	Address = {Hamamatsu, Japan},
	Author = {Won, Sook Y. and Chan, Humane and Liu, Jeremy},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {Controllers, MIDI, light sensors, Pure Data.},
	Pages = {209--210},
	Title = {Light Pipes: A Light Controlled MIDI Instrument},
	Url = {http://www.nime.org/proceedings/2004/nime2004_209.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_209.pdf}}

@inproceedings{Young:2004,
	Address = {Hamamatsu, Japan},
	Author = {Young, Diana and Fujinaga, Ichiro},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {bluetooth,drum stick,japanese drum,taiko,wireless},
	Pages = {23--26},
	Title = {AoBachi: A New Interface for Japanese Drumming},
	Url = {http://www.nime.org/proceedings/2004/nime2004_023.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_023.pdf}}

@inproceedings{Zaborowski:2004,
	Abstract = {This paper describes ThumbTEC, a novel general purposeinput device for the thumb or finger that is useful in a widevariety of applications from music to text entry. The device i smade up of three switches in a row and one miniature joystickon top of the middle switch. The combination of joystickdirection and switch(es) controls what note or alphanumericcharacter is selected by the finger. Several applications aredetailed.},
	Address = {Hamamatsu, Japan},
	Author = {Zaborowski, Philippe S.},
	Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
	Date-Modified = {2013-08-16 18:23:29 +0000},
	Editor = {Nagashima, Yoichi and Ito, Yasuo and Furuta, Yuji},
	Keywords = {One-Thumb Input Device, HCI, Isometric Joystick, Mobile Computing, Handheld Devices, Musical Instrument.},
	Pages = {112--115},
	Title = {ThumbTec: A New Handheld Input Device},
	Url = {http://www.nime.org/proceedings/2004/nime2004_112.pdf},
	Year = {2004},
	Bdsk-Url-1 = {http://www.nime.org/proceedings/2004/nime2004_112.pdf}}
